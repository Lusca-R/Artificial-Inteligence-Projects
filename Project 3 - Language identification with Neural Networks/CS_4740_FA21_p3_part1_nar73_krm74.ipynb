{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS_4740_FA21_p3_part1_nar73_krm74.ipynb","provenance":[{"file_id":"1R8W1SXfgC128SsLiIY3038Zg-QsQUaDv","timestamp":1635878111019},{"file_id":"1yY5w2vnjeijEGjSU84AMq32QX7moaShv","timestamp":1633825714560}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"epRaBvj_bapY"},"source":["# Project 3 (Part 1): Language identification with Neural Networks\n","## CS4740/5740 Fall 2021\n","\n","Names: Lusca Robinson, Kyrus Mama\n","\n","Netids: nar73, krm74\n","\n","### Project Submission Due: Friday Nov 5 11:59pm\n","Please submit a **pdf file** of this notebook on **Gradescope**, and your **ipynb** on **CMS**. For instructions on generating pdf and ipynb files, please refer to project 1 or project 2 instructions.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EIEkIveBVYyU"},"source":["## Introduction\n","In this two-part project we will consider **neural networks**. In part 1 you will study a Feedforward Neural Network (FFNN) for performing language identification (i.e., determining whether tha piece of text  is Shakespearean English or modern English).\n","\n","You will be given an implementation for a FFNN and be asked to debug it in a specific way. You also will be required to submit a description of any additional libraries used (other than those that we import for you), how your group divided up the work, and your feedback regarding the assignment (the latter with **Part 2** of project 3). The google doc template for the write up is [here](https://docs.google.com/document/d/16scmrRva5WczB4a4QDYhFJBXaaAJd8oesEEAdi6ehbY/edit)."]},{"cell_type":"markdown","metadata":{"id":"-1HSug5yWVrI"},"source":["## Advice ðŸš€\n","As always, the report is important! The report is where you get to show\n","that you understand not only what you are doing but also why and how you are doing it. So be clear, organized and concise; avoid vagueness and excess verbiage."]},{"cell_type":"markdown","metadata":{"id":"-fyl3-8TWyR9"},"source":["## Dataset\n","You are given access to a set of parallel sentences. One sentence is written in modern English (the \"source\") and another is in Shakespearean English (the \"target\"). For this project, given modern English you will need to translate it into Shakespearean English. This is usually called (Neural) Machine Translation. We'll simply refer to it as NMT or Neural Machine Translation in the project.\n","\n","We will minimally preprocess the source/target sentences and handle tokenization in what we release. For this assignment, we do not anticipate any further preprocessing to be done by you. Should you choose to do so, it would be interesting to hear about in the report (along with whether or not it helped performance), but it is not a required aspect of the assignment."]},{"cell_type":"code","metadata":{"id":"ARl1pk1PGL2Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636051014246,"user_tz":240,"elapsed":94380,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"eddcd4e9-d4df-4ced-a4dd-ef19a6e075e1"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive', force_remount=True)\n","\n","#path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS 4740\", \"Project 3\", \"Dataset\")\n","train_path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS 4740\", \"Project 3\", \"Dataset\", \"train.txt\") # replace based on your Google drive organization\n","val_path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS 4740\", \"Project 3\", \"Dataset\", \"val.txt\") # replace based on your Google drive organization"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"rD5wwTl3W3CO"},"source":["# Part 1: Feedforward Neural Network\n","\n","In this section, there are two main coding components relevant to **Part 1**.\n","\n","1. `Data loader`\\\n","As the name suggests, this section loads the data from the dataset files and handles other preprocessing and setup. You will **not** need to change this file and should **not** change this file throughout the assignment.\n","\n","2. `ffnn`\\\n","This contains the model and code that uses the model for **Part 1**.\n","\n","In the `ffnn` section, you will find a Feedforward Neural Net serving as the underlying model for performing language detection.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ciCL-fBNaErA"},"source":["## Part 1: Tips\n","\n","We do not assume you have **any** experience working with neural networks and/or debugging them. You may discover debugging neural network code is quite different from debugging in general software engineering and from debugging in other areas of CS such as algorithms and systems.\n","\n","We suggest you systematically step through the code and simultanously (perhaps by physically drawing out the vectors, matrices, and tensors involved) describe what the computations _mean_. What you are looking for is where the code differs from what you would expect a neural net to do."]},{"cell_type":"markdown","metadata":{"id":"pzGvj2WzaJNh"},"source":["## Part 1: Rules\n","\n","For **Part 1**, you will not be able to ask any questions on EdStem and we will be unable to provide any meaningful advice in office hours. Unfortunately, this is the nature of debugging, it is unlikely anyone can give you specific advice for most problems you encounter and we have already provided general tips in the preceding section. If you absolutely must ask a question or you believe there is some kind of issue with the assignment for this part, please submit a private EdStem post and we will respond swiftly.\n","\n","As a reminder **communication about the assignment _between_ distinct groups is not permitted and is a violation of the Academic Integrity policy**. For this assignment, we will be _extremely_ stringent about this, given that debugging is entirely pointless if someone else in a different group tells you where the error is."]},{"cell_type":"markdown","metadata":{"id":"qeC3pYiebc6r"},"source":["## Import libraries and connect to Google Drive"]},{"cell_type":"code","metadata":{"id":"quIJujja-jS2"},"source":["import json\n","import math\n","import os\n","from pathlib import Path\n","import random\n","import time\n","from tqdm.notebook import tqdm, trange\n","from typing import Dict, List, Set, Tuple\n","\n","import numpy as np\n","import nltk\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from tqdm.notebook import tqdm, trange"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1a8IrpLbqbh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636051048944,"user_tz":240,"elapsed":665,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"17bf6fee-2954-4b63-9d63-9e4a4c4460d4"},"source":["nltk.download(\"punkt\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"TU8Yq_dMbhtQ"},"source":["## Data loader"]},{"cell_type":"code","metadata":{"id":"pfVaPnSf1WU-"},"source":["language_to_idx = {\n","    \"Modern\": 0,\n","    \"Shakespeare\": 1,\n","}\n","idx_to_language = {v: k for k, v in language_to_idx.items()}\n","UNK = \"<UNK>\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ajMvRL3b2sz"},"source":["def fetch_data(train_data_path, val_data_path):\n","    \"\"\"fetch_data retrieves the data from a json/csv and outputs the validation\n","    and training data\n","\n","    :param train_data_path:\n","    :type train_data_path: str\n","    :return: Training, validation pair where the training is a list of document, label pairs\n","    :rtype: Tuple[\n","        List[Tuple[List[str], int]],\n","        List[Tuple[List[str], int]], \n","    ]\n","    \"\"\"\n","    with open(train_data_path) as training_f:\n","        training = training_f.read().split(\"\\n\")\n","    with open(val_data_path) as valid_f:\n","        validation = valid_f.read().split(\"\\n\")\n","\t\n","    # If needed you can shrink the training and validation data to speed up somethings but this isn't always safe to do by setting k < 10000\n","    # k = #fill in\n","    # training = random.shuffle(training)\n","    # validation = random.shuffle(validation)\n","    # training, validation = training[:k], validation[:(k // 10)]\n","\n","    tra = []\n","    val = []\n","    for elt in training:\n","        if elt == '':\n","            continue\n","        txt, language = elt.split(\"\\t\")\n","        tra.append((nltk.word_tokenize(txt), language))\n","    for elt in validation:\n","        if elt == '':\n","            continue\n","        txt, language = elt.split(\"\\t\")\n","        val.append((nltk.word_tokenize(txt), language))\n","\n","    return tra, val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GucjLU2dXztT"},"source":["def make_vocab(data):\n","    \"\"\"make_vocab creates a set of vocab words that the model knows\n","\n","    :param data: The list of documents that is used to make the vocabulary\n","    :type data: List[str]\n","    :returns: A set of strings corresponding to the vocabulary\n","    :rtype: Set[str]\n","    \"\"\"\n","    vocab = set()\n","    for document, _ in data:\n","        for word in document:\n","            vocab.add(word)\n","    return vocab \n","\n","\n","def make_indices(vocab):\n","\t\"\"\"make_indices creates a 1-1 mapping of word and indices for a vocab.\n","\n","\t:param vocab: The strings corresponding to the vocabulary in train data.\n","\t:type vocab: Set[str]\n","\t:returns: A tuple containing the vocab, word2index, and index2word.\n","\t\tvocab is a set of strings in the vocabulary including <UNK>.\n","\t\tword2index is a dictionary mapping tokens to its index (0, ..., V-1)\n","\t\tindex2word is a dictionary inverting the mapping of word2index\n","\t:rtype: Tuple[\n","\t\tSet[str],\n","\t\tDict[str, int],\n","\t\tDict[int, str],\n","\t]\n","\t\"\"\"\n","\tvocab_list = sorted(vocab)\n","\tvocab_list.append(UNK)\n","\tword2index = {}\n","\tindex2word = {}\n","\tfor index, word in enumerate(vocab_list):\n","\t\tword2index[word] = index \n","\t\tindex2word[index] = word \n","\tvocab.add(UNK)\n","\treturn vocab, word2index, index2word \n","\n","\n","def convert_to_vector_representation(data, word2index, test=False):\n","\t\"\"\"convert_to_vector_representation converts the list of strings into a vector\n","\n","\t:param data: The dataset to be converted into a vectorized format\n","\t:type data: Union[\n","\t\tList[Tuple[List[str], int]],\n","\t\tList[str],\n","\t]\n","\t:param word2index: A mapping of word to index\n","\t:type word2index: Dict[str, int]\n","\t:returns: A list of vector representations of the input or pairs of vector\n","\t\trepresentations with expected output\n","\t:rtype: List[Tuple[torch.Tensor, int]] or List[torch.Tensor]\n","\n","\tList[Tuple[List[torch.Tensor], int]] or List[List[torch.Tensor]]\n","\t\"\"\"\n","\tif test:\n","\t\tvectorized_data = []\n","\t\tfor document in data:\n","\t\t\tvector = torch.zeros(len(word2index)) \n","\t\t\tfor word in document:\n","\t\t\t\tindex = word2index.get(word, word2index[UNK])\n","\t\t\t\tvector[index] += 1\n","\t\t\tvectorized_data.append(vector)\n","\telse:\n","\t\tvectorized_data = []\n","\t\tfor document, y in data:\n","\t\t\tvector = torch.zeros(len(word2index)) \n","\t\t\tfor word in document:\n","\t\t\t\tindex = word2index.get(word, word2index[UNK])\n","\t\t\t\tvector[index] += 1\n","\t\t\tvectorized_data.append((vector, y))\n","\treturn vectorized_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vDZVRVG_WAp"},"source":["class LanguageDataset(Dataset):\n","    \"\"\"LanguageDataset is a torch dataset to interact with the Language data.\n","\n","    :param data: The vectorized dataset with input and expected output values\n","    :type data: List[Tuple[List[torch.Tensor], int]]\n","    \"\"\"\n","    def __init__(self, data):\n","        self.data = data\n","        self.len = len(data)\n","    \n","    def __len__(self):\n","        \"\"\"__len__ returns the number of samples in the dataset.\n","\n","        :returns: number of samples in dataset\n","        :rtype: int\n","        \"\"\"\n","        return self.len\n","    \n","    def __getitem__(self, index):\n","        \"\"\"__getitem__ returns the tensor, output pair for a given index\n","\n","        :param index: index within dataset to return\n","        :type index: int\n","        :returns: A tuple (x, y) where x is model input and y is our label\n","        :rtype: Tuple[torch.Tensor, int]\n","        \"\"\"\n","        return convert_to_vector_representation(self.data[index:index+1], word2index)[0]\n","\n","def get_data_loaders(train, val, batch_size=16):\n","    \"\"\"\n","    \"\"\"\n","    # First we create the dataset given our train and validation lists\n","    dataset = LanguageDataset(train + val)\n","\n","    # Then, we create a list of indices for all samples in the dataset\n","    train_indices = [i for i in range(len(train))]\n","    val_indices = [i for i in range(len(train), len(train) + len(val))]\n","\n","    # Now we define samplers and loaders for train and val\n","    train_sampler = SubsetRandomSampler(train_indices)\n","    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n","    \n","    val_sampler = SubsetRandomSampler(val_indices)\n","    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n","\n","    return train_loader, val_loader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I_YuWdHc99TW"},"source":["train, val = fetch_data(train_path, val_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uS_DWhQ9CTG"},"source":["vocab = make_vocab(train)\n","vocab, word2index, index2word = make_indices(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yG7pKH07ClOv"},"source":["train_loader, val_loader = get_data_loaders(train, val, batch_size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7GPWuoQVzgT"},"source":["# Note: Colab has 12 hour limits on GPUs, also potential inactivity may kill the notebook. Save often!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dSKmtKpXbk8_"},"source":["## 1.1 FFNN Implementation"]},{"cell_type":"markdown","metadata":{"id":"Zqv3EwKyaGaN"},"source":["### 1.1 Task\n","Assume that an onmiscient oracle has told you there are **4 fundamental errors** in the **FFNN** implementation. They may be anywhere in the section below (everything below until the start of section 1.2) unless otherwise indicated. Your objective is to _find_ and _fix_ each of these errors and to include in the report a description of the original error along with the fix. To help your efforts, the oracle has provided you with additional information about the properties of the errors as follows:\n","\n","* _Correctness_ \\\n","Each error causes the code to be strictly incorrect. There is absolutely no ambiguity that the errant code (or missing code) is incorrect. This means errors are not due to the code being inefficient (in run-time or in memory).\n","\n","* _Localized_ \\\n","Each error can be judged to be erroneous by strictly looking at the code (along with your knowledge of machine learning as taught through this course). The errors therefore are not due to the model being uncompetitive in terms of performance with state-of-the-art performance for this task nor are they due to the amount of data being insufficient for this task in general.\n","\n","* _General_ \\\n","Each error is general in nature. They will not be triggered only by the model receiving a pathological input, i.e. they will not be something that is triggered specifically when inputting some phrase such as \"roman fool\" or word such as \"sword\".\n","\n","* _Fundamental_ \\\n","Each error is a fundamental failure in terms of doing what is intended. This means that errors do not hinge on nuanced understanding of specific PyTorch functionality. This also means they will not exploit properties of the dataset in\n","a subtle way that could only be realized by someone who has comprehensively studied the data.\n","\n","The bottom line: the errors should be fairly obvious. The oracle further reminds you that performance/accuracy of the (resulting) model should not be how you ensure you have debugged successfully. For example, if you correct some, but not all, of the errors, the remaining errors may mask the impact of your fixes. Further, performance is not guaranteed to improve by fixing any particular error. Consider the case where the training set is also employed as the test set; performance will be very high but there is something very wrong. And fixing the problem will reduce performance.\n","In fixing each error, the oracle provides some further insight about the fixes:\n","\n","* _Minimal_ \\\n","A reasonable fix for each error can be achieved in < 5 lines of code being changes. We do not require you to make fixes of 4 or fewer lines, but it should be a cause for concern if your fixes are far more elaborate.\n","\n","* _Ill-posed_ \\\n","While the errors are unambiguous, the method for fixing them is under-specified: You are free to implement any reasonable fix and all such fixes will equally receive full credit."]},{"cell_type":"code","metadata":{"id":"fHbr7ltPADIu"},"source":["# Lambda to switch to GPU if available\n","get_device = lambda : \"cuda:0\" if torch.cuda.is_available() else \"cpu\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EsNf3SOHcbCa"},"source":["unk = '<UNK>'\n","\n","# Consult the PyTorch documentation for information on the functions used below:\n","# https://pytorch.org/docs/stable/torch.html\n","\n","class FFNN(nn.Module):\n","\tdef __init__(self, input_dim, h, output_dim):\n","\t\tsuper(FFNN, self).__init__()\n","\t\tself.h = h\n","\t\tself.W1 = nn.Linear(input_dim, h)\n","\t\tself.activation = nn.ReLU() # The rectified linear unit; one valid choice of activation function\n","\t\t# ERROR 1 on line below: error in size of dimensions, should be h x output dim but is h x h\n","\t\tself.W2 = nn.Linear(h, output_dim) \n","    # The below two lines are not a source for an error\n","\t\tself.softmax = nn.LogSoftmax(dim=1) # The softmax function that converts vectors into probability distributions; computes log probabilities for computational benefits\n","\t\tself.loss = nn.NLLLoss() # The cross-entropy/negative log likelihood loss taught in class\n","\n","\tdef compute_Loss(self, predicted_vector, gold_label):\n","\t\treturn self.loss(predicted_vector, gold_label)\n","\n","\tdef forward(self, input_vector):\n","\t\t# The z_i are just there to record intermediary computations for your clarity\n","\t\tz1 = self.W1(input_vector)\n","\t\t# ERROR 2 on line below: Fixed the error of no activation function on the first layer of the nn\n","\t\tz2 = self.activation(z1) \n","\t\tz3 = self.W2(z2)\n","\t\tpredicted_vector = self.softmax(z3)\n","\t\treturn predicted_vector\n","\t\n","\tdef load_model(self, save_path):\n","\t\tself.load_state_dict(torch.load(save_path))\n","\t\n","\tdef save_model(self, save_path):\n","\t\ttorch.save(self.state_dict(), save_path)\n","\n","\n","def train_epoch(model, train_loader, optimizer):\n","\tmodel.train()\n","\ttotal = 0\n","\tloss = 0\n","\tcorrect = 0\n","\tfor (input_batch, expected_out) in tqdm(train_loader, leave=False, desc=\"Training Batches\"):\n","\t\texpected_out = torch.tensor([int(i) for i in expected_out])\n","\t\toutput = model(input_batch.to(get_device()))\n","\t\ttotal += output.size()[0]\n","\t\t_, predicted = torch.max(output, 1)\n","\t\tcorrect += (expected_out == predicted.to(\"cpu\")).cpu().numpy().sum()\n","\n","\t\t# ERROR 3 on line below: We need to zero out the gradients before we call our loss function so our gradients on EACH batch is accuracte \n","\t\toptimizer.zero_grad()  \n","\t\tloss = model.compute_Loss(output, expected_out.to(get_device()))\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\t# TODO: You'll want to print accuracy here\n","\tacc = correct/total\n","\tprint(\"Tain accuracy: {:.4f}.\\tLoss: {:.4f}\".format(acc, loss.item()))\n"," \n","\treturn\n","\n","\n","def evaluation(model, val_loader, optimizer):\n","\tmodel.eval()\n","\tloss = 0\n","\tcorrect = 0\n","\ttotal = 0\n","\tfor (input_batch, expected_out) in tqdm(val_loader, leave=False, desc=\"Validation Batches\"):\n","\t\texpected_out = torch.tensor([int(i) for i in expected_out])\n","\t\toutput = model(input_batch.to(get_device()))\n","\t\ttotal += output.size()[0]\n","\t\t_, predicted = torch.max(output, 1)\n","\t\tcorrect += (expected_out.to(\"cpu\") == predicted.to(\"cpu\")).cpu().numpy().sum()\n","\n","\t\tloss += model.compute_Loss(output, expected_out.to(get_device()))\n","\tloss /= len(val_loader)\n","\t# TODO: You'll want to print some validation metrics here\n","\tacc = correct/total\n","\tprint(\"Tain accuracy: {:.4f}.\\tLoss: {:.4f}\".format(acc, loss.item()))\n"," \n","\tpass\n","\n","def train_and_evaluate(number_of_epochs, model, train_loader, val_loader):\n","\toptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\tfor epoch in trange(number_of_epochs, desc=\"Epochs\"):\n","\t\t# ERROR 4 on line below: We need to train on the training set, not the validation set\n","\t\ttrain_epoch(model, train_loader, optimizer) \n","\t\twith torch.no_grad():\n","\t\t\tevaluation(model, val_loader, optimizer)\n","\treturn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qFrrcE3CVxzX"},"source":["h = 512\n","model = FFNN(len(vocab), h, len(language_to_idx)).to(get_device())\n","train_and_evaluate(2, model, train_loader, val_loader)\n","model.save_model(\"ffnn_fixed.pth\") # Save our model!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZqpEPzW_lXX"},"source":["# Example of how to load\n","loaded_model = FFNN(len(vocab), h, len(language_to_idx))\n","loaded_model.load_model(\"ffnn_fixed.pth\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XxW8Xgq6YbPE"},"source":["## 1.2 Part 1 Report\n","Please include a description of the error, a description of your fix, and a python comment in the code indicating where fix is for each of the 4 errors. The python comment will look something along the lines of \"# Error 1 fix on line below\". As a convenience, the template for the google doc write up is [here](https://docs.google.com/document/d/16scmrRva5WczB4a4QDYhFJBXaaAJd8oesEEAdi6ehbY/edit)\n","\n","### Error 1:\n","Your answer here.\n","\n","### Error 2:\n","Your answer here.\n","\n","### Error 3:\n","Your answer here.\n","\n","### Error 4:\n","Your answer here."]},{"cell_type":"markdown","metadata":{"id":"kb4Tc3Fe2Shr"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"bD0pgD0Jh84T"},"source":["# Part 2: Miscellaneous\n","List any additional libraries you used and sources you referenced and cited (labelled with the section in which you referred to them). Include a description of how your group split\n","up the work. Include brief feedback on this asignment."]},{"cell_type":"markdown","metadata":{"id":"snL2qqbR3SbN"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"4VeQ8uBwiR-a"},"source":["**Each section must be clearly labelled, complete, and the corresponding pages should be correctly assigned to the corresponding Gradescope rubric item.** If you follow these steps for each of the 4 components requested, you are guaranteed full credit for this section. Otherwise, you will receive no credit for this section."]}]}