{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS4740_FA21_p1_nar73_krm74.ipynb","provenance":[{"file_id":"1gcXCC8nZ-kMSe5H0QRqi5KSwxrzf7X17","timestamp":1631738843225},{"file_id":"1m49rLA37JMbWPMZ4GoubQ6D-POZkSKt5","timestamp":1630699782490}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6d69f00c58ba411a96369435f163e103":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8bc18707871046c798031eaa43f227d6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_365329d4dbf4422385954a9e9e7cb498","IPY_MODEL_bb3c5597091a4744bf62873cbc5f8b21","IPY_MODEL_ab4e0ad833c44fb183e7687972806028"]}},"8bc18707871046c798031eaa43f227d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"365329d4dbf4422385954a9e9e7cb498":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6e5c12a04fca4395993e75efd5297f59","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 95%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cfa2542e580f41b882764f9a4e322cad"}},"bb3c5597091a4744bf62873cbc5f8b21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b71d4adcf50a4f9f9c12d20306829f42","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":642,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":642,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c07c12062e8463b90e2c7beff73eb38"}},"ab4e0ad833c44fb183e7687972806028":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_948b05a3a71e4dfc8330d2e384d9725e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 613/642 [00:01&lt;00:00, 499.21it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_884865fc8427483ba50727040d8cafe7"}},"6e5c12a04fca4395993e75efd5297f59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cfa2542e580f41b882764f9a4e322cad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b71d4adcf50a4f9f9c12d20306829f42":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0c07c12062e8463b90e2c7beff73eb38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"948b05a3a71e4dfc8330d2e384d9725e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"884865fc8427483ba50727040d8cafe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e949e1caf6254f30b55eff2464f3b9bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6d0ae4508b5a487fa1d7cdc4c120ef83","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_454345fc49514174abdcce7986cd337f","IPY_MODEL_04b7fcfdcc5245c28eb80dd1de2080e0","IPY_MODEL_b93565bb66564b3b82a0a6e15be448fe"]}},"6d0ae4508b5a487fa1d7cdc4c120ef83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"454345fc49514174abdcce7986cd337f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2ecc76489ebc437db68d139c006793e6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 99%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5c66d91dbb142ee8eca5b7284fe7a0a"}},"04b7fcfdcc5245c28eb80dd1de2080e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_22cfbe3b20f44a4a9bd7ea1134ff7a58","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":638,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":638,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d41ce856ca724c50ab57978ff489bc48"}},"b93565bb66564b3b82a0a6e15be448fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f50248646d3d4abcad6929bd7c82dcaf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 630/638 [00:01&lt;00:00, 561.14it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5e2c7b41dd0b4149978633afc26a0b78"}},"2ecc76489ebc437db68d139c006793e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b5c66d91dbb142ee8eca5b7284fe7a0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"22cfbe3b20f44a4a9bd7ea1134ff7a58":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d41ce856ca724c50ab57978ff489bc48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f50248646d3d4abcad6929bd7c82dcaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5e2c7b41dd0b4149978633afc26a0b78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bf796efb3f6b4bae81460d21e35adf02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_119b33bd8d0247c89b11b1e00c2d82c6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_33b2c8945c7644c6bbf1da688bb7b514","IPY_MODEL_2a8f5cff409a47fcbf1c4e2092c4ba02","IPY_MODEL_a959bb96ba6440f8a1ca6129ebb997bc"]}},"119b33bd8d0247c89b11b1e00c2d82c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33b2c8945c7644c6bbf1da688bb7b514":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9a64159ee4a04f49b09730ce6ec91fdd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 62%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a319f323f28941eeb2f16e89b41a5222"}},"2a8f5cff409a47fcbf1c4e2092c4ba02":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a8e95492d76b4e8eb7325425ae8f6969","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":80,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":80,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e7b7a591df7448779479093d5168f5ef"}},"a959bb96ba6440f8a1ca6129ebb997bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8fb3eed84ab94448b85d6baf42c64ebf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 50/80 [00:00&lt;00:00, 285.20it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f58b573d961541d4a360dbc62e608061"}},"9a64159ee4a04f49b09730ce6ec91fdd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a319f323f28941eeb2f16e89b41a5222":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a8e95492d76b4e8eb7325425ae8f6969":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e7b7a591df7448779479093d5168f5ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8fb3eed84ab94448b85d6baf42c64ebf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f58b573d961541d4a360dbc62e608061":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"317e57f8d08a4305b2f5da3182b612cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_675c8d70840b4cd2a37651ceb7c444d2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dde230f970d24f1095d28cb14503749d","IPY_MODEL_cf9aecec91244f18bb0fc86d2197a039","IPY_MODEL_1294f3169ea340f580ec8a50567ff419"]}},"675c8d70840b4cd2a37651ceb7c444d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dde230f970d24f1095d28cb14503749d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_70c145e2bbff49f99f15e2e319bcf0a4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 79%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f2bf5dd834c4d96b7fa0a52c24ff7f2"}},"cf9aecec91244f18bb0fc86d2197a039":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3c6423927d6b40cfb92760a1f1e74f18","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":80,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":80,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2f6daca854174629ac4544a8aac01db5"}},"1294f3169ea340f580ec8a50567ff419":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ea024ac019d149cf80b4d77fc3b8deed","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 63/80 [00:00&lt;00:00, 331.52it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3e7766014abb4c3385bcc61762dd702e"}},"70c145e2bbff49f99f15e2e319bcf0a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0f2bf5dd834c4d96b7fa0a52c24ff7f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3c6423927d6b40cfb92760a1f1e74f18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2f6daca854174629ac4544a8aac01db5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ea024ac019d149cf80b4d77fc3b8deed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3e7766014abb4c3385bcc61762dd702e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3f24d50b247b4b68b0ef2b3adc8c0de8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ae4414a60d94400da5873cd1645d6770","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c0ae3b1fe227494fa47574308acdf6bc","IPY_MODEL_88cc7fe8c59e496d824b66e481650593","IPY_MODEL_15a3ac7fab2446548ebe8db6fdc12805"]}},"ae4414a60d94400da5873cd1645d6770":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c0ae3b1fe227494fa47574308acdf6bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2f25bf8b015649419ebc14b28b92c543","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 74%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75b2b41582374a1bb8771577b3e3a448"}},"88cc7fe8c59e496d824b66e481650593":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_81573ae57b394d4ba761db8877fec9df","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":160,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":160,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dd2ede3c34f04ee1ad1a688f8f157b94"}},"15a3ac7fab2446548ebe8db6fdc12805":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_417aa1d6c74645c7852a905d3bf2b666","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 118/160 [00:00&lt;00:00, 430.04it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_46d386a397b34da4baa4408dc0862a9a"}},"2f25bf8b015649419ebc14b28b92c543":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75b2b41582374a1bb8771577b3e3a448":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"81573ae57b394d4ba761db8877fec9df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dd2ede3c34f04ee1ad1a688f8f157b94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"417aa1d6c74645c7852a905d3bf2b666":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"46d386a397b34da4baa4408dc0862a9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"53e5d618b61d4d188cd2d86fd741b544":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d2a87bc19ae94691a1a84abb7c068002","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d686711e9692482f80dfb87b5c5a675a","IPY_MODEL_a7eb7546aee847adbffd56d47b691961","IPY_MODEL_ce93c2b3b5c0471f84672d2bebe7b351"]}},"d2a87bc19ae94691a1a84abb7c068002":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d686711e9692482f80dfb87b5c5a675a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_640ebfcb4fc64020a9b2ff94e88a3dd2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 76%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bc9c21444bc14975824e5b01cc6e526d"}},"a7eb7546aee847adbffd56d47b691961":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_58637e1de1bf4bc68b39eb6634b9c6eb","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":160,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":160,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a4b51e32395c497ab5db608975390b88"}},"ce93c2b3b5c0471f84672d2bebe7b351":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7a93d1d4162849e9b844836172224578","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 122/160 [00:00&lt;00:00, 443.73it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_13419bade2334a8cb44b5c4a2efd08eb"}},"640ebfcb4fc64020a9b2ff94e88a3dd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bc9c21444bc14975824e5b01cc6e526d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"58637e1de1bf4bc68b39eb6634b9c6eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a4b51e32395c497ab5db608975390b88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a93d1d4162849e9b844836172224578":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"13419bade2334a8cb44b5c4a2efd08eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"tMR2in3Elmb0"},"source":["# Project 1: Language Modeling and Fake Review Classification"]},{"cell_type":"markdown","metadata":{"id":"t-ArOW3FXbdT"},"source":["Names: Lusca Robinson, Kyrus Mama\n","\n","Netids: nar73, krm74\n"]},{"cell_type":"markdown","metadata":{"id":"Nw061uMf-vyu"},"source":["**After you make your own copy, please rename this notebook by clicking on it's name in the upper left corner.** It should be named: CS4740_FA21_p1_netid1_netid2\n","\n","Don't forget to share your newly copied notebook with your partner!"]},{"cell_type":"markdown","metadata":{"id":"9e1Dnqk6GmdH"},"source":["**Reminder: both of you can't work in this notebook at the same time from different computers/browser windows because of sync issues. We even suggest to close the tab with this notebook when you are not working on it so your partner doesn't get sync issues.**\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"sDCg0sniPJGy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVNUIk8d-oBn"},"source":["## Introduction\n","In this project we will build an **n-gram-based language model** for deceptive review classification. We will also investigate a feature-based **Naive Bayes model**. The task we are faced with is to **decide whether a hotel review is deceptive or truthful**. This is a relavent problem as websites that contain consumer reviews are a target of opinion spam. Typically, these deceptive opinions are neither easily ignored nor even identifiable by a human reader so we'd like to assist in flagging reviews. The dataset we are investigating looks at *deceptive opinion spam*, that is decetive opinions that have been purposely written to sound genuine ([Ott et al](https://arxiv.org/pdf/1107.4557.pdf)).\n","\n","To help us approach this problem, we will use NLP techniques covered thus far to frame this as a (supervised) binary classification task, where each opinion will have a label $y \\in \\{0,1\\}$, where *0 indicates a truthful review* and *1 indicates a deceptive one*. You will train and validate your two different models and then run them on a test data set with hidden $y$ labels. You will then submit the results on the test data set to Kaggle to participate in our class-wide competition!\n","\n","The project is divided into six parts:\n","1. Dataset loading and preprocessing\n","2. Unsmoothed n-gram language model (LM): build the unsmoothed n-gram language model using our Fake Review corpus. \n","3. Smoothed n-gram language model: build a smoothed version of the model from part 2.\n","4. Perplexity: compute perplexity for both the unsmoothed and smoothed model\n","5. Putting everything together and submitting the first model to Kaggle\n","6. Naive Bayes: build a feature-based Naive Bayes model to perform the same classification task. Compare the LM with Naive Bayes and identify the pros and cons of each."]},{"cell_type":"markdown","metadata":{"id":"8YxRcxlf1rqT"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"n5dUybGbhQXQ"},"source":["## Logistics (IMPORTANT!)\n","- You should work in **groups of 2 students**. Students in the same group will get the same grade. Thus, you should make sure that everyone in your group contributes to the project. \n","- **Remember to form groups on BOTH CMS and Gradescope** or not all group members will receive grades. You can use make a post on EdStem to find a partner for this project.\n","- Please complete the written questions of this notebook in a clear and informative way. We have created a template document for you to answer the written questions. This document can be found [here](https://docs.google.com/document/d/11GX5vG8TeHk1F2eakOgbTFaqYl3lfvZf9fYoMUTVGMs/edit?usp=sharing). Please make a copy of this document for yourself and add your names and netids in the header and answer the written questions on it. You will need to submit this document to gradescope as well (do not forget to do this please!).\n","- At the end: please make sure to submit the following 3 items:\n","  1. PDF version of Colab notebook on Gradescope (instructions for converting to PDF are at the end).\n","  2. PDF version of Google Doc with written answers on Gradescope.\n","  3. .ipynb version of your colab notebook on CMS.\n","\n","**Advice:** The written questions is where you get to show us that you understand not only what you are doing but also why and how you are doing it. So be clear, organized and concise; avoid vagueness and excess verbiage. Spend time doing error analysis for the models. This is how you understand the advantages and drawbacks of the systems you build. It's also useful to think about how the theory of n-grams/Naive Bayes bridges with the real world application we are building. Think about what you expect from these models based on your current understanding, and then see if your expectation aligns with empirical results that you'll get. "]},{"cell_type":"markdown","metadata":{"id":"HwgqdDbTPVeA"},"source":["## General Guidelines\n","In this project, we provide a few code snippets or starter points in case you need them. You DO NOT need to follow the structure. \n","\n","If you think you have a better idea, go for it. You can ADD, MODIFY, or DELETE any code snippets given to you.\n","\n","You are expected to use functions or classes to organize your code. A portion of the grade is regarding code cleanliness / readability and applying these models in the real world means we need to collaborate with others (ie. other people should be able to read your code and run it)!\n","\n","To help with debugging and testing, you should use this example from class [09-02 Thurs - lec3: N-gram models](https://edstem.org/us/courses/12801/resources) as your training corpus:\n","\n","```\n","<s> I see what I eat and I eat what I see.\n","```\n","\n","The test sentence you can use also comes from class:\n","```\n","I see what\n","```\n","\n","**Let's do this** 🚀"]},{"cell_type":"markdown","metadata":{"id":"LLmJ-2h5pr6n"},"source":["### Dataset\n","\n","You are given a **Review Corpus** on CMS, which consists of roughly the same amount of real and fake reviews.\n","\n","Real review example:\n","```\n","Stayed with a group for a bachelorette party, and was disappointed. The hotel is beautiful, the staff was all rather friendly. The main problem was the room/sleeping situation. We had booked rooms with 2 queen beds several weeks before, but received an email a few days before our visit stating they were sold out (how that happens I don't know!!) so they \"upgraded\" us to two \"suites\" with a king and a pull out. First, this meant our party was split up and on different floors. Second, that meant two of us were stuck on a pull out couch. :( I'm not a picky, unreasonable person, but that was the WORST \"bed\" I've ever slept on! It was sunken in the middle so we literally rolled into each other unless we balanced ourselves on the very edge of the bed. Then there were the springs poking into our backs ALL night! Just awful! For the amount of money we spent I expected to be comfortable! I would not stay here again after this experience.\n","```\n","\n","Fake review example:\n","```\n","I truly enjoyed my stay at the Omni Chicago Hotel. We stayed in a suite, which was clean and extremely nice, at a very reasonable rate. My husband and I spent quite a bit of time in the indoor pool, but personally I preferred laying out on the sundeck. Service was excellent; they were friendly and all of our needs were met promptly. I would definitely recommend this hotel to anyone looking to have a great experience in the downtown Chicago area.\n","```\n","\n","In the dataset folder you should find 2 files, training and validation splits for both real and fake reviews.\n","\n","The project will proceed generally as follows in terms of code development:\n","1. Write code to train unsmoothed unigram and bigram language models for an arbitrary corpus\n","2. Implement smoothing and unknown word handling. \n","3. Implement the Perplexity calculation. \n","4. Using 1, 2 and 3, together with the provided training and validation sets, develop a language-model-based approach for Fake Review Classification.\n","5. Apply your best language-model-based review classifier (from 4) to the\n","provided test set. Submit the results to the online Kaggle competition. \n","6. Use any existing implementation of Naive Bayes (and the provided training and validation sets) to create an additional Naive Bayes fake review classifier. Apply your best NB classifier to the provided test set. Submit the results to the separate Kaggle competition (for NB classifiers). \n","\n","We will progress towards these tasks throughout this notebook."]},{"cell_type":"markdown","metadata":{"id":"b2O1icKLpIdF"},"source":["# Part 1: Preprocessing the Dataset\n","In this part, you are going to do a few things:\n","* Connect to the google drive where the data set is stored\n","* Load and read files\n","* Preprocess the text\n","\n","------\n","**Please upload the dataset to each partner's individual Google Drive now.** We suggest using the same folder structure within Google Drive because the notebook is shared among you, so the code to load the data would have to be changed every time if folder structures are different. One folder structure might be: Google Drive/CS 4740/Project 1/Dataset/ or whatever works for you. See our code below for an example of how we load the data from Google Drive."]},{"cell_type":"markdown","metadata":{"id":"zqJXHkfirHCX"},"source":["## 1.1 Connect to google drive"]},{"cell_type":"code","metadata":{"id":"og--UAtbrP4-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632422355083,"user_tz":240,"elapsed":22323,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"df5eae60-a3c8-4513-99ff-3a759a8c4ea3"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"yspKotLCr8kj"},"source":["## 1.2 Load and read files\n","First, let's install [NLTK](https://www.nltk.org/), a very widely package for NLP preprocessing (and other tasks) for Python."]},{"cell_type":"code","metadata":{"id":"HyUwo9CthoiW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632422366406,"user_tz":240,"elapsed":7139,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"91d77f85-22ed-4f98-800b-4906ccbb1d72"},"source":["!pip install -U nltk tqdm"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Collecting nltk\n","  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.2)\n","Collecting tqdm\n","  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Installing collected packages: tqdm, nltk\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.62.2\n","    Uninstalling tqdm-4.62.2:\n","      Successfully uninstalled tqdm-4.62.2\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.6.3 tqdm-4.62.3\n"]}]},{"cell_type":"markdown","metadata":{"id":"UNdLcCH9hpYX"},"source":["Then we read and load data."]},{"cell_type":"code","metadata":{"id":"CDIzMEQ0sMqK","colab":{"base_uri":"https://localhost:8080/","height":52,"referenced_widgets":["6d69f00c58ba411a96369435f163e103","8bc18707871046c798031eaa43f227d6","365329d4dbf4422385954a9e9e7cb498","bb3c5597091a4744bf62873cbc5f8b21","ab4e0ad833c44fb183e7687972806028","6e5c12a04fca4395993e75efd5297f59","cfa2542e580f41b882764f9a4e322cad","b71d4adcf50a4f9f9c12d20306829f42","0c07c12062e8463b90e2c7beff73eb38","948b05a3a71e4dfc8330d2e384d9725e","884865fc8427483ba50727040d8cafe7","e949e1caf6254f30b55eff2464f3b9bd","6d0ae4508b5a487fa1d7cdc4c120ef83","454345fc49514174abdcce7986cd337f","04b7fcfdcc5245c28eb80dd1de2080e0","b93565bb66564b3b82a0a6e15be448fe","2ecc76489ebc437db68d139c006793e6","b5c66d91dbb142ee8eca5b7284fe7a0a","22cfbe3b20f44a4a9bd7ea1134ff7a58","d41ce856ca724c50ab57978ff489bc48","f50248646d3d4abcad6929bd7c82dcaf","5e2c7b41dd0b4149978633afc26a0b78","bf796efb3f6b4bae81460d21e35adf02","119b33bd8d0247c89b11b1e00c2d82c6","33b2c8945c7644c6bbf1da688bb7b514","2a8f5cff409a47fcbf1c4e2092c4ba02","a959bb96ba6440f8a1ca6129ebb997bc","9a64159ee4a04f49b09730ce6ec91fdd","a319f323f28941eeb2f16e89b41a5222","a8e95492d76b4e8eb7325425ae8f6969","e7b7a591df7448779479093d5168f5ef","8fb3eed84ab94448b85d6baf42c64ebf","f58b573d961541d4a360dbc62e608061","317e57f8d08a4305b2f5da3182b612cc","675c8d70840b4cd2a37651ceb7c444d2","dde230f970d24f1095d28cb14503749d","cf9aecec91244f18bb0fc86d2197a039","1294f3169ea340f580ec8a50567ff419","70c145e2bbff49f99f15e2e319bcf0a4","0f2bf5dd834c4d96b7fa0a52c24ff7f2","3c6423927d6b40cfb92760a1f1e74f18","2f6daca854174629ac4544a8aac01db5","ea024ac019d149cf80b4d77fc3b8deed","3e7766014abb4c3385bcc61762dd702e"]},"executionInfo":{"status":"ok","timestamp":1632424095706,"user_tz":240,"elapsed":3650,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"b1d8ab8d-4752-4485-f63c-56c618532729"},"source":["import os\n","import csv\n","import io\n","import math\n","from nltk import word_tokenize, sent_tokenize\n","import nltk\n","from tqdm.notebook import tqdm\n","nltk.download('punkt')\n","\n","root_path = os.path.join(os.getcwd(), \"drive\", \"My Drive/CS 4740/Project 1\") # replace based on your Google drive organization\n","dataset_path = os.path.join(root_path, \"Dataset\") # same here\n","\n","real_review_train = []\n","real_review_validation = [] #loop through each word in list. Find wrd in dictionary. Compute probabilitiy of word\n","fake_review_train = []\n","fake_review_validation = []\n","\n","def load_real_fake_dataset(dataset_path, filename):\n","    real = []\n","    fake = []\n","    with open(os.path.join(dataset_path, filename)) as fp:\n","        csvreader = csv.reader(fp, delimiter=\"|\")\n","        for txt, label in csvreader:\n","            label = int(label)\n","            if label:\n","                fake.append(txt)\n","            else:\n","                real.append(txt)\n","    \n","    return real, fake\n","\n","real_review_train, fake_review_train = load_real_fake_dataset(dataset_path, \"P1_real_fake_review_train.txt\")\n","\n","real_review_validation, fake_review_validation = load_real_fake_dataset(dataset_path, \"P1_real_fake_review_val.txt\")\n","\n","\n","def tokenize_reviews(reviews):\n","    return [\n","        [\n","            word.lower() for sent in sent_tokenize(review)\n","            for word in word_tokenize(sent)\n","        ]\n","        for review in tqdm(reviews, leave=False)\n","    ]\n","\n","tokenized_real_review_training = tokenize_reviews(real_review_train)\n","tokenized_fake_review_training = tokenize_reviews(fake_review_train)\n","tokenized_real_review_validation = tokenize_reviews(real_review_validation)\n","tokenized_fake_review_validation = tokenize_reviews(fake_review_validation)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d69f00c58ba411a96369435f163e103","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/642 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e949e1caf6254f30b55eff2464f3b9bd","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/638 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf796efb3f6b4bae81460d21e35adf02","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/80 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"317e57f8d08a4305b2f5da3182b612cc","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/80 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"miP2sevQbPbs"},"source":["Sanity checks for our real and fake training sets"]},{"cell_type":"code","metadata":{"id":"BP8TzVmBj95T"},"source":["tokenized_real_review_training[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8K3DBwDmenY"},"source":["tokenized_fake_review_training[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ok_0NzjkDrde"},"source":["## 1.3 Data Preprocessing & Preparation\n","\n","There's a well-known parable in machine learning that 80% of the work is all about data preparation, 10% is supporting infrastructure and 10% is actual modeling. If your \"raw\" dataset is not preprocessed and prepared in a way to maximize its value, then your model will be more like this: https://xkcd.com/1838/. For this project, modeling is the star of the show for learning purposes, but we still want you to pay attention to the preprocessing stage.\n","\n","*We've already tokenized and lowercased* the raw data for you. We have not added a start of sentence token but feel free to do so (it is not neccessary). Here are a few extra things you might want to do:\n","\n","- Think about edge cases. For example, you don't want to accidentally append a period to the last word of a sentence. \n","- Watch out for apostrophes and other tricky things like quotations, they cause lots of edge cases. For example, \"they're\" can be all one token, or two tokens (\"they\", \"'re\") or even three tokens (\"they\", \" ' \", \"re\"). \n","\n","Why did we lowercase all tokens? Because the computer will otherwise consider \"The\" and \"the\" as two separate words and this will cause problems.\n","\n","Note that you may use existing\n","tools just for the purpose of preprocessing. \n","\n","Advice: don't get bugged down in the dozens of preprocessing packages and suggestions that you can find on Towards Data Science or Stack Overflow. Start with this [NLTK tutorial](https://lost-contact.mit.edu/afs/cs.pitt.edu/projects/nltk/docs/tutorial/introduction/nochunks.html#:~:text=The%20Natural%20Language%20Toolkit%20(NLTK,tokenization%2C%20tagging%2C%20and%20parsing.) and that should be plenty."]},{"cell_type":"code","metadata":{"id":"Pf2phKs8bvaB","executionInfo":{"status":"ok","timestamp":1632424100957,"user_tz":240,"elapsed":259,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["# TODO: preprocessing\n","def preprocessing(processList):\n","  for reviewEntry in processList:\n","    i=0\n","    while i < len(reviewEntry):\n","      if reviewEntry[i] == \"n't\":\n","        reviewEntry[i-1] = reviewEntry[i-1] + reviewEntry[i]\n","        reviewEntry.pop(i)\n","      elif reviewEntry[i][0] == \"'\":    \n","        reviewEntry.pop(i)\n","      elif \".\" in reviewEntry[i] and len(reviewEntry[i]) > 1 and reviewEntry[i] != \"...\":\n","        temp = reviewEntry.pop(i)\n","        stopLoc = temp.index(\".\")\n","        if stopLoc > 0:\n","          reviewEntry.insert(i, temp[0:stopLoc])\n","        reviewEntry.insert(i+1, \".\")\n","        if stopLoc+1 < len(temp):\n","          reviewEntry.insert(i+2, temp[stopLoc+1:])\n","      elif reviewEntry[i] in [\".\", \"?\", \"!\", \"...\"]:\n","           reviewEntry.insert(i+1, \"SOS\")\n","           i += 2\n","      elif i == 0:\n","        reviewEntry.insert(i,\"SOS\") \n","        i += 1\n","      elif reviewEntry[i].isnumeric():\n","        reviewEntry[i] = \"NUMBER\"\n","      # elif re.match(\"^[,^%$#@*()[]{}\\/]+$\", reviewEntry[i])\n","      else:\n","        i+=1\n","\n","preprocessing(tokenized_real_review_training)\n","preprocessing(tokenized_fake_review_training)\n","preprocessing(tokenized_real_review_validation)\n","preprocessing(tokenized_fake_review_validation)\n","\n"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEab9h6xpuhx"},"source":["**Q1.1: Show some observations or statistics from the dataset** (should be quantitative – i.e. most frequent words, most frequent bigram, etc.) You may do the computations for your graphs/statistics on the colab notebook, however, please mmake sure you transfer all your work (statistics, graphs, snapshots of thh code if needed) to the Google Doc!\n","\n","Please answer on your writeup doc!"]},{"cell_type":"code","metadata":{"id":"9MQq0uNnqWmR","executionInfo":{"status":"ok","timestamp":1632431054626,"user_tz":240,"elapsed":816,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["# TODO: observations/statistics\n","word_count = {}\n","bigram_count = {}\n","\n","def include_words(processList, word_count_dict={}):\n","  for reviewEntry in processList:\n","      for word in reviewEntry:\n","        if word in word_count_dict:\n","          word_count_dict[word] += 1\n","        else:\n","          word_count_dict[word] = 1\n","\n","def include_bigrams(processList, word_count_dict={}):\n","  for reviewEntry in processList:\n","      for i in range(len(reviewEntry)-1):\n","        key = (reviewEntry[i], reviewEntry[i+1])\n","        if key in word_count_dict:\n","          word_count_dict[key] += 1\n","        else:\n","          word_count_dict[key] = 1\n","\n","include_words(tokenized_real_review_training, word_count)\n","include_words(tokenized_fake_review_training, word_count)\n","\n","real_word_count = {}\n","fake_word_count = {}\n","include_words(tokenized_real_review_training, real_word_count)\n","include_words(tokenized_fake_review_training, fake_word_count)\n","\n","include_bigrams(tokenized_real_review_training, bigram_count)\n","include_bigrams(tokenized_fake_review_training, bigram_count)\n","\n","real_bigram_count = {}\n","fake_bigram_count = {}\n","include_bigrams(tokenized_real_review_training, real_bigram_count)\n","include_bigrams(tokenized_fake_review_training, fake_bigram_count)\n","\n","all_unigrams = set(real_word_count.keys()) | set(fake_word_count.keys())\n","\n","all_bigrams = set(real_bigram_count.keys()) | set(fake_bigram_count.keys())\n","\n","# tokens = word_count.keys()\n","# freq = [word_count[key] for key in tokens]\n","\n","# tog = zip(freq, tokens)\n","# sor = sorted(tog)\n","# print(sor)\n","\n","# tokens = bigram_count.keys()\n","# freq = [bigram_count[key] for key in tokens]\n","\n","# tog = zip(freq, tokens)\n","# sor = sorted(tog)\n","# print(sor)"],"execution_count":76,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rcw8M5nesOQD"},"source":["**Please answer the following question**:\n","\n","**Q1.2: What did you do in your preprocessing part?**\n","\n","Example answer format:\n","\n","A: We tokenized and lowercased all the words."]},{"cell_type":"markdown","metadata":{"id":"zQVi0l76MBcs"},"source":["Please answer on your writeup doc!"]},{"cell_type":"markdown","metadata":{"id":"d8oo9zHrsYBD"},"source":["# Part 2: Compute Unsmoothed Language Models.\n","\n","To start, you will write a program that computes unsmoothed unigram and bigram probabilities. You should consider real and deceptive reviews as separate corpora and\n","generate a separate language model for each set of reviews.\n","We have already loaded the data and (partially) preprocessed it and you probably did some of your own preprocessing. \n","\n","Note that you were allowed to use existing\n","tools for the purpose of preprocessing, but you must write the code for gathering n-gram counts and computing n-gram probabilities yourself. \n","\n","For example, consider the\n","simple corpus consisting of the sole sentence:\n","\n","\n","> the students liked the class\n","\n","Part of what your program would compute for a unigram and bigram model, for example,\n","would be the following:\n","\n","\n","> $P(\"the\") = 0.4; P(\"liked\") = 0.2; P(\"the\"|\"liked\") = 1.0; P(\"students\"|\"the\") = 0.5$\n","\n","Remember to add a symbol to mark the beginning of sentence. See Sept. 2nd lecture, p25-28 for an example.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IgdpbCKw8B0r"},"source":["**Advice**: jupyter notebooks (including colab) can be a double-edged sword. It's amazing and liberating to just start writing code and run it by simply running a cell. However, it gets messy very quickly. So, once you're done prototyping, you should be using functions (classes may be unnecessary but go for it if you want) to make things cleaner and easier to debug."]},{"cell_type":"markdown","metadata":{"id":"MqkMkZIkst8i"},"source":["## 2.1 Unsmoothed Uni-gram Model.\n","\n","In this part of the project, you are trying to compute the probabilities for a unigram model. You might want to take in a list of words, and return the probabilities for each\n","occurence. Think of an efficient data structure to use here given what ratio of reads and puts you expect.\n","\n","Please look at the example above and consider how we get the probabilities.\n","\n","Below is a starter point you can go from, but you DO NOT need to stick it. Feel free to use your own design."]},{"cell_type":"code","metadata":{"id":"aSpCXMDUs3Xs","executionInfo":{"status":"ok","timestamp":1632431057990,"user_tz":240,"elapsed":121,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["def uu_lprob(word, word_count, total_num):\n","  return math.log(word_count[word] / total_num) if word in word_count else -1000000000\n","\n","\"\"\"\n","Reference code for start. You do not need to follow this.\n","Function [unsmoothed_unigram] computes the probabilities for a unigram model\n","lst: a list of words in a sentence\n","Return: [data structure of your choice] that stores the result\n","\"\"\"\n","def unsmoothed_unigram(lst):\n","  # TODO\n","  tokens = real_word_count.keys()\n","  freq = [real_word_count[key] for key in tokens]\n","  real_total_num = sum(freq)\n","\n","  tokens = fake_word_count.keys()\n","  freq = [fake_word_count[key] for key in tokens]\n","  fake_total_num = sum(freq)\n","\n","  real_prob = 0.0\n","  fake_prob = 0.0\n","  for word in lst:\n","    real_prob += uu_lprob(word, real_word_count, real_total_num)\n","    fake_prob += uu_lprob(word, fake_word_count, fake_total_num)\n","  return real_prob < fake_prob, real_prob, fake_prob\n","\n","# for review in tokenized_real_review_validation:\n","#   print(unsmoothed_unigram(review))\n","# print(unsmoothed_unigram([\"SOS\", \"the\", \"hotel\", \"is\", \"good\", \".\", \"SOS\"]))"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cuyn_xPjs4TR"},"source":["## 2.2 Unsmoothed Bi-gram Model.\n","\n","In this part of the project, you are trying to compute the probabilities for a bigram model. You can approach this with similar methods as above.\n","\n","Remember the definition:\n","$p(w_n\\mid w_{n-1})=\\frac{C(w_{n-1}w_n)}{C(w_{n-1})}$ this means you might want to store two things (count of $w_{n-1}$ and count of $w_{n-1}w_n$)."]},{"cell_type":"code","metadata":{"id":"j7opVGtks_kY","executionInfo":{"status":"ok","timestamp":1632431064795,"user_tz":240,"elapsed":168,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["# TODO: Add code for bigram probability calculation. \n","def ub_lprob(bigram, bigram_count, word_count):\n","  prev = bigram[0]\n","  return math.log(bigram_count[bigram] / word_count[prev]) \\\n","    if bigram in bigram_count else -1000000000\n","    \n","\"\"\"\n","Reference code for start. You do not need to follow this.\n","Function [unsmoothed_unigram] computes the probabilities for a unigram model\n","lst: a list of words in a sentence\n","Return: [data structure of your choice] that stores the result\n","\"\"\"\n","def unsmoothed_bigram(lst):\n","  # TODO\n","  real_prob = 0.0\n","  fake_prob = 0.0\n","  for i in range(len(lst)-1):\n","    bigram = (lst[i], lst[i+1])\n","    real_prob += ub_lprob(bigram, real_bigram_count, real_word_count)\n","    fake_prob += ub_lprob(bigram, fake_bigram_count, fake_word_count)\n","  return real_prob < fake_prob, real_prob, fake_prob\n","\n","# for review in tokenized_real_review_validation:\n","#   print(unsmoothed_bigram(review))\n","# print(unsmoothed_bigram([\"SOS\", \"the\", \"hotel\", \"is\", \"good\", \".\", \"SOS\"]))"],"execution_count":78,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejmIJWSVwRJG"},"source":["**Please answer the following question**:\n","\n","**Q2: What data structure are you using to store probabilities for unigrams and bigrams? Why did you select this data structure?**\n","\n","Please answer on your writeup doc!"]},{"cell_type":"markdown","metadata":{"id":"miWWgYxZtAZu"},"source":["# Part 3: Smoothed Language Model\n","In this part, you will need to implement **at least one** smoothing method and **at least one** method to handle unknown words in the test data. You can choose any method(s) that you want for each. You should make clear\n","**what method(s)** were selected and **why**, providing a description for any non-standard approach (e.g., an approach that was not covered in class or in the readings). \n","\n","You should use the\n","provided validation sets to experiment with different smoothing/unknown word handling\n","methods if you wish to see which one is more effective for this task. (We will cover this in Part 4)."]},{"cell_type":"markdown","metadata":{"id":"SfiAOKfEuMGy"},"source":["## 3.1 Unknown Words Handling\n","\n","**Please answer the following questions:**\n","\n","**Q3.1: How are you going to handle unknown words? What parameters might be needed? Do you need a method to determine the value?**\n","\n","Please answer on your writeup doc!\n"]},{"cell_type":"code","metadata":{"id":"OF8ktPAouR_F"},"source":["# TODO: Add your unknown word handling code "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRGK0_x_t46P"},"source":["## 3.2 Smoothing\n","\n","In this part of project, we are going to compute the probabilities for unigram and bigram models after smoothing.\n","There are several smoothing methods you can start with:\n","* add-k\n","* Kneser-Ney\n","* Good-Turing\n","* ...\n","\n","You need to compute for both unigram and bigram models.\n","\n","Below is a starter point using add-k smoothing. As always, you DO NOT need to follow it; you do need to implement add-k smoothing however feel free to implement any other smoothing methods you'd like and use those for later parts of the assignment!"]},{"cell_type":"code","metadata":{"id":"nmW8G3mqt-5z","executionInfo":{"status":"ok","timestamp":1632431075770,"user_tz":240,"elapsed":116,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["def su_lprob(word, word_count, total_num, k, all_unigrams):\n","  return math.log(((word_count[word] if word in word_count else 0) + k) /\n","                          (total_num + k * len(all_unigrams)))\n","\n","\n","\"\"\"\n","Reference code for add-k smoothing on unigram model.\n","dic: a dictionary of your unigrams. key: words, val: occurence\n","k: parameter k for smoothing\n","Return: a dictionary of results after smoothing\n","\"\"\"\n","def add_k_unigram(lst, k=65):\n","  tokens = real_word_count.keys()\n","  freq = [real_word_count[key] for key in tokens]\n","  real_total_num = sum(freq)\n","\n","  tokens = fake_word_count.keys()\n","  freq = [fake_word_count[key] for key in tokens]\n","  fake_total_num = sum(freq)\n","\n","  real_prob = 0.0\n","  fake_prob = 0.0\n","  for word in lst:\n","    real_prob += su_lprob(word, real_word_count, real_total_num, k, all_unigrams)\n","    fake_prob += su_lprob(word, fake_word_count, fake_total_num, k, all_unigrams)\n","  return real_prob < fake_prob, real_prob, fake_prob\n","\n","# for review in tokenized_real_review_validation:\n","#   print(add_k_unigram(review))\n","# print(add_k_unigram([\"SOS\", \"the\", \"hotel\", \"is\", \"good\", \".\", \"SOS\"]))\n"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"Szs3zqwSFDEY","executionInfo":{"status":"ok","timestamp":1632431103792,"user_tz":240,"elapsed":154,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["def sb_lprob(bigram, bigram_count, word_count, k, all_unigrams):\n","  prev = bigram[0]\n","  return math.log(((bigram_count[bigram] if bigram in bigram_count else 0) + k) /\n","                          ((word_count[prev] if prev in word_count else 0) + k * len(all_unigrams)))\n","\n","\"\"\"\n","Reference code for add-k smoothing on bigram model.\n","uni_dic: a dictionary of your unigrams.\n","bi_dic: a dictionary of your bigrams.\n","k: parameter k for smoothing\n","Return: a dictionary of results after smoothing\n","\"\"\"\n","def add_k_bigram(lst, k=0.5):\n","  real_prob = 0.0\n","  fake_prob = 0.0\n","  for i in range(len(lst)-1):\n","    bigram = (lst[i], lst[i+1])\n","    real_prob += sb_lprob(bigram, real_bigram_count, real_word_count, k, all_unigrams)\n","    fake_prob += sb_lprob(bigram, fake_bigram_count, fake_word_count, k, all_unigrams)\n","  return real_prob < fake_prob, real_prob, fake_prob\n","\n","# for review in tokenized_real_review_validation:\n","#   print(add_k_bigram(review))\n","# print(add_k_bigram([\"SOS\", \"the\", \"hotel\", \"is\", \"good\", \".\", \"SOS\"]))"],"execution_count":81,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AbHkdI7-t_7D"},"source":["**Please answer the following question:**\n","\n","**Q3.2: Which smoothing method did you choose? Are there any parameters, if so how are you planning to pick the value? If you choose to implement more than 1 method (not a requirement), please state each of them. Providing a description for any non-standard approach, e.g., an approach that was not covered in class or in the readings**\n","\n","Please answer on your writeup doc!"]},{"cell_type":"markdown","metadata":{"id":"ML03appeuSm3"},"source":["# Part 4: Perplexity\n","At this point, we have developed several language models: unigram vs bigram, unsmoothed vs smoothed. We now want to compare all the models. \n","\n","Implement code to compute the perplexity of a **“development set.”** (“Development set”\n","is just another way to refer to the validation set—part of a dataset that is distinct from\n","the training portion and the test portion.) Compute and report the perplexity of each\n","of the language models (one trained on true reviews and fake reviews) on\n","the development corpora. Compute perplexity as follows:\n","\\begin{align*}\n","PP &= \\left(\\prod_i^N\\frac{1}{P\\left(W_i\\mid W_{i-1}, ...W_{i-n+1}\\right)}\\right)^{\\frac{1}{N}}\\\\\n","&=\\exp \\frac{1}{N}\\sum_{i}^N-\\log P\\left(W_i\\mid W_{i-1}, ...W_{i-n+1}\\right)\n","\\end{align*}\n","where $N$ is the total number of tokens in the test corpus and $P\\left(W_i\\mid W_{i-1}, ...W_{i-n+1}\\right)$\n","is the n-gram probability of your model. Under the second definition above, perplexity\n","is a function of the average (per-word) log probability: use this to avoid numerical\n","computation errors.\n","\n","Please complete the following tasks and report what you have observed. Remember, lower perplexity means better model."]},{"cell_type":"markdown","metadata":{"id":"aT1IRyohJw7e"},"source":["## Task 1: Compute perplexity for smoothed unigram and smoothed bigram. \n","*Note: If you choose more than one smoothing method, pick one of them to compute. If you need to try different values of parameters, you can try them out here.*\n"]},{"cell_type":"code","metadata":{"id":"WFFj7XqDKXfX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632431151946,"user_tz":240,"elapsed":424,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"dc53065f-1de3-41e7-f59f-56e988dd064f"},"source":["# TODO: compute perplexity for one smoothing method on unigram, and one smoothing method on bigram.\n","\n","tokens = real_word_count.keys()\n","freq = [real_word_count[key] for key in tokens]\n","real_total_num = sum(freq)\n","\n","acc = 0\n","N_real = 0\n","for review in real_review_validation:\n","  for word in review:\n","    N_real += 1\n","    acc -= su_lprob(word, real_word_count, real_total_num, 65, all_unigrams)\n","print(\"real unigram \", math.exp(acc/N_real))\n","\n","tokens = fake_word_count.keys()\n","freq = [fake_word_count[key] for key in tokens]\n","fake_total_num = sum(freq)\n","\n","acc = 0\n","N_fake = 0\n","for review in fake_review_validation:\n","  for word in review:\n","    N_fake += 1\n","    acc -= su_lprob(word, fake_word_count, fake_total_num, 65, all_unigrams)\n","print(\"fake unigram \", math.exp(acc/N_fake))\n","\n","\n","acc = 0\n","N_real = 0\n","for review in real_review_validation:\n","  N_real += 1\n","  for i in range(len(review)-1):\n","    N_real += 1\n","    bigram = (review[i], review[i+1])\n","    acc -= sb_lprob(bigram, real_bigram_count, real_word_count, 40, all_unigrams)\n","print(\"real bigram \", math.exp(acc/N_real))\n","\n","acc = 0\n","N_fake = 0\n","for review in fake_review_validation:\n","  N_fake += 1\n","  for i in range(len(review)-1):\n","    N_fake += 1\n","    bigram = (review[i], review[i+1])\n","    acc -= sb_lprob(bigram, fake_bigram_count, fake_word_count, 1, all_unigrams)\n","print(\"fake bigram \", math.exp(acc/N_fake))\n"],"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["real unigram  6327.406995350338\n","fake unigram  6412.516794423919\n","real bigram  8896.064302000623\n","fake bigram  9141.851943418138\n"]}]},{"cell_type":"markdown","metadata":{"id":"QbxTiHHh7Lwc"},"source":["**Q4.1: Why do we need to compute perplexity after smoothing?**\n","\n","Please answer on your writeup doc!\n","\n","**Q4.2: Did you choose any values for parameters?**\n","\n","Please answer on your writeup doc!"]},{"cell_type":"markdown","metadata":{"id":"RQxfllB0KfKf"},"source":["## Task 2: Compute perplexity for other smoothing methods (BONUS 🎉). \n","*Note: If you only pick one smoothing method, you can omit this task. If you need to try different values of parameters, you can try them out here.*"]},{"cell_type":"code","metadata":{"id":"5fU-sCOYK_xI"},"source":["# TODO: compute perplexity for your rest of smoothing method."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VaALsOnk7FZJ"},"source":["**Q4.3: If your smoothing method needs to pick a parameter, what is the value of your parameter?**\n","\n","Please answer on your writeup doc!\n","\n","**Q4.4: Which smoothing method is the best among your choices?**\n","\n","Please answer on your writeup doc!"]},{"cell_type":"markdown","metadata":{"id":"JKH7JpZWvdP5"},"source":["# Part 5: Putting Everything Together and Submitting to Kaggle\n","Combining all the previous parts together, we have developed a bunch of language models. Before we proceed to the next step, let's check a few things (no need to answer):\n","* Did you train your model only on training set?\n","* Did you validate your model only on validation/development set?\n","* Did you determine all your parameters?\n","\n","Finally, please answer:\n","\n","**Q5: What is your choice of language model, and why?** (Hint: How do we usually choose language models? What is our selection criteria? _Look at the Sept. 9th lecture_)\n","\n","Please answer on your writeup doc!\n","\n"]},{"cell_type":"code","metadata":{"id":"ukEVlpFow6mz"},"source":["# TODO: anything that helps you answer/check the above points."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7bWKZQjv2Ht"},"source":["## Part 5.1: First Model Submission to Kaggle\n","\n","Now we need to apply our model to testing data. What you need to do:\n","* Takes the test data as input, and generates an output of your prediction based on your chosen language model\n","* Your output file should be ONLY your predictions\n","* Submit to Kaggle\n","\n","You should use your trained model to predict labels for all the reviews in `TestData.txt`. Output your predictions to a **csv** file and submit it to kaggle. Each line should contain the id of the test review and its corresponding prediction (in total 160 lines). In other words, your output should look like (**including the header**):\n","```\n","Id,Prediction\n","0,0\n","1,0\n","2,1\n","3,0\n","...\n","160,1\n","```\n","Note that you should add the header `Id,Prediction` and there is no space in the output. The Id starts from 0 (not 1).\n","\n","Use this kaggle [link](https://www.kaggle.com/t/eb382e53c0cc448d9da21b3527d) to submit your output. Your team name should be the concatenation of your netids, **exactly in the same order as this notebook is named**. For example, if notebook is 4740_FA21_p1_mb2363_ssc255, then Kaggle group should be mb2363_ssc255.\n","\n","You have 10 submissions **per day** so do not wait until the last minute! There is additionally a baseline score on Kaggle for you to benchmark against.\n"]},{"cell_type":"code","metadata":{"id":"wqECbQs7wDFr","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3f24d50b247b4b68b0ef2b3adc8c0de8","ae4414a60d94400da5873cd1645d6770","c0ae3b1fe227494fa47574308acdf6bc","88cc7fe8c59e496d824b66e481650593","15a3ac7fab2446548ebe8db6fdc12805","2f25bf8b015649419ebc14b28b92c543","75b2b41582374a1bb8771577b3e3a448","81573ae57b394d4ba761db8877fec9df","dd2ede3c34f04ee1ad1a688f8f157b94","417aa1d6c74645c7852a905d3bf2b666","46d386a397b34da4baa4408dc0862a9a"]},"executionInfo":{"status":"ok","timestamp":1632431171279,"user_tz":240,"elapsed":682,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"fbc56080-cdcf-431a-9a15-73ce60f20161"},"source":["# TODO: Add code to generate the Kaggle output file and submit the output file to Kaggle\n","\n","# real_review_test, fake_review_test = load_real_fake_dataset(dataset_path, \"P1_real_fake_review_test.txt\")\n","filename = \"P1_real_fake_review_test.txt\"\n","lst = []\n","with open(os.path.join(dataset_path, filename)) as fp:\n","        csvreader = list(csv.reader(fp, delimiter=\"\\n\"))\n","        csvreader = csvreader[1:]\n","        for txt in csvreader:\n","            lst.append(txt[0].split(\"\\\"\")[1])\n","\n","tokenized_review_test = tokenize_reviews(lst)\n","\n","preprocessing(tokenized_review_test)\n","\n","count = 0\n","print(\"Id,Prediction\")\n","for review in tokenized_review_test:\n","  print(str(count)+\",\"+str(int(add_k_bigram(review, k=1.0)[0])))\n","  count += 1\n"],"execution_count":83,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f24d50b247b4b68b0ef2b3adc8c0de8","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/160 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Id,Prediction\n","0,1\n","1,1\n","2,1\n","3,1\n","4,0\n","5,1\n","6,0\n","7,1\n","8,1\n","9,1\n","10,0\n","11,0\n","12,1\n","13,0\n","14,1\n","15,1\n","16,1\n","17,0\n","18,1\n","19,1\n","20,0\n","21,0\n","22,1\n","23,1\n","24,1\n","25,0\n","26,0\n","27,0\n","28,1\n","29,1\n","30,1\n","31,0\n","32,0\n","33,1\n","34,0\n","35,1\n","36,1\n","37,1\n","38,0\n","39,1\n","40,1\n","41,0\n","42,0\n","43,1\n","44,1\n","45,1\n","46,0\n","47,0\n","48,1\n","49,1\n","50,1\n","51,0\n","52,0\n","53,1\n","54,1\n","55,0\n","56,1\n","57,1\n","58,0\n","59,0\n","60,0\n","61,0\n","62,1\n","63,0\n","64,1\n","65,1\n","66,1\n","67,1\n","68,0\n","69,0\n","70,1\n","71,1\n","72,0\n","73,1\n","74,0\n","75,0\n","76,1\n","77,1\n","78,0\n","79,0\n","80,0\n","81,0\n","82,0\n","83,1\n","84,0\n","85,1\n","86,0\n","87,0\n","88,0\n","89,0\n","90,1\n","91,0\n","92,1\n","93,0\n","94,1\n","95,0\n","96,1\n","97,1\n","98,1\n","99,1\n","100,0\n","101,1\n","102,1\n","103,1\n","104,1\n","105,1\n","106,1\n","107,0\n","108,0\n","109,1\n","110,1\n","111,1\n","112,1\n","113,0\n","114,1\n","115,1\n","116,1\n","117,0\n","118,0\n","119,1\n","120,1\n","121,1\n","122,0\n","123,1\n","124,1\n","125,1\n","126,1\n","127,0\n","128,1\n","129,1\n","130,1\n","131,0\n","132,0\n","133,0\n","134,1\n","135,1\n","136,1\n","137,0\n","138,1\n","139,0\n","140,1\n","141,0\n","142,0\n","143,1\n","144,0\n","145,1\n","146,1\n","147,1\n","148,1\n","149,0\n","150,0\n","151,0\n","152,0\n","153,0\n","154,1\n","155,0\n","156,0\n","157,0\n","158,0\n","159,1\n"]}]},{"cell_type":"markdown","metadata":{"id":"uBtISMsewD4e"},"source":["# Part 6: Naive Bayes\n","\n","The Naive Bayes classification method is based on Bayes Rule. Suppose we have a review *d* and its label *c* (either 0 or 1).\n","\\begin{align*}\n","P(c|d)=\\frac{P(d|c)P(c)}{P(d)}\n","\\end{align*}\n","Likelihood: $P(d|c)$. In real/deception corpus, how likely *d* would appear.\n","\n","Prior: $P(c)$. The probability of real/deceptive reviews in general.\n","\n","Posterior: $P(c|d)$. Given *d*, how likely is it that it is real/deceptive.\n","\n","Goal: $\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(c|d)$, which is equivalent to $\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(d|c)P(c)$.\n","\n","The equivalence holds because $P(d)$ is the same for any $c$. Thus the denominator can be dropped.\n","\n","Denote $d=\\{x_1, x_2, ..., x_n\\}$ where $x_i$'s are words in the reviews *d* (sometimes called features). Unlike n-gram language modelling, we make the multinomial Naive Bayes independence assumption here, where we assume positions of words do not matter. Formally, \n","\\begin{align*}\n","&\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(d|c)P(c)\\\\\n","=&\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(x_1, ..., x_n|c)P(c)\\\\\n","=&\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(x_1|c)P(x_2|c)...P(x_n|c)\n","\\end{align*}\n","\n","Now we only need to collect the occurences of each word for the classification. This is often called a **bag of words** feature. \n","\n","For instance, in the sentence `All for one and one for all .`, the bag of words feature would be `{\"all\": 2, \"for\": 2, \"one\": 2, \"and\": 1, \".\": 1}`. Essentially, the bag of words feature is a dictionary which maps the word to its occurences. We can see that the order is not considered here.\n","\n","Now, your goal is to implement the Multinomial Naive Bayes. You can use existing codes or Python packages, and adapt them to our reviews classification task.\n","\n","You might find the following packages/functions useful:\n","\n","* nltk.word_tokenize(), nltk.word_tokenize()\n","* nltk.classify.naivebayes()\n","* sklearn.feature_extraction.text\n","* sklearn.naive_bayes.MultinomialNB()"]},{"cell_type":"markdown","metadata":{"id":"ZtQpd3ciodGZ"},"source":["**Please answer the following question(s).**\n","\n","**Q6: Comparing Multinomial Naive Bayes with the unigram language model, which one do you expect to perform better? Why?**\n","\n","Please answer on your writeup doc!"]},{"cell_type":"markdown","metadata":{"id":"ZaZTryrewV4I"},"source":["## 6.1 Implementation"]},{"cell_type":"code","metadata":{"id":"BmaInFPSwY9G","executionInfo":{"status":"ok","timestamp":1632431189831,"user_tz":240,"elapsed":116,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["# TODO: Naive Bayes implementation \n","\n","def nb_prob(word, word_count, total_num):\n","  return math.log(word_count[word] / total_num) if word in word_count else 0  # skip words that aren't in the dict\n","\n","def naive_bayes(lst):\n","  tokens = real_word_count.keys()\n","  freq = [real_word_count[key] for key in tokens]\n","  real_total_num = sum(freq)\n","\n","  tokens = fake_word_count.keys()\n","  freq = [fake_word_count[key] for key in tokens]\n","  fake_total_num = sum(freq)\n","\n","  real_prob = math.log(len(real_review_train) / (len(real_review_train) + len(fake_review_train)))\n","  fake_prob = math.log(len(fake_review_train) / (len(real_review_train) + len(fake_review_train)))\n","  for word in lst:\n","    real_prob += uu_lprob(word, real_word_count, real_total_num)\n","    fake_prob += uu_lprob(word, fake_word_count, fake_total_num)\n","  return real_prob < fake_prob, real_prob, fake_prob\n","\n","# for review in tokenized_real_review_validation:\n","#   print(naive_bayes(review))\n","# print(naive_bayes([\"SOS\", \"the\", \"hotel\", \"is\", \"good\", \".\", \"SOS\"]))"],"execution_count":85,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWurBZeiwZmT"},"source":["## 6.2 Putting Everything Together and Submitting to Kaggle\n","\n","You should use your trained model to predict labels for all the reviews in `P1_real_fake_review_test.txt`. Output your predictions to a **csv** file and submit it to kaggle. The format should follow Part 6 as well.\n","\n","Use the previous kaggle link to submit your output! (You are allowed multiple submissions!)"]},{"cell_type":"code","metadata":{"id":"r3pTjTh6wjI-","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["53e5d618b61d4d188cd2d86fd741b544","d2a87bc19ae94691a1a84abb7c068002","d686711e9692482f80dfb87b5c5a675a","a7eb7546aee847adbffd56d47b691961","ce93c2b3b5c0471f84672d2bebe7b351","640ebfcb4fc64020a9b2ff94e88a3dd2","bc9c21444bc14975824e5b01cc6e526d","58637e1de1bf4bc68b39eb6634b9c6eb","a4b51e32395c497ab5db608975390b88","7a93d1d4162849e9b844836172224578","13419bade2334a8cb44b5c4a2efd08eb"]},"executionInfo":{"status":"ok","timestamp":1632430133987,"user_tz":240,"elapsed":1293,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"91322287-9b4f-4b82-a851-169ac182122e"},"source":["# TODO: Code for predicting the test labels and generating the output file. Then submit the output file to Kaggle\n","\n","filename = \"P1_real_fake_review_test.txt\"\n","lst = []\n","with open(os.path.join(dataset_path, filename)) as fp:\n","        csvreader = list(csv.reader(fp, delimiter=\"\\n\"))\n","        csvreader = csvreader[1:]\n","        for txt in csvreader:\n","            lst.append(txt[0].split(\"\\\"\")[1])\n","\n","tokenized_review_test = tokenize_reviews(lst)\n","\n","preprocessing(tokenized_review_test)\n","\n","count = 0\n","print(\"Id,Prediction\")\n","for review in tokenized_review_test:\n","  print(str(count)+\",\"+str(int(naive_bayes(review)[0])))\n","  count += 1"],"execution_count":71,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53e5d618b61d4d188cd2d86fd741b544","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/160 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Id,Prediction\n","0,1\n","1,0\n","2,1\n","3,1\n","4,0\n","5,1\n","6,0\n","7,1\n","8,1\n","9,1\n","10,0\n","11,1\n","12,1\n","13,0\n","14,1\n","15,0\n","16,1\n","17,1\n","18,0\n","19,0\n","20,0\n","21,0\n","22,1\n","23,1\n","24,1\n","25,1\n","26,0\n","27,0\n","28,1\n","29,1\n","30,1\n","31,1\n","32,1\n","33,1\n","34,0\n","35,0\n","36,1\n","37,0\n","38,0\n","39,1\n","40,1\n","41,1\n","42,1\n","43,0\n","44,0\n","45,1\n","46,1\n","47,0\n","48,1\n","49,1\n","50,0\n","51,0\n","52,0\n","53,1\n","54,0\n","55,1\n","56,1\n","57,0\n","58,1\n","59,0\n","60,0\n","61,1\n","62,0\n","63,0\n","64,0\n","65,0\n","66,0\n","67,1\n","68,0\n","69,0\n","70,1\n","71,1\n","72,1\n","73,1\n","74,1\n","75,0\n","76,0\n","77,0\n","78,0\n","79,0\n","80,0\n","81,1\n","82,1\n","83,1\n","84,0\n","85,1\n","86,0\n","87,0\n","88,0\n","89,0\n","90,0\n","91,0\n","92,1\n","93,0\n","94,1\n","95,0\n","96,1\n","97,1\n","98,0\n","99,1\n","100,0\n","101,1\n","102,1\n","103,0\n","104,1\n","105,0\n","106,1\n","107,0\n","108,0\n","109,0\n","110,1\n","111,1\n","112,1\n","113,0\n","114,1\n","115,1\n","116,1\n","117,0\n","118,0\n","119,1\n","120,1\n","121,1\n","122,0\n","123,1\n","124,0\n","125,1\n","126,1\n","127,0\n","128,0\n","129,1\n","130,1\n","131,0\n","132,0\n","133,0\n","134,0\n","135,0\n","136,1\n","137,0\n","138,0\n","139,0\n","140,1\n","141,1\n","142,0\n","143,1\n","144,0\n","145,1\n","146,1\n","147,1\n","148,1\n","149,0\n","150,0\n","151,1\n","152,0\n","153,0\n","154,0\n","155,0\n","156,0\n","157,0\n","158,1\n","159,1\n"]}]},{"cell_type":"markdown","metadata":{"id":"9MVtIIyEzSS2"},"source":["# Work Distribution\n","\n","**Please briefly describe how you divided the work.**\n","\n","Please answer on your writeup doc!\n"]},{"cell_type":"markdown","metadata":{"id":"ddOznZ7P2nlI"},"source":["# Project Feedback [1 point]\n"," e on the course staff are trying our best to adapt our teaching, projects and everything else in the class to best help you learn (and hope you get as excited about NLP as we do!). We would immenselly appreciate it if you could provide us feedback (it's a super short form!!) on this project and **it's worth 1 point of your project grade**\n","\n","Link to the feedback form: https://forms.gle/EfYoeggeGkr2Mb6Y7\n","\n","We will use this feedback to improve both **upcoming projects** and projects for next year. \n","\n","Thank you so much!"]},{"cell_type":"markdown","metadata":{"id":"fBrWSR2bs77m"},"source":["# Submitting the Notebook\n","\n","1. Go to File (upper left corner) -> Download .ipynb -> submit this downloaded file to cms\n","2. Run the first code block\n","3. Replace our placeholder for your correct Google Drive directory structure in the 2nd code block below. Run the code block\n","4. Put the name of this notebook into our placeholder in the 3rd code block. Run the code block\n","5. Then go to the folder icon on the very left panel, under the orange CO logo. Click on the folder and wait for a PDF version of your notebook to appear. Might take a few minutes.\n","6. Download the pdf version and submit to Gradescope"]},{"cell_type":"code","metadata":{"id":"8jg5v1sRzP5m","executionInfo":{"status":"ok","timestamp":1632431209169,"user_tz":240,"elapsed":6312,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["%%capture\n","!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc"],"execution_count":86,"outputs":[]},{"cell_type":"code","metadata":{"id":"_eOenvlzzRTh","executionInfo":{"status":"ok","timestamp":1632431213998,"user_tz":240,"elapsed":413,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["%%capture\n","# the red text is a placeholder! Change it to your directory structure!\n","!cp 'drive/My Drive/CS 4740/Project 1/CS4740_FA21_p1_nar73_krm74.ipynb' ./ "],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOMiIp6Uzj1S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632431222602,"user_tz":240,"elapsed":6058,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"192912a5-90de-4a9f-d0d1-64b2ed3b20cd"},"source":["# the red text is a placeholder! Change it to the name of this notebook!\n","!jupyter nbconvert --to PDF \"CS4740_FA21_p1_nar73_krm74.ipynb\""],"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook CS4740_FA21_p1_nar73_krm74.ipynb to PDF\n","[NbConvertApp] Writing 92418 bytes to ./notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n","[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n","[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 102908 bytes to CS4740_FA21_p1_nar73_krm74.pdf\n"]}]},{"cell_type":"markdown","metadata":{"id":"U6JokGl8f7wy"},"source":["# Once Again\n","Please make sure do the following:\n","1. Submit the PDF version of your colab notebook on Gradescope.\n","2. Submit a PDF version of your Google Doc with all your written questions complete on Gradescope.\n","3. Submit the .ipynb of your colab notebook on CMS."]},{"cell_type":"markdown","metadata":{"id":"HvET3mDvxL2j"},"source":["# You are done! ✅"]}]}