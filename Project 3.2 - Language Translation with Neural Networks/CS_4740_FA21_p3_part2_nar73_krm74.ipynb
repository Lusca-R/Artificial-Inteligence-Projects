{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS_4740_FA21_p3_part2_nar73_krm74.ipynb","provenance":[{"file_id":"1pKEM3fAPRoQaMIgvFfHPtNypZHhLPtmj","timestamp":1637089116953},{"file_id":"1yY5w2vnjeijEGjSU84AMq32QX7moaShv","timestamp":1633825714560}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0a814c69e02143528d12d7711c5e6aa0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_59966a8ba43142458aa857a8dda1c24b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cb8a6a122a114ac0904f6c7c7e41bf7e","IPY_MODEL_3a8e99040c464fff9377728248fecede","IPY_MODEL_f2cfd98a925b4e6cad163f0425c84fe8"]}},"59966a8ba43142458aa857a8dda1c24b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb8a6a122a114ac0904f6c7c7e41bf7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_edfbca8298744f4fa6c0763c1f593c10","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_658560341025401ba6fa830eec0dbe77"}},"3a8e99040c464fff9377728248fecede":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_73f1b514f3ef407f8404d53ad196ff4f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1809,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1809,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_afde21448d254ffba671d83361714adc"}},"f2cfd98a925b4e6cad163f0425c84fe8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_64d2f98e42f340508e17e7c26e430b41","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1809/1809 [01:16&lt;00:00, 23.57it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fdb2466d543847d2b12d012f72c9fb54"}},"edfbca8298744f4fa6c0763c1f593c10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"658560341025401ba6fa830eec0dbe77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73f1b514f3ef407f8404d53ad196ff4f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"afde21448d254ffba671d83361714adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"64d2f98e42f340508e17e7c26e430b41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fdb2466d543847d2b12d012f72c9fb54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"epRaBvj_bapY"},"source":["# Project 3 (part 2): Language Translation with Neural Networks\n","## CS4740/5740 Fall 2021\n","\n","Names: Lusca Robinson, Kyrus Mama\n","\n","Netids: nar73, krm74\n","\n","### Project Submission Due: November 23, 2021\n","Please submit the **pdf file** of this notebook on **Gradescope**, and the **ipynb** on **CMS**. For instructions on generating pdf and ipynb files, please refer to project 1 or project 2 instructions.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EIEkIveBVYyU"},"source":["## Introduction\n","In this project we will consider **neural networks**:  a Recurrent Neural Network (RNN), for performing neural machine translation (i.e. translating from one language into another).\n","\n","The project is divided into parts. In **Part 1**, you will implement an RNN model for performing the neural machine translation. In **Part 2**, you will analyze these models in two types of comparative studies and in **Part 3** you will answer questions describing what you have learned through this project. You also will be required to submit a description of libraries used, how your group divided up the work, and your feedback regarding the assignment (**Part 4**).\n","\n","The writeup for the document is linked [here](https://docs.google.com/document/d/1IWgYqS6M4G_gJowM97Bq8g5smsGOa75dutIUGjolpAI/edit)."]},{"cell_type":"markdown","metadata":{"id":"-1HSug5yWVrI"},"source":["## Advice üöÄ\n","As always, the report is important! The report is where you get to show\n","that you understand not only what you are doing but also why and how you are doing it. So be clear, organized and concise; avoid vagueness and excess verbiage. Spend time doing error analysis for the models. This is how you understand the advantages and drawbacks of the systems you build. The reports should read more like the papers that we have been writing critiques for."]},{"cell_type":"markdown","metadata":{"id":"-fyl3-8TWyR9"},"source":["## Dataset\n","You are given access to a set of parallel sentences. One sentence is written in modern English (the \"source\") and another is in Shakespearean English (the \"target\"). For this project, given modern English you will need to translate this into Shakespearean English. This is usually called (Neural) Machine Translation. We'll simply refer to it as NMT or Neural Machine Translation in the project.\n","\n","We will minimally preprocess the source/target sentences and handle tokenization in what we release. For this assignment, we do not anticipate any further preprocessing to be done by you. Should you choose to do so, it would be interesting to hear about in the report (along with whether or not it helped performance), but it is not a required aspect of the assignment."]},{"cell_type":"code","metadata":{"id":"ARl1pk1PGL2Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637723818153,"user_tz":300,"elapsed":1160,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"b375f393-3313-4b63-807a-3cd04862eba8"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive', force_remount=True)\n","\n","source_path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS 4740\", \"Project 3.2\", \"Dataset\", \"source.txt\") # replace based on your Google drive organization\n","target_path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS 4740\", \"Project 3.2\", \"Dataset\", \"target.txt\") # replace based on your Google drive organization\n","test_path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS 4740\", \"Project 3.2\", \"Dataset\", \"test.txt\") # replace based on your Google drive organization"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"qeC3pYiebc6r"},"source":["## Import libraries and connect to Google Drive"]},{"cell_type":"code","metadata":{"id":"21NRQju0KuEo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637705314577,"user_tz":300,"elapsed":2855,"user":{"displayName":"Lusca Robinson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9N9oiCLT21lFXFt7ZQFYlnFQu9pN0W882pJvjkQ=s64","userId":"16217302501777754899"}},"outputId":"932a2ecd-d900-4efb-be02-2b242e987cd7"},"source":["!pip install -U gensim"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n"]}]},{"cell_type":"code","metadata":{"id":"quIJujja-jS2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637705318372,"user_tz":300,"elapsed":3797,"user":{"displayName":"Lusca Robinson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9N9oiCLT21lFXFt7ZQFYlnFQu9pN0W882pJvjkQ=s64","userId":"16217302501777754899"}},"outputId":"121e72b9-55ab-4d1f-b47f-a34ca28800d7"},"source":["!pip3 install sentencepiece\n","from collections import Counter, namedtuple\n","from itertools import chain\n","import json\n","import math\n","import os\n","from pathlib import Path\n","import random\n","import time\n","import sys\n","from tqdm.notebook import tqdm, trange\n","from typing import List, Tuple, Dict, Set, Union\n","\n","\n","import gensim\n","import nltk\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","import numpy as np\n","import sentencepiece as spm\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","import torch.nn.utils\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","\n","from tqdm.notebook import tqdm, trange"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"]}]},{"cell_type":"code","metadata":{"id":"eQ2HeOVlKp-4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637705318373,"user_tz":300,"elapsed":6,"user":{"displayName":"Lusca Robinson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9N9oiCLT21lFXFt7ZQFYlnFQu9pN0W882pJvjkQ=s64","userId":"16217302501777754899"}},"outputId":"1b9351dc-825f-400a-a8d9-c70c7eacf616"},"source":["nltk.download(\"punkt\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"mMjtuKhBBvBA"},"source":["!pip install -qqq wandb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eb_qmIW-CEsk","executionInfo":{"status":"ok","timestamp":1637705322546,"user_tz":300,"elapsed":1542,"user":{"displayName":"Lusca Robinson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9N9oiCLT21lFXFt7ZQFYlnFQu9pN0W882pJvjkQ=s64","userId":"16217302501777754899"}},"outputId":"cf2c534c-aab6-4d66-8102-b7f17b802143"},"source":["import wandb\n","!wandb login "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlusca\u001b[0m (use `wandb login --relogin` to force relogin)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKXQJW-NCND8","executionInfo":{"status":"ok","timestamp":1637705323448,"user_tz":300,"elapsed":904,"user":{"displayName":"Lusca Robinson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9N9oiCLT21lFXFt7ZQFYlnFQu9pN0W882pJvjkQ=s64","userId":"16217302501777754899"}},"outputId":"b9a31339-15fa-4887-b0ea-3748a6b9ce4f"},"source":["!wandb online"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W&B online, running your script from this directory will now sync to the cloud.\n"]}]},{"cell_type":"markdown","metadata":{"id":"9D2uTXRmbn5Q"},"source":["# Part 1: Recurrent Neural Network\n","Recurrent neural networks have been the workhorse of NLP for a number of years. A fundamental reason for this success is they can inherently deal with _variable_ length sequences. This is axiomatically important for natural language; words are formed from a variable number of characters, sentences from a variable number of words, paragraphs from a variable number of sentences, and so forth. This differs from a field like Computer Vision where images are (generally) of a fixed size.\n","<br></br>\n","This is a also very different scenario than that of the classifiers we have studied (e.g. Naive Bayes, Perceptron Learning, Feedforward Neural Networks), which take in a\n","fixed-length vector.\n","<br></br>\n","To clarify this, we can think of the _types_ of the mathematical functions described by a FFNN and an RNN. What is critical to note in what follows is that k (the length of a sequence) need not be constant\n","across examples.\n","\n","Below we define the general problem set up of FFNNs and RNNs.\n","\n","$\\textbf{FFNN.}$ \\\n","$Input: \\text{We have an input vector }\\vec{x} \\in \\mathcal{R}^d$ \\\n","$Model\\text{ }Output: \\text{The model has some intermediate output }\\vec{z} \\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid}$ \\\n","$Final\\text{ }Output: \\text{ The model outputs a vector } \\vec{y} \\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid}$ \\\n","$\\vec{y}$ satisfies the constraint of being a probability distribution, i.e. $\\underset{i \\in \\mid \\mathcal{Y} \\mid}{\\sum} \\vec{y}[i] = 1$ and $\\underset{i \\in \\mid \\mathcal{y} \\mid}{min} \\text{ }\\vec{y}[i] \\leq 1$, which is achieved via _Softmax_ applied to $\\vec{z}$.\n","<br></br>\n","$\\textbf{RNN.}$ \\\n","$Input: \\text{The model takes as input a sequence of vectors} \\vec{x}_1,\\vec{x}_2, \\dots, \\vec{x}_k; \\vec{x}_i \\in \\mathcal{R}^d$ \\\n","$Model\\text{ }Output: \\text{The model generates some intermediate sequence output} \\vec{z}_1,\\vec{z}_2, \\dots, \\vec{z}_k; \\vec{z}_i \\in \\mathcal{R}^{h}, \\text{ where h is the hidden state size.}$\n","$Final\\text{ }Output: \\text{The model generates some final sequence output} \\vec{y}_1, \\dots, \\vec{y}_k \\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid}$ \\\n","$\\vec{y}$ satisfies the constraint of being a probability distribution, i.e. $\\underset{i \\in \\mid \\mathcal{Y} \\mid}{\\sum} \\vec{y}_j[i] = 1$ and $\\underset{i \\in \\mid \\mathcal{y} \\mid}{min} \\text{ }\\vec{y}_j[i] \\geq 0$, which is achieved by the process described later in this report and as you have seen in class."]},{"cell_type":"markdown","metadata":{"id":"A4n8IVFtflPQ"},"source":["Intuitively, an RNN takes in a sequence of vectors and computes a new vector corresponding to each vector in the original sequence. It achieves this by processing the input sequence one vector at a time to (a) compute an updated representation of the entire sequence (which is then re-used when processing the next vector in the input sequence), and (b) produce an output for the current position. The vector computed in (a) therefore not only contains information about the current input vector but also about the previous input vectors. Hence, $\\vec{z}_j$ is computed after having observed $\\vec{x}_1, \\dots, \\vec{x}_j$. As such, a simple observation is we can treat the last vector computed by the RNN, ie $\\vec{z}_k$ as a representation of the entire sequence. Accordingly, we can use this as the input to a single-layer linear classifier to compute a vector $\\vec{y}$ as we will need for classification.\n","\n","$$\\vec{y}_j = Softmax(W\\vec{z}_j); \\text{ where }W\\in \\mathcal{R}^{\\mid \\mathcal{Y}\\mid \\times h} \\text{ is a weight matrix that is learned through training}$$"]},{"cell_type":"markdown","metadata":{"id":"wfjD4PdHLJeg"},"source":["In Machine Translation, our goal is to convert a sentence from the source language (e.g. Modern English) to the target language (e.g. Shakespearean English). In this assignment, we will implement a sequence-to-sequence (Seq2Seq) network, to build a Neural Machine Translation (NMT) system. In this section, we describe the training procedure for the proposed NMT system, which uses a Bidirectional RNN Encoder and a Unidirectional RNN Decoder. We'll recap the theoretical component here and in the modules where you are writing code, we will repeat the steps more explicitly in an algorithmic manner.\n","\n","<Insert diagram here>\n","\n","Given a sentence in the source language, we look up the word embeddings from an embeddings matrix, yielding $x_1,\\dots, x_n$ ($x_i \\in R^{e}$), where n is the length of the source sentence and e is the embedding size. We feed these embeddings to the bidirectional encoder, yielding hidden states for both the forward (‚Üí) and backward (‚Üê) RNNs. The forward and backward versions are concatenated to give hidden states $h_i^{enc}$\n","\n","\n","$$h_i^{enc} = [\\overrightarrow{h_i^{enc}}; \\overleftarrow{h_i^{enc}}] \\text{ where }h_i^{enc} \\in R^{2h}, \\overrightarrow{h_i^{enc}}, \\overleftarrow{h_i^{enc}} \\in R^{h}$$\n","\n","\n","We then initialize the decoder‚Äôs first hidden state $h_0^{dec}$ with a linear projection of the encoder‚Äôs final hidden state\n","\n","$$h_0^{dec} = W_h[\\overrightarrow{h_n^{enc}}; \\overleftarrow{h_0^{enc}}] \\text{ where }h_0^{dec} \\in R^{h}, W_h \\in R^{h \\times 2h}$$\n","\n","With the decoder initialized, we must now feed it a target sentence. On the $t^{th}$ step, we look up the embedding for the $t^{th}$ word, $y_t \\in R^{e}$. We then concatenate $y_t$ with the combined-output vector $o_{t‚àí1} \\in R^{h}$ from the previous timestep (we will explain what this is later but this is just the output from the previous step) to produce $y_t \\in R^{e+h}$. Note that for the first target (i.e. the start token) $o_0$ is usually a zero-vector (but it can be random or a learned vector as well). We then feed $y_t$ as input to the decoder.\n","\n","$$ h_t^{dec} = Decoder(y_t, h_{t-1}^{dec})\\text{ where }h_{t-1}^{dec} ‚àà R^{h}$$\n","\n","We can take the decoder hidden state $h_t^{dec}$ and pass this through a linear layer to obtain an intermediate output $v_t$. This is then passed through an activation function (like tanh) to obtain our combined-output vector $o_t$\n","\n","$$v_t = W_v h_t^{dec} \\text{ where } W_v \\in R^{h \\times h}, v_t \\in R^{h}$$\n","$$o_t = \\tanh{(v_t)} \\text{ where } o_t \\in R^{h}$$\n","\n","Then, we produce a probability distribution $P_t$ over target words at the $t^{th}$ timestep.\n","\n","$$P_t = Softmax(W_{v_{target}} o_t) \\text{ where }P_t \\in R^{V_{target}}, W_{v_{target}}\\in R^{V_{target} \\times h}$$\n","\n","\n","Here, $V_{target}$ is the size of the target vocabulary. Finally, to train the network we then compute the softmax cross entropy loss between $P_t$ and $g_t$, where $g_t$ is the one-hot vector of the target word at timestep t:\n","\n","$$Loss(Model) = CrossEntropy(P_t, g_t)$$\n","\n","Now that we have described the model, let‚Äôs try implementing it for Modern English to Shakespearean English translation.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sQ3IOuMC-sqh"},"source":["### How do we evaluate NMT models?\n","We can evaluate these models in a few different ways. Recall in lecture that we called these encoder-decoder models \"Conditional Language Models\" since they condition on some prefix before generating text similar to the language models we have seen before. Therefore, we can use **perplexity** to measure the performance of our model.\n","\n","However, perplexity is more of an intrinsic measure and so we'd like to directly measure how closely the model output is to our generated translations. How do we do this? We can look at how well our translation _overlaps_ with the reference translation. A common metric for this is the **BLEU** (Bilingual Evaluation Understudy) metric. The approach works by counting matching n-grams in the candidate translation to n-grams in the reference text, where 1-gram or unigram would be each token and a bigram comparison would be each word pair. The comparison is made regardless of word order. BLEU uses N-grams of size 1-4 in its computation."]},{"cell_type":"markdown","metadata":{"id":"oFYJdd9kYhXq"},"source":["## Part 1: Rules\n","**Part 1** requires implementing an RNN in PyTorch for translation. Countless blog posts, internet tutorials and other implementations available publicly (and privately) do precisely this. In fact, many students in [Cornell NLP](https://nlp.cornell.edu/people/) likely have some code for doing this or something similar on their Github. You **cannot** use any such code (though you may use anything you find in course notes or course texts) irrespective of whether you cite it or not.\n","\n","Submissions will be passed through the MOSS system, which is a sophisticated system for detecting plagiarism in code and is robust in the sense that it tries to find alignments in the underlying semantics of the code and not just the surface level syntax. Similarly, the course staff are also quite astute with respect to programming neural models for NLP and we will strenuously look at your code. We flagged multiple groups for this last year, so we strongly suggest you resist any such temptation (if the Academic Integrity policy alone is insufficient at dissuading you)."]},{"cell_type":"markdown","metadata":{"id":"oS1lQBsWWNLc"},"source":["## 1.1 RNN Implementation\n","\n","Recall from the previous portion of this assignment as well as the PyTorch tutorial we used a `Data loader` component; we will want to use something similar here as well as a new `NMT` component. We don't envision that it will be useful to copy and modify the previous `Data loader` here. We have included some stubs to help give you a place to start for the NMT.\n","\n","Additionally, we remind you that the previous assignment furnishes a near-functional implementation of a similar neural model (but for a different task). If you successfully completed the FFNN bug fixes , it will be wholly functional. Using it as a guide for Part 1 below is both prudent and suggested.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"89rP1JrUyZwZ"},"source":["### 1.1.1 Data loading"]},{"cell_type":"code","metadata":{"id":"s0NUSRffyCYP"},"source":["Hypothesis = namedtuple('Hypothesis', ['value', 'score'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vziuNgbJpYvg"},"source":["def pad_sents(sents, pad_token):\n","    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n","        The paddings should be at the end of each sentence.\n","    :param sents: list of sentences, where each sentence\n","                                    is represented as a list of words\n","    :type sents: list[list[str]]\n","    :param pad_token: padding token\n","    :type pad_token: str\n","    :returns sents_padded: list of sentences where sentences shorter\n","        than the max length sentence are padded out with the pad_token, such that\n","        each sentence in the batch now has equal length.\n","    :rtype: list[list[str]]\n","    \"\"\"\n","    sents_padded = []\n","\n","    max_len = max([len(sent) for sent in sents])\n","    sents_padded = [(sent + ([pad_token] * (max_len - len(sent)))) for sent in sents]\n","\n","    return sents_padded"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T77mk7z8pyJw"},"source":["def read_corpus(file_path, source):\n","    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n","    :param file_path: path to file containing corpus\n","    :type file_path: str\n","    :param source: \"tgt\" or \"src\" indicating whether text\n","        is of the source language or target language\n","    :type source: str\n","    \"\"\"\n","    data = []\n","    for line in open(file_path):\n","        sent = nltk.word_tokenize(line)\n","        # only append <s> and </s> to the target sentence\n","        if source == 'tgt':\n","            sent = ['<s>'] + sent + ['</s>']\n","        data.append(sent)\n","\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhtfKCGspU1U"},"source":["class Vocab(object):\n","    \"\"\" Vocabulary, i.e. structure containing either\n","    src or tgt language terms.\n","    \"\"\"\n","    def __init__(self, word2id=None):\n","        \"\"\" Init Vocab Instance.\n","        \n","        :param word2id: dictionary mapping words 2 indices\n","        :type word2id: dict[str, int]\n","        \"\"\"\n","        if word2id:\n","            self.word2id = word2id\n","        else:\n","            self.word2id = dict()\n","            self.word2id['<pad>'] = 0   # Pad Token\n","            self.word2id['<s>'] = 1     # Start Token\n","            self.word2id['</s>'] = 2    # End Token\n","            self.word2id['<unk>'] = 3   # Unknown Token\n","        self.unk_id = self.word2id['<unk>']\n","        self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","    def __getitem__(self, word):\n","        \"\"\" Retrieve word's index. Return the index for the unk\n","        token if the word is out of vocabulary.\n","        \n","        :param word: word to look up\n","        :type word: str\n","        :returns: index of word\n","        :rtype: int\n","        \"\"\"\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        \"\"\" Check if word is captured by Vocab.\n","        \n","        :param word: word to look up\n","        :type word: str\n","        :returns: whether word is in vocab\n","        :rtype: bool\n","        \"\"\"\n","        return word in self.word2id\n","\n","    def __setitem__(self, key, value):\n","        \"\"\" Raise error, if one tries to edit the Vocab directly.\n","        \"\"\"\n","        raise ValueError('vocabulary is readonly')\n","\n","    def __len__(self):\n","        \"\"\" Compute number of words in Vocab.\n","        \n","        :returns: number of words in Vocab\n","        :rtype: int\n","        \"\"\"\n","        return len(self.word2id)\n","\n","    def __repr__(self):\n","        \"\"\" Representation of Vocab to be used\n","        when printing the object.\n","        \"\"\"\n","        return 'Vocabulary[size=%d]' % len(self)\n","\n","    def id2word(self, wid):\n","        \"\"\" Return mapping of index to word.\n","        \n","        :param wid: word index\n","        :type wid: int\n","        :returns: word corresponding to index\n","        :rtype: str\n","        \"\"\"\n","        return self.id2word[wid]\n","\n","    def add(self, word):\n","        \"\"\" Add word to Vocab, if it is previously unseen.\n","        \n","        :param word: to add to Vocab\n","        :type word: str\n","        :returns: index that the word has been assigned\n","        :rtype: int\n","        \"\"\"\n","        if word not in self:\n","            wid = self.word2id[word] = len(self)\n","            self.id2word[wid] = word\n","            return wid\n","        else:\n","            return self[word]\n","\n","    def words2indices(self, sents):\n","        \"\"\" Convert list of words or list of sentences of words\n","        into list or list of list of indices.\n","        \n","        :param sents: sentence(s) in words\n","        :type sents: Union[List[str], List[List[str]]]\n","        :returns: sentence(s) in indices\n","        :rtype: Union[List[int], List[List[int]]]\n","        \"\"\"\n","        if type(sents[0]) == list:\n","            return [[self[w] for w in s] for s in sents]\n","        else:\n","            return [self[w] for w in sents]\n","\n","    def indices2words(self, word_ids):\n","        \"\"\" Convert list of indices into words.\n","        \n","        :param word_ids: list of word ids\n","        :type word_ids: List[int]\n","        :returns: list of words\n","        :rtype: List[Str]\n","        \"\"\"\n","        return [self.id2word[w_id] for w_id in word_ids]\n","\n","    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n","        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n","        shorter sentences.\n","        \n","        :param sents: list of sentences (words)\n","        :type sents: List[List[str]]\n","        :param device: Device on which to load the tensor, ie. CPU or GPU\n","        :type device: torch.device\n","        :returns: Sentence tensor of (max_sentence_length, batch_size)\n","        :rtype: torch.Tensor\n","        \"\"\"\n","\n","        word_ids = self.words2indices(sents)\n","        sents_t = pad_sents(word_ids, self['<pad>'])\n","        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n","        return torch.t(sents_var)\n","\n","    @staticmethod\n","    def from_corpus(corpus, size, freq_cutoff=2):\n","        \"\"\" Given a corpus construct a Vocab.\n","        \n","        :param corpus: corpus of text produced by read_corpus function\n","        :type corpus: List[str]\n","        :param size: # of words in vocabulary\n","        :type size: int\n","        :param freq_cutoff: if word occurs n < freq_cutoff times, drop the word\n","        :type freq_cutoff: int\n","        :returns: Vocab instance produced from provided corpus\n","        :rtype: Vocab\n","        \"\"\"\n","        vocab_entry = Vocab()\n","        word_freq = Counter(chain(*corpus))\n","        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n","        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n","              .format(len(word_freq), freq_cutoff, len(valid_words)))\n","        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n","        for word in top_k_words:\n","            vocab_entry.add(word)\n","        return vocab_entry\n","    \n","    @staticmethod\n","    def from_subword_list(subword_list):\n","        \"\"\"Given a list of subwords, construct the Vocab.\n","        \n","        :param subword_list: list of subwords in corpus\n","        :type subword_list: List[str]\n","        :returns: Vocab instance produced from provided list\n","        :rtype: Vocab\n","        \"\"\"\n","        vocab_entry = Vocab()\n","        for subword in subword_list:\n","            vocab_entry.add(subword)\n","        return vocab_entry"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2CihDrhyMI_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637705332077,"user_tz":300,"elapsed":8465,"user":{"displayName":"Lusca Robinson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9N9oiCLT21lFXFt7ZQFYlnFQu9pN0W882pJvjkQ=s64","userId":"16217302501777754899"}},"outputId":"5c4fa15b-f958-4d0d-b9d6-ffe4731c2f95"},"source":["print('initialize source vocabulary ..')\n","src_sents = read_corpus(source_path, \"src\")\n","src = Vocab.from_corpus(src_sents, 20000, 2) # 7098, 9422\n","\n","print('initialize target vocabulary ..')\n","tgt_sents = read_corpus(target_path, \"tgt\")\n","tgt = Vocab.from_corpus(tgt_sents, 20000, 2) # 6893, 10956"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["initialize source vocabulary ..\n","number of word types: 13252, number of word types w/ frequency >= 2: 9167\n","initialize target vocabulary ..\n","number of word types: 15216, number of word types w/ frequency >= 2: 10725\n"]}]},{"cell_type":"code","metadata":{"id":"kHkmbdw1zZXB"},"source":["## YOUR CODE HERE\n","# Train embeddings or load embeddings\n","# or use other feature representation for words (e.g 1 hot encoding)\n","#\n","# We want a numpy array that has shape |V| x |embedding size| that can potentially\n","# be passed into our NMT model for our pretrained_source / pretrained_target\n","# arguments. This allows our model to start off with a good starting point and\n","# we can decide whether to keep our embeddings static or update them as we go.\n","#\n","# Some ideas as to what to do here are using pre-trained word embeddings from gensim\n","# >>> import gensim.downloader as api\n","# >>> model = api.load(\"glove-wiki-gigaword-300\")  # load glove vectors\n","# >>> model.wv['chicken'] # Get word vector for chicken\n","#\n","# OR potentially train your own new embeddings using the SkipGram algorithm discussed in lecture.\n","# >>> model = gensim.models.Word2Vec(sentences, min_count=1, vector_size=300, sg=1, negative=5)\n","# Tutorial: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n","#\n","# OR explicitly choose to do nothing here and the embeddings are learned end-to-end during training in the NMT class\n","\n","import gensim.downloader as api\n","embeddingModel = api.load(\"glove-wiki-gigaword-300\")  # load glove vectors\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9n79ex_zZyCJ"},"source":["# print(model.get_vector('chicken'))\n","# print(src.word2id)\n","vocabEmbedding=np.zeros((len(src.word2id),embeddingModel.vector_size), dtype=np.float32)\n","for i in range(len(src.word2id)):\n","  if src.id2word[i] in embeddingModel.key_to_index:\n","    vocabEmbedding[i] = embeddingModel.get_vector(src.id2word[i])\n","\n","vocabEmbedding=torch.from_numpy(vocabEmbedding)\n","# print(vocabEmbedding[5:])\n","# tgtEmbedding=np.zeros((len(tgt.word2id),embeddingModel.vector_size), dtype=np.float32)\n","# for j in range(len(tgt.word2id)):\n","#   if tgt.id2word[j] in embeddingModel.key_to_index:\n","#     tgtEmbedding[j]=embeddingModel.get_vector(tgt.id2word[i])\n","# tgtEmbedding=torch.from_numpy(tgtEmbedding)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQmyoO0FzzmX"},"source":["# Split into training and validation data\n","train_data_src, val_data_src, train_data_tgt, val_data_tgt = train_test_split(src_sents, tgt_sents, test_size=0.045922, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0CcUGWHz7WE"},"source":["train_data = list(zip(train_data_src, train_data_tgt))\n","val_data = list(zip(val_data_src, val_data_tgt))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fD6Ro03A0h4"},"source":["# ml = min(len(train_data), len(val_data))\n","# train_data = train_data[:ml]\n","# val_data = val_data[:ml]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GjWp9yhmyRWR"},"source":["### 1.1.2 NMT Model Implementation\n","\n","For the implementation below, we have given a framework / skeleton for your code. Within the skeleton are sections that define where you should place your code."]},{"cell_type":"code","metadata":{"id":"xnwncrLnb6kx"},"source":["def generate_sent_masks(enc_hiddens: torch.Tensor, source_lengths: List[int], device: torch.device) -> torch.Tensor:\n","    \"\"\" Generate sentence masks for encoder hidden states.\n","\n","    :param enc_hiddens: encodings of shape (b, src_len, 2*h), where b = batch size,\n","        src_len = max source length, h = hidden size.\n","    :type enc_hiddens: torch.Tensor\n","    :param source_lengths: List of actual lengths for each of the sentences in the batch.   \n","    :type source_lengths: List[int]\n","    :param device: Device on which to load the tensor, ie. CPU or GPU\n","    :type device: torch.device\n","    :returns: Tensor of sentence masks of shape (b, src_len),\n","        where src_len = max source length, h = hidden size.\n","    :rtype: torch.Tensor\n","    \"\"\"\n","    enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n","    for e_id, src_len in enumerate(source_lengths):\n","        enc_masks[e_id, src_len:] = 1\n","    return enc_masks.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EApec5FFcNsY"},"source":["class Encoder(nn.Module):\n","    def __init__(self, embed_size, hidden_size, source_embeddings):\n","        \"\"\"\n","        \"\"\"\n","        super(Encoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embed_size = embed_size\n","        self.embedding = source_embeddings\n","\n","        # we think  embed_size == 300\n","        self.encoder = nn.RNN(embed_size, hidden_size, 2, bias=True, bidirectional=True)\n","        self.h_projection = nn.Linear(2*hidden_size, hidden_size, bias=False)\n","        ### YOUR CODE HERE (~2 Lines)\n","        ### TODO - Initialize the following variables:\n","        ###     self.encoder (Bidirectional RNN with bias)\n","        ###     self.h_projection (Linear Layer with no bias), called W_{h} above.\n","        ###\n","        ### Note that you are free to use any architecture (vanilla RNN, LSTM, GRU)\n","        ### that you would like. Additionally, you are free to use any hyperparameters\n","        ### that you would like (e.g. number of layers). You will discuss your choice\n","        ### of hyperparameters in the write up later as well.\n","        ###\n","        ### Use the following docs to properly initialize these variables:\n","        ###     RNN:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.RNN\n","        ###     LSTM:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n","        ###     Linear Layer:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n","\n","        \n","    \n","    def forward(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        \"\"\"\n","        \"\"\"\n","        enc_hiddens, dec_init_state = None, None\n","\n","        # print('16 ==', len(source_lengths))\n","        batch_size = config[\"Train batch size\"]  # len(source_lengths)\n","        src_len = np.max(source_lengths)\n","        # X = torch.zeros(src_len, batch_size, self.embed_size)\n","        # for i in range(batch_size):\n","        #   for j in range(source_lengths[i]):\n","        #     X[j, i] = self.embedding(source_padded[j, i])  # check i,j vs j,i\n","        X = self.embedding(source_padded)\n","\n","        padded_X = nn.utils.rnn.pack_padded_sequence(X, source_lengths, enforce_sorted=False)\n","        enc_hiddens, last_hidden = self.encoder(padded_X)\n","\n","        enc_hiddens = nn.utils.rnn.pad_packed_sequence(enc_hiddens, batch_first=True)\n","        enc_hiddens = enc_hiddens[0]\n","   \n","        last_hidden = torch.cat([last_hidden[0], last_hidden[1]], 1)\n","\n","        dec_init_state = self.h_projection(last_hidden)\n","\n","        ### YOUR CODE HERE (~ 8 Lines)\n","        ### TODO:\n","        ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.\n","        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n","        ###         that there is no initial hidden state or cell for the encoder.\n","        ###     2. Compute `enc_hiddens`, `last_hidden` by applying the encoder to `X`.\n","        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.\n","        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n","        ###         - Note that the shape of the tensor returned by the encoder is (src_len, b, h*2) and we want to\n","        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.\n","        ###     3. Compute `dec_init_state` = init_decoder_hidden:\n","        ###         - `init_decoder_hidden`:\n","        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forward and backwards.\n","        ###             Concatenate the forward and backward tensors to obtain a tensor shape (b, 2*h).\n","        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n","        ###             This is h_0^{dec} in above in the writeup. Here b = batch size, h = hidden size\n","        ###\n","        ### See the following docs, as you may need to use some of the following functions in your implementation:\n","        ###     Pack the padded sequence X before passing to the encoder:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n","        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n","        ###     Tensor Concatenation:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n","        ###     Tensor Permute:\n","        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute\n","        \n","\n","        ### END YOUR CODE\n","\n","        return enc_hiddens, dec_init_state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zj-HTuLCcWum"},"source":["class Decoder(nn.Module):\n","    def __init__(self, embed_size, hidden_size, target_embedding, device):\n","        \"\"\"\n","        \"\"\"\n","        super(Decoder, self).__init__()\n","        self.embed_size = embed_size\n","        self.hidden_size = hidden_size\n","        self.device = device\n","        self.embedding = target_embedding\n","        output_vocab_size = self.embedding.weight.size(0)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","        self.decoder = nn.RNNCell(embed_size+hidden_size, hidden_size, bias=True)\n","        self.combined_output_projection = nn.Linear(hidden_size, hidden_size, bias=False)\n","        self.target_vocab_projection = nn.Linear(hidden_size,output_vocab_size, bias=False)\n","        ### YOUR CODE HERE (~3 lines)\n","        ###     self.decoder (RNN Cell with bias)\n","        ###     self.combined_output_projection (Linear Layer with no bias), called W_{v} above.\n","        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} above.\n","        ###\n","        ### Use the following docs to properly initialize these variables:\n","        ###     RNN Cell:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell\n","        ###     LSTM Cell:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n","        ###     Linear Layer:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n","\n","\n","\n","    \n","    def forward(self, enc_hiddens: torch.Tensor,\n","                dec_init_state: torch.Tensor, target_padded: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        \"\"\"\n","        # Chop of the <END> token for max length sentences.\n","        target_padded = target_padded[:-1]\n","\n","        dec_state = dec_init_state\n","\n","        # Initialize previous combined output vector o_{t-1} as zero\n","        batch_size = enc_hiddens.size(0)\n","        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n","\n","        # Initialize a list we will use to collect the combined output o_t on each step\n","        combined_outputs = []\n","\n","        ### YOUR CODE HERE (~9 Lines)\n","        tgt_len = target_padded.shape[0]\n","        batch_size = config[\"Train batch size\"]  # (target_padded.shape[0])\n","        # y = torch.zeros(tgt_len, batch_size, self.embed_size).to(self.device)\n","        # for i in range(batch_size):\n","        #   for j in range(target_padded.shape[0]):\n","        #     y[j, i] = self.embedding(target_padded[j, i])\n","        y = self.embedding(target_padded)\n","\n","        split_y = torch.split(y, 1, dim=0)\n","        for Y_t in split_y:\n","          Y_t = torch.squeeze(Y_t, dim=0)\n","          Ybar_t = torch.cat([Y_t, o_prev], dim=1)\n","          dec_state, o_t = self.step(Ybar_t, dec_state, enc_hiddens)\n","          combined_outputs.append(o_t)\n","          o_prev = o_t\n","        combined_outputs = torch.stack(combined_outputs, dim=0)\n","\n","        ### TODO:\n","        ###     1. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.\n","        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n","        ###     2. Use the torch.split function to iterate over the time dimension of Y.\n","        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n","        ###             - Squeeze Y_t into a tensor of dimension (b, e). \n","        ###             - Construct Ybar_t by concatenating Y_t with o_prev on their last dimension\n","        ###             - Use the step function to compute the the Decoder's next (cell, state) values\n","        ###               as well as the new combined output o_t.\n","        ###             - Append o_t to combined_outputs\n","        ###             - Update o_prev to the new o_t.\n","        ###     3. Use torch.stack to convert combined_outputs from a list length tgt_len of\n","        ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)\n","        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n","        ###\n","        ### Note:\n","        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n","        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n","        ###   \n","        ### You may find some of these functions useful:\n","        ###     Zeros Tensor:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros\n","        ###     Tensor Splitting (iteration):\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.split\n","        ###     Tensor Dimension Squeezing:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n","        ###     Tensor Concatenation:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n","        ###     Tensor Stacking:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.stack\n","\n","\n","\n","        ### END YOUR CODE\n","\n","        return combined_outputs\n","    \n","    def step(self, Ybar_t: torch.Tensor,\n","            dec_state: Tuple[torch.Tensor, torch.Tensor],\n","            enc_hiddens: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n","        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n","\n","        :param Ybar_t: Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n","                                where b = batch size, e = embedding size, h = hidden size.\n","        :type Ybar_t: torch.Tensor\n","        :param dec_state: Tensors with shape (b, h), where b = batch size, h = hidden size.\n","                Tensor is decoder's prev hidden state\n","        :type dec_state: torch.Tensor\n","        :param enc_hiddens: Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n","                                    src_len = maximum source length, h = hidden size.\n","        :type enc_hiddens: torch.Tensor\n","\n","        :returns dec_state: Tensors with shape (b, h), where b = batch size, h = hidden size.\n","                Tensor is decoder's new hidden state. For an LSTM, this should be a tuple\n","                of the hidden state and cell state.\n","        returns combined_output: Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n","        \"\"\"\n","\n","        combined_output = None\n","\n","        ### YOUR CODE HERE (~2 Lines)\n","        dec_state = self.decoder(Ybar_t, dec_state)\n","        dec_hidden = dec_state\n","        ### TODO:\n","        ###     1. Apply the decoder to `Ybar_t` and `dec_state` to obtain the new dec_state.\n","        ###     2. Rename dec_state to dec_hidden\n","        ###\n","        ###       Hints:\n","        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t above\n","        ###\n","\n","\n","        ### END YOUR CODE\n","\n","\n","        ### YOUR CODE HERE (~2 Lines)\n","        V_t = self.combined_output_projection(dec_hidden)\n","        O_t = torch.tanh(V_t)\n","        ### TODO:\n","        ###     1. Apply the combined output projection layer to h^dec_t to compute tensor V_t\n","        ###     2. Compute tensor O_t by applying the Tanh function.\n","        ###\n","        ### Use the following docs to implement this functionality:\n","        ###     Softmax:\n","        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax\n","        ###     Batch Multiplication:\n","        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n","        ###     Tensor View:\n","        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n","        ###     Tensor Concatenation:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n","        ###     Tanh:\n","        ###         https://pytorch.org/docs/stable/torch.html#torch.tanh\n","\n","        # P_t = self.softmax(self.target_vocab_projection(O_t))\n","        ### END YOUR CODE\n","\n","        combined_output = O_t\n","        return dec_state, combined_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HtWqP3WNcYH3"},"source":["class NMT(nn.Module):\n","    \"\"\" Simple Neural Machine Translation Model:\n","        - Bidrectional RNN Encoder\n","        - Unidirection RNN Decoder\n","    \"\"\"\n","    def __init__(self, embed_size, hidden_size, src_vocab, tgt_vocab, device=torch.device(\"cpu\"), pretrained_source=None,pretrained_target=None,):\n","        \"\"\" Init NMT Model.\n","\n","        :param embed_size: Embedding size (dimensionality)\n","        :type embed_size: int\n","        :param hidden_size: Hidden Size, the size of hidden states (dimensionality)\n","        :type hidden_size: int\n","        :param src_vocab: Vocabulary object containing src language\n","        :type src_vocab: Vocab\n","        :param tgt_vocab: Vocabulary object containing tgt language\n","        :type tgt_vocab: Vocab\n","        :param device: torch device to put all modules on\n","        :type device: torch.device\n","        :param pretrained_source: Matrix of pre-trained source word embeddings\n","        :type pretrained_source: Optional[torch.Tensor]\n","        :param pretrained_target: Matrix of pre-trained target word embeddings\n","        :type pretrained_target: Optional[torch.Tensor]\n","        \"\"\"\n","        super(NMT, self).__init__()\n","        self.device=device\n","        self.embed_size = embed_size\n","        self.src_vocab = src_vocab\n","        self.tgt_vocab = tgt_vocab\n","        src_pad_token_idx = src_vocab['<pad>']\n","        tgt_pad_token_idx = tgt_vocab['<pad>']\n","        self.source_embedding = nn.Embedding(len(src_vocab), embed_size, padding_idx=src_pad_token_idx)\n","        self.target_embedding = nn.Embedding(len(tgt_vocab), embed_size, padding_idx=tgt_pad_token_idx)\n","        \n","        with torch.no_grad():\n","            if pretrained_source is not None:\n","                self.source_embedding.weight.data = pretrained_source\n","                # TODO: Decide if we want the embeddings to update as we train\n","                self.source_embedding.weight.requires_grad = False\n","        \n","            if pretrained_target is not None:\n","                self.target_embedding.weight.data = pretrained_target\n","                # TODO: Decide if we want the embeddings to update as we train\n","                self.target_embedding.weight.requires_grad = False\n","        \n","        self.hidden_size = hidden_size\n","\n","        self.encoder = Encoder(\n","            embed_size=embed_size,\n","            hidden_size=hidden_size,\n","            source_embeddings=self.source_embedding,\n","        )\n","        self.decoder = Decoder(\n","            embed_size=embed_size,\n","            hidden_size=hidden_size,\n","            target_embedding=self.target_embedding,\n","            device=self.device,\n","        )\n","\n","\n","    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n","        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n","        target sentences under the language models learned by the NMT system.\n","\n","        :param source: list of source sentence tokens\n","        :type source: List[List[str]]\n","        :param target: list of target sentence tokens, wrapped by `<s>` and `</s>`\n","        :type target: List[List[str]]\n","        :returns scores: a variable/tensor of shape (b, ) representing the\n","                                    log-likelihood of generating the gold-standard target sentence for\n","                                    each example in the input batch. Here b = batch size.\n","        :rtype: torch.Tensor\n","        \"\"\"\n","        # Compute sentence lengths\n","        source_lengths = [len(s) for s in source]\n","\n","        # Convert list of lists into tensors\n","        source_padded = self.src_vocab.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n","        target_padded = self.tgt_vocab.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n","        \n","        ###     Run the network forward:\n","        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n","        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n","        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n","        ###     4. Compute log probability distribution over the target vocabulary using the\n","        ###        combined_outputs returned by the `self.decode()` function.\n","\n","        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n","        enc_masks = generate_sent_masks(enc_hiddens, source_lengths, self.device)\n","        combined_outputs = self.decode(enc_hiddens, dec_init_state, target_padded)\n","        P = F.log_softmax(self.decoder.target_vocab_projection(combined_outputs), dim=-1)\n","\n","        # Zero out, probabilities for which we have nothing in the target text\n","        target_masks = (target_padded != self.tgt_vocab['<pad>']).float()\n","        \n","        # Compute log probability of generating true target words\n","        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n","        scores = target_gold_words_log_prob.sum(dim=0)\n","        return scores\n","\n","\n","    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n","            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n","\n","        :param source_padded: Tensor of padded source sentences with shape (src_len, b), where\n","            b = batch_size, src_len = maximum source sentence length. Note that these have\n","            already been sorted in order of longest to shortest sentence.\n","        :type source_padded: torch.Tensor\n","        :param source_lengths: List of actual lengths for each of the source sentences in the batch\n","        :type source_lengths: List[int]\n","        :returns: Tuple of two items. The first is Tensor of hidden units with shape (b, src_len, h*2),\n","            where b = batch size, src_len = maximum source sentence length, h = hidden size. The second is\n","            Tuple of tensors representing the decoder's initial hidden state and cell.\n","        :rtype: Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n","        \"\"\"\n","        return self.encoder(source_padded, source_lengths)\n","\n","\n","    def decode(self, enc_hiddens: torch.Tensor,\n","                dec_init_state: torch.Tensor, target_padded: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Compute combined output vectors for a batch.\n","\n","        :param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n","                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n","        :param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n","        :param target_padded: Gold-standard padded target sentences (tgt_len, b), where\n","                                       tgt_len = maximum target sentence length, b = batch size. \n","\n","        :returns combined_outputs: combined output tensor  (tgt_len, b,  h), where\n","                                    tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n","        :rtype: torch.Tensor\n","        \"\"\"\n","        return self.decoder(enc_hiddens, dec_init_state, target_padded)\n","\n","    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n","        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n","        :param src_sent: a single source sentence (words)\n","        :type src_sent: List[str]\n","        :param beam_size: beam size\n","        :type beam_size: int\n","        :param max_decoding_time_step: maximum number of time steps to unroll the decoding RNN\n","        :type max_decoding_time_step: int\n","        :returns hypotheses: a list of hypothesis, each hypothesis has two fields:\n","                value: List[str]: the decoded target sentence, represented as a list of words\n","                score: float: the log-likelihood of the target sentence\n","        :rtype: List[Hypothesis]\n","        \"\"\"\n","        src_sents_var = self.src_vocab.to_input_tensor([src_sent], self.device)\n","\n","        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n","\n","        h_tm1 = dec_init_vec\n","        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n","\n","        eos_id = self.tgt_vocab['</s>']\n","\n","        hypotheses = [['<s>']]\n","        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n","        completed_hypotheses = []\n","\n","        t = 0\n","        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n","            t += 1\n","            hyp_num = len(hypotheses)\n","\n","            exp_src_encodings = src_encodings.expand(hyp_num,\n","                                                     src_encodings.size(1),\n","                                                     src_encodings.size(2))\n","\n","            y_tm1 = torch.tensor([self.tgt_vocab[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n","            y_t_embed = self.target_embedding(y_tm1)\n","\n","            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n","\n","            h_t, att_t = self.decoder.step(x, h_tm1,\n","                                exp_src_encodings)\n","            \n","            ## TODO: Uncomment the line below if this is an LSTM\n","            # h_t, c_t = h_t\n","\n","            # log probabilities over target words\n","            log_p_t = F.log_softmax(self.decoder.target_vocab_projection(att_t), dim=-1)\n","\n","            live_hyp_num = beam_size - len(completed_hypotheses)\n","            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n","            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n","\n","            prev_hyp_ids = torch.div(top_cand_hyp_pos, len(self.tgt_vocab), rounding_mode='floor')\n","            hyp_word_ids = top_cand_hyp_pos % len(self.tgt_vocab)\n","\n","            new_hypotheses = []\n","            live_hyp_ids = []\n","            new_hyp_scores = []\n","\n","            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n","                prev_hyp_id = prev_hyp_id.item()\n","                hyp_word_id = hyp_word_id.item()\n","                cand_new_hyp_score = cand_new_hyp_score.item()\n","\n","                hyp_word = self.tgt_vocab.id2word[hyp_word_id]\n","                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n","                if hyp_word == '</s>':\n","                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n","                                                           score=cand_new_hyp_score))\n","                else:\n","                    new_hypotheses.append(new_hyp_sent)\n","                    live_hyp_ids.append(prev_hyp_id)\n","                    new_hyp_scores.append(cand_new_hyp_score)\n","\n","            if len(completed_hypotheses) == beam_size:\n","                break\n","\n","            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n","\n","            h_tm1 = h_t[live_hyp_ids]\n","            ### TODO: Uncomment the below if it is an LSTM and comment out line\n","            # above. Otherwise leave.\n","            # h_tm1 = h_t[live_hyp_ids], c_t[live_hyp_ids]\n","            att_tm1 = att_t[live_hyp_ids]\n","\n","            hypotheses = new_hypotheses\n","            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n","\n","        if len(completed_hypotheses) == 0:\n","            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n","                                                   score=hyp_scores[0].item()))\n","\n","        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n","\n","        return completed_hypotheses\n","\n","\n","    def greedy(self, src_sent: List[str], max_decoding_time_step: int=70) -> List[Hypothesis]:\n","        return self.beam_search(src_sent, beam_size=1, max_decoding_time_step=max_decoding_time_step)\n","\n","\n","    @staticmethod\n","    def load(model_path: str):\n","        \"\"\" Load the model from a file.\n","        @param model_path (str): path to model\n","        \"\"\"\n","        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n","        args = params['args']\n","        model = NMT(\n","            src_vocab=params['vocab']['source'],\n","            tgt_vocab=params['vocab']['target'],\n","            **args\n","        )\n","        model.load_state_dict(params['state_dict'])\n","\n","        return model\n","\n","    def save(self, path: str):\n","        \"\"\" Save the model to a file.\n","        @param path (str): path to the model\n","        \"\"\"\n","        print('save model parameters to [%s]' % path, file=sys.stderr)\n","\n","        params = {\n","            'args': dict(embed_size=self.embed_size, hidden_size=self.hidden_size),\n","            'vocab': dict(source=self.src_vocab, target=self.tgt_vocab),\n","            'state_dict': self.state_dict()\n","        }\n","\n","        torch.save(params, path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNAHNVKnniOi"},"source":["def batch_iter(data, batch_size, shuffle=False):\n","    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n","    :param data: list of tuples containing source and target sentence. ie.\n","        (list of (src_sent, tgt_sent))\n","    :type data: List[Tuple[List[str], List[str]]]\n","    :param batch_size: batch size\n","    :type batch_size: int\n","    :param shuffle: whether to randomly shuffle the dataset\n","    :type shuffle: boolean\n","    \"\"\"\n","    batch_num = math.ceil(len(data) / batch_size)\n","    index_array = list(range(len(data)))\n","\n","    if shuffle:\n","        np.random.shuffle(index_array)\n","\n","    for i in range(batch_num):\n","        indices = index_array[i * batch_size: (i + 1) * batch_size]\n","        examples = [data[idx] for idx in indices]\n","\n","        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n","        src_sents = [e[0] for e in examples]\n","        tgt_sents = [e[1] for e in examples]\n","\n","        yield src_sents, tgt_sents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wGIm-Vo8mWot"},"source":["def evaluate_ppl(model, val_data, batch_size=32):\n","    \"\"\" Evaluate perplexity on dev sentences\n","    :param model: NMT Model\n","    :type model: NMT\n","    :param dev_data: list of tuples containing source and target sentence.\n","        i.e. (list of (src_sent, tgt_sent))\n","    :param val_data: List[Tuple[List[str], List[str]]]\n","    :param batch_size: size of batches to extract\n","    :type batch_size: int\n","    :returns ppl: perplexity on val sentences\n","    \"\"\"\n","    was_training = model.training\n","    model.eval()\n","\n","    cum_loss = 0.\n","    cum_tgt_words = 0.\n","\n","    # no_grad() signals backend to throw away all gradients\n","    with torch.no_grad():\n","        for src_sents, tgt_sents in batch_iter(val_data, batch_size):\n","            loss = -model(src_sents, tgt_sents).sum()\n","\n","            cum_loss += loss.item()\n","            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            cum_tgt_words += tgt_word_num_to_predict\n","\n","        ppl = np.exp(cum_loss / cum_tgt_words)\n","        avg_val_loss = cum_loss / len(val_data)\n","        wandb.log({\n","            \"Average val lost\" : avg_val_loss,\n","            \"Average val perplexity\" : ppl\n","        })\n","\n","    if was_training:\n","        model.train()\n","\n","    return ppl\n","\n","\n","def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n","    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n","    :param references: a list of gold-standard reference target sentences\n","    :type references: List[List[str]]\n","    :param hypotheses: a list of hypotheses, one for each reference\n","    :type hypotheses: List[Hypothesis]\n","    :returns bleu_score: corpus-level BLEU score\n","    \"\"\"\n","    if references[0][0] == '<s>':\n","        references = [ref[1:-1] for ref in references]\n","    bleu_score = corpus_bleu([[ref] for ref in references],\n","                             [hyp.value for hyp in hypotheses])\n","    return bleu_score\n","\n","\n","def evaluate_bleu(references, model, source):\n","    \"\"\"Generate decoding results and compute BLEU score.\n","    :param model: NMT Model\n","    :type model: NMT\n","    :param references: a list of gold-standard reference target sentences\n","    :type references: List[List[str]]\n","    :param source: a list of source sentences\n","    :type source: List[List[str]]\n","    :returns bleu_score: corpus-level BLEU score\n","    \"\"\"\n","    with torch.no_grad():\n","        top_hypotheses = []\n","        for s in tqdm(source, leave=False):\n","            hyps = model.beam_search(s, beam_size=16, max_decoding_time_step=(len(s)+10))\n","            top_hypotheses.append(hyps[0])\n","    \n","    s1 = compute_corpus_level_bleu_score(references, top_hypotheses)\n","    \n","    return s1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSq1z1L7lumv"},"source":["def train_and_evaluate(model, train_data, val_data, optimizer, epochs=10, train_batch_size=32, clip_grad=2, log_every = 100, valid_niter = 500, model_save_path=\"NMT_model.ckpt\"):\n","    num_trail = 0\n","    cum_examples = report_examples = epoch = valid_num = 0\n","    hist_valid_scores = []\n","    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n","\n","    print('Begin Maximum Likelihood training')\n","    train_time = begin_time = time.time()\n","\n","    val_data_tgt = [tgt for _, tgt in val_data]\n","    val_data_src = [src for src, _ in val_data]\n","\n","    for epoch in tqdm(range(epochs)):\n","        wandb.log({\"epoch\":epoch})\n","        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n","            train_iter += 1\n","            \n","            optimizer.zero_grad()\n","            \n","            batch_size = len(src_sents)\n","            \n","            example_losses = -model(src_sents, tgt_sents)\n","            batch_loss = example_losses.sum()\n","            loss = batch_loss / batch_size\n","            loss.backward()\n","            \n","            # clip gradient\n","            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n","            \n","            optimizer.step()\n","            \n","            batch_losses_val = batch_loss.item()\n","            report_loss += batch_losses_val\n","            cum_loss += batch_losses_val\n","            \n","            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n","            report_tgt_words += tgt_words_num_to_predict\n","            cum_tgt_words += tgt_words_num_to_predict\n","            report_examples += batch_size\n","            cum_examples += batch_size\n","\n","            if train_iter % log_every == 0:\n","                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n","                        'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n","                                                                                            report_loss / report_examples,\n","                                                                                            math.exp(report_loss / report_tgt_words),\n","                                                                                            cum_examples,\n","                                                                                            report_tgt_words / (time.time() - train_time),\n","                                                                                            time.time() - begin_time))\n","                wandb.log({\n","                    \"Average train loss\": (report_loss / report_examples),\n","                    \"Average train perplexity\": math.exp(report_loss / report_tgt_words)\n","                })\n","                train_time = time.time()\n","                report_loss = report_tgt_words = report_examples = 0.\n","\n","                \n","\n","            # perform validation\n","            if train_iter % valid_niter == 0:\n","                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n","                                                                                            cum_loss / cum_examples,\n","                                                                                            np.exp(cum_loss / cum_tgt_words),\n","                                                                                            cum_examples))\n","                \n","                cum_loss = cum_examples = cum_tgt_words = 0.\n","                valid_num += 1\n","\n","                print('begin validation ...')\n","\n","                # compute dev. ppl and bleu\n","                dev_ppl = evaluate_ppl(model, val_data, batch_size=128)   # dev batch size can be a bit larger\n","                valid_metric = -dev_ppl\n","                \n","                bleu_score = evaluate_bleu(val_data_tgt, model, val_data_src)*100\n","\n","                print('validation: iter %d, dev. ppl %f, bleu_score %f' % (train_iter, dev_ppl, bleu_score))\n","                wandb.log({\n","                    \"BLEU Score\": bleu_score\n","                })\n","                is_better = len(hist_valid_scores) == 0 or bleu_score > max(hist_valid_scores)\n","                hist_valid_scores.append(bleu_score)\n","\n","                if is_better:\n","                    print('save currently the best model to [%s]' % model_save_path)\n","                    model.save(model_save_path)\n","\n","                    # also save the optimizers' state\n","                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTsmtb0-D83_"},"source":["wandb.config ={\n","    \"Embed Size\": 300,\n","    \"Hidden Size\": 256,\n","    \"Epochs\": 10,\n","    \"Train batch size\": 32,\n","    \"Clip Grad\": 2,\n","    \"Learning Rate\":5e-4\n","}\n","config =wandb.config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ShIVo6llLvy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637722424235,"user_tz":300,"elapsed":188,"user":{"displayName":"Lusca Robinson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9N9oiCLT21lFXFt7ZQFYlnFQu9pN0W882pJvjkQ=s64","userId":"16217302501777754899"}},"outputId":"20173f8f-5071-4f7d-d15a-4a44c142731d"},"source":["# embed_size = 300\n","# #originally is 300\n","# hidden_size = 512\n","src_vocab = src\n","tgt_vocab = tgt\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":152}]},{"cell_type":"code","metadata":{"id":"r2aRBV96RcGe"},"source":["# epochs = 10\n","# train_batch_size = 32 # 16\n","# clip_grad = 2\n","log_every = 100 #100\n","valid_niter = 500 # 500\n","model_save_path=\"NMT_model.ckpt\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6pBNYqULlVDJ"},"source":["model = NMT(\n","    config[\"Embed Size\"],\n","    config[\"Hidden Size\"],\n","    src_vocab,\n","    tgt_vocab,\n","    device=device,\n","    pretrained_source=vocabEmbedding,\n","    pretrained_target=None,\n",")  #change pretrained_source to vocabEmbedding\n","# pretrained_source=None\n","model.to(device)\n","model.train()\n","optimizer = torch.optim.Adam(model.parameters(), lr=config[\"Learning Rate\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I11DvY80odkg"},"source":["# Define each of the variables then you can run this command!\n","# 21min 46s\n","wandb.init(project=\"p3-rnn\", entity=\"lusca\", reinit= True)\n","wandb.watch(model)\n","train_and_evaluate(\n","    model,\n","    train_data,\n","    val_data,\n","    optimizer,\n","    config[\"Epochs\"],\n","    config[\"Train batch size\"],\n","    config[\"Clip Grad\"],\n","    log_every,\n","    valid_niter,\n","    model_save_path\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qv8Ytb6puxuH"},"source":["import shutil\n","\n","#save weights onto google drive\n","shutil.move(\"NMT_model.ckpt\",\"drive/MyDrive/CS 4740/Project 3.2/Dataset/TE_EP1_TB32_HS264.ckpt\")\n"," \n","\n","#Load weights into model\n","# model = model.load(\"drive/MyDrive/CS 4740/Project 3.2/Dataset/NoPretrainedWeights.ckpt\")\n","# model.to(device)\n","# model.train()\n","# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_UbsqRZtWsp_"},"source":["## 1.2 Part 2 Report\n","For Section 1, your report should have a description of each major step of implementing the RNN accompanied by the associated code-snippet. Each step should have an explanation for why you decided to do something (when one could reasonably accomplish the same step in a different way); your justification should not be based on empirical results in this section but should relate to something we said in class, something mentioned in any of the course texts, or some other source (i.e. literature in NLP or official PyTorch documentation). **Unjustified, vague, and/or under-substantiated explanations will not receive credit.** As a reminder, the template for the write up is linked [here](https://docs.google.com/document/d/1IWgYqS6M4G_gJowM97Bq8g5smsGOa75dutIUGjolpAI/edit).\n","\n","Things to include:\n","\n","1. _Representation_ \\\n","Each $\\vec{x}_i$ needs to be produced in some way and should correspond to word $i$ in the text. This is different from the text classification approaches we have studied previously (BoW for example) where the entire document is represented with a single vector. Where and how is this being done for the RNN?\n","\n","2. _Initialization_ \\\n","There will be weights that you update in training the RNN. Where and how are these initialized?\n","\n","3. _Training_ \\\n","You are given the entire training set of N examples. How do you make use of this training set? How does the model modify its weights in training (this likely entails somewhere where gradients are computed and somehwere else where these gradients are used to update the model)? Note: This is code you may not have written but that we have written for you!\n","\n","4. _Model_ \\\n","This is the core model code, ie. where and how you apply the RNN to the $\\vec{x}_i$\n","\n","\n","5. _Stopping_ \\\n","How does your training procedure terminate? Note: This is code you may not have written but that we have written for you!\n","\n","6. _Hyperparameters_ \\\n","To run your model, you must fix some hyperparameters, such as $h$ (the hidden dimensionality of the $\\vec{z}_i$ referenced above). Be sure to exhaustively describe these hyperparameters and why you set them as you did ( this almost certainly will require some brief exploration: we suggest the course text by Yoav Goldberg as well as possibly the PyTorch official documentation). Be sure to accurately cite either source.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eA519cwBuhIW"},"source":["### 1.2.1 Representation\n"]},{"cell_type":"markdown","metadata":{"id":"-pqvbTja2Fmv"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"5MwU9A8jumDH"},"source":["### 1.2.2 Initialization\n"]},{"cell_type":"markdown","metadata":{"id":"fB2JyiWM2GX0"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"QHzl4RR6vZZO"},"source":["### 1.2.3 Training\n"]},{"cell_type":"markdown","metadata":{"id":"rx_wSNoq2KyT"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"rCBvcHgRvn8x"},"source":["### 1.2.4 Model\n"]},{"cell_type":"markdown","metadata":{"id":"hzUweVa72MoU"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Cjia6Ge32O4v"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"AEfJKPWlvvPO"},"source":["### 1.2.5 Stopping\n"]},{"cell_type":"markdown","metadata":{"id":"kb4Tc3Fe2Shr"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"wpT2Ntgmv0RD"},"source":["### 2.2.6 Hyperparameters\n"]},{"cell_type":"markdown","metadata":{"id":"kdSHK0HF2ipj"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"nJqQTVxdZjQ3"},"source":["# Part 2: Analysis\n","In **Part 2**, you will conduct a comprehensive analysis of these Neural Machine Translation models, focusing on two comparative settings."]},{"cell_type":"markdown","metadata":{"id":"szhOtAUuheQU"},"source":["## Part 2 Note\n","You will be required to submit the code used in finding these results on CMSX. This code should be legible and we will consult it if we find issues in the results. It is worth noting that in **Part 1** , we primarily are considering the correctness of the code-snippets in the report. If your model is flawed in a way that isn‚Äôt exposed by those snippets, this will likely surface in your results for **Part 2**. We will deduct points for correctness in this section to reflect this and we will try to localize where the error is (or think it is, if it is opaque from your code). That said, we will be lenient about absolute performance (within reason) in this section. As a reminder, the template for the write up is linked [here](https://docs.google.com/document/d/1IWgYqS6M4G_gJowM97Bq8g5smsGOa75dutIUGjolpAI/edit)."]},{"cell_type":"markdown","metadata":{"id":"-Of6kWAcfgde"},"source":["## Part 2.1: Within-model comparison\n","In **Part 2.1: Within-Model Comparison**, you will need to study what happens when you change parameters within a model.\n","\n","A large aspect of rigorous experimentation in NLP (and other domains) is the _ablation study_. In this, we _ablate_ or remove aspects of a more complex model, making it less complex, to evaluate whether each aspect was neccessary. To be concrete, for this part, you should train 4 variants of the RNN model and describe them as we do below:\n","\n","1. Baseline model\n","2. Baseline model made more complex by modification $A$ (e.g. changing the hidden dimensionality from $h$ to $2h$).\n","3. Baseline model made more complex by modification $B$ (where $B$ is an entirely distinct/different update from $A$).\n","4. Baseline model with both modifications $A$ and $B$ applied.\n","\n","Under the framing of an ablation study, you would describe this as beginning with model 4 and then ablating (i.e. removing) each of the two modifications, in turn; and then removing both to see if they were genuinely neccessary for the performance you observe. \n","\n","Once you describe each of the four models, report the quantitative bleu score and perplexity. Conclude by performing a nuanced analysis.\n","\n","The descriptive analysis can take one of two forms:\n","\n","1. _Nuanced quantitative analysis_ \\\n","If you choose this option, you will need to further break down the quantitative statistics you reported initially. We provide some initial strategies to prime you for what you should think about in doing this: one possible starting point is to consider: if model $X$ achieves greater accuracy than model $Y$, to what extent is $X$ getting everything correct that $Y$ gets correct? Alternatively, how is model performance affected if you measure performance on a specific strata/subset of the source sentences?\n","\n","2. _Nuanced qualitative analysis_ \\\n","If you choose this option, you will need to select individual examples and try to explain or reason about why one model may be getting them right whereas the other isn‚Äôt. Are there any examples that all 4 models get right or wrong and, if so, can you hypothesize a reason why this occurs?\n","\n","\n","**NOTE:** Although we code individual sections below for each of the configurations. The report should be written keeping all of them in mind discussing all of their performances as well as doing the nuanced analysis with _all_ of the models."]},{"cell_type":"markdown","metadata":{"id":"sIt_mg_H2b4h"},"source":["The function below will be useful for analyzing translations by piecing back together the prediction into a cohesive sequence of tokens."]},{"cell_type":"code","metadata":{"id":"hwKsMKaU2X0P"},"source":["import re\n","def untokenize(words):\n","    \"\"\"\n","    Untokenizing a text undoes the tokenizing operation, restoring\n","    punctuation and spaces to the places that people expect them to be.\n","    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n","    except for line breaks.\n","    \"\"\"\n","    text = ' '.join(words)\n","    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n","    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n","    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n","    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n","    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n","         \"can not\", \"cannot\")\n","    step6 = step5.replace(\" ` \", \" '\")\n","    return step6.strip()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TULtij8lzQpt"},"source":["### 2.1.1 Configuration 1\n","Modify the code below for this configuration."]},{"cell_type":"code","metadata":{"id":"NBoo8VMeD103"},"source":["baseline_nmt = NMT(300,\n","    256,\n","    src_vocab,\n","    tgt_vocab,\n","    device=device,\n","    pretrained_source=vocabEmbedding,\n","    pretrained_target=None,) #No pretrained ebeddings and batch size 16"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x2Q4LpZOzxa6"},"source":["### 2.1.2 Configuration 2\n","Modify the code below for this configuration."]},{"cell_type":"code","metadata":{"id":"jSE9bbSnGI2A"},"source":["mod_a_nmt = NMT(300,\n","    256,\n","    src_vocab,\n","    tgt_vocab,\n","    device=device,\n","    pretrained_source=vocabEmbedding,\n","    pretrained_target=None,)#Learning rate 5e-4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVG-Pqa9z60-"},"source":["### 2.1.3 Configuration 3\n","Modify the code below for this configuration."]},{"cell_type":"code","metadata":{"id":"4WsI_BPpGJ5r"},"source":["mod_b_nmt = NMT(300,\n","    256,\n","    src_vocab,\n","    tgt_vocab,\n","    device=device,\n","    pretrained_source=vocabEmbedding,\n","    pretrained_target=None,)#Hidden layer size 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tfukYWxcz-Za"},"source":["### 2.1.4 Configuration 4\n","Modify the code below for this configuration."]},{"cell_type":"code","metadata":{"id":"VbwLS9lEGLJU"},"source":["both_mod_nmt = NMT( 300,\n","    256,\n","    src_vocab,\n","    tgt_vocab,\n","    device=device,\n","    pretrained_source=vocabEmbedding,\n","    pretrained_target=None,) #2 hidden layers, learning rate 5e-4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-CiPPaO-0Dmy"},"source":["### 2.1.5 Report\n","Describe variants in the ablation style described, report the results, and then perform a nuanced analysis."]},{"cell_type":"markdown","metadata":{"id":"81c2YMpY3MYH"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"ACmdw-pyh4Lj"},"source":["# Part 3: Questions\n","In **Part 3**, you will need to answer the three questions below. We expect answers to be to-the-point; answers that are vague, meandering, or imprecise **will receive fewer points** than a precise but partially correct answer."]},{"cell_type":"markdown","metadata":{"id":"juQW_iF_ETtg"},"source":["## 3.1 Q1\n","Earlier in the course, we studied models that make use of _Markov_ assumptions. Recurrent neural networks do not make any such assumption. That said, RNNs are known to struggle with long-distance dependencies. What is a fundamental reason for why this is the case?"]},{"cell_type":"markdown","metadata":{"id":"HIZAmdtoVpBK"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"8CmtdmOWEboE"},"source":["## 3.2 Q2\n","In applying RNNs to tasks in NLP, we have discovered that (at least for tasks in English) feeding a sentence into an RNN backwards (i.e. inputting the sequence of vectors corresponding to ($course$, $great$, $a$, $is$, $NLP$) instead of ($NLP$, $is$, $a$, $great$, $course$)) tends to improve performance. Why might this be the case?"]},{"cell_type":"markdown","metadata":{"id":"i69IrRd4Vpdh"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"zB6y4__iEk6_"},"source":["## 3.3 Q3\n","In using RNNs and word embeddings for NLP tasks, we are no longer required to engineer specific features that are useful for the task; the model discovers them automatically. Stated differently, it seems that neural models tend to discover better features than human researchers can directly specify. This comes at the cost of systems having to consume tremendous amounts of data to learn these kinds of patterns from the data. Beyond concerns of dataset size (and the computational resources required to process and train using this data as well as the further environmental harm that results from this process), why might we disfavor RNN models?"]},{"cell_type":"markdown","metadata":{"id":"cOdh784UVp9f"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"bD0pgD0Jh84T"},"source":["# Part 4: Miscellaneous\n","List the libraries you used and sources you referenced and cited (labelled with the section in which you referred to them). Include a description of how your group split\n","up the work. Include brief feedback on this asignment."]},{"cell_type":"markdown","metadata":{"id":"snL2qqbR3SbN"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"4VeQ8uBwiR-a"},"source":["**Each section must be clearly labelled, complete, and the corresponding pages should be correctly assigned to the corresponding Gradescope rubric item.** If you follow these steps for each of the 4 components requested, you are guaranteed full credit for this section. Otherwise, you will receive no credit for this section."]},{"cell_type":"markdown","metadata":{"id":"kqPxplOuZ876"},"source":["# Part 5: Gradescope Submission\n","\n","Note: This section is not required however we will have a Gradescope submission open to submit predictions and see how your models compare against one another!"]},{"cell_type":"code","metadata":{"id":"K5wMEwKX_4Hs"},"source":["# Create Gradescope submission function\n","gradescope_model = model\n","nmt_document_preprocessor = lambda x: nltk.word_tokenize(x) # This is for your RNN\n","file_name = \"submission.tsv\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFc6rb5y8ScE"},"source":["def generate_submission(filename, model, document_preprocessor, test):\n","    with Path(filename).open(\"w\") as fp:\n","        fp.write(\"Id\\tPredicted\\n\")\n","        for idx, input_string in tqdm(enumerate(test), total=len(test)):\n","            translation = untokenize(\n","                model.beam_search(\n","                    document_preprocessor(input_string),\n","                    beam_size=16,\n","                    max_decoding_time_step=len(input_string)+10\n","                )[0].value)\n","            fp.write(f\"{idx}\\t{translation}\\n\")\n","    return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMRiusKQ4UpF"},"source":["with open(test_path) as fp:\n","    test = [line for line in fp]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wu2n9Kn9-c1U","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["0a814c69e02143528d12d7711c5e6aa0","59966a8ba43142458aa857a8dda1c24b","cb8a6a122a114ac0904f6c7c7e41bf7e","3a8e99040c464fff9377728248fecede","f2cfd98a925b4e6cad163f0425c84fe8","edfbca8298744f4fa6c0763c1f593c10","658560341025401ba6fa830eec0dbe77","73f1b514f3ef407f8404d53ad196ff4f","afde21448d254ffba671d83361714adc","64d2f98e42f340508e17e7c26e430b41","fdb2466d543847d2b12d012f72c9fb54"]},"executionInfo":{"status":"ok","timestamp":1637717497236,"user_tz":300,"elapsed":76540,"user":{"displayName":"Lusca Robinson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9N9oiCLT21lFXFt7ZQFYlnFQu9pN0W882pJvjkQ=s64","userId":"16217302501777754899"}},"outputId":"90e038a9-d0cc-493c-fa98-02ac24f834f7"},"source":["generate_submission(file_name, gradescope_model, nmt_document_preprocessor, test)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a814c69e02143528d12d7711c5e6aa0","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1809 [00:00<?, ?it/s]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"8nu4zL4nWnGB"},"source":["# Live running demo"]},{"cell_type":"code","metadata":{"id":"4qO4HFGI92-5","colab":{"base_uri":"https://localhost:8080/","height":120},"executionInfo":{"status":"ok","timestamp":1637720323565,"user_tz":300,"elapsed":498,"user":{"displayName":"Lusca Robinson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9N9oiCLT21lFXFt7ZQFYlnFQu9pN0W882pJvjkQ=s64","userId":"16217302501777754899"}},"outputId":"9c8e007c-578f-41fc-ec92-3a50e6b00001"},"source":["#@title Translation\n","#@markdown Enter a sentence to see the translation\n","#input_string = \"why should i play the roman fool and die on my own sword?\" #@param {type:\"string\"}\n","#Possible sentences\n","# How was your day\n","# I want to eat\n","#So there seems to be some confusion regarding the initialization component of the writeup.  \n","# My mind is an Enigma\n","# I'm launching a rocketship to mars\n","input_string = \"So there seems to be some confusion regarding the initialization component of the writeup.\"\n","model_type = \"baseline_nmt\" #@param [\"baseline_nmt\", \"mod_a_nmt\", \"mod_b_nmt\", \"both_mods_nmt\"]\n","from IPython.display import HTML\n","\n","import re\n","def untokenize(words):\n","    \"\"\"\n","    Untokenizing a text undoes the tokenizing operation, restoring\n","    punctuation and spaces to the places that people expect them to be.\n","    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n","    except for line breaks.\n","    \"\"\"\n","    text = ' '.join(words)\n","    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n","    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n","    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n","    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n","    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n","         \"can not\", \"cannot\")\n","    step6 = step5.replace(\" ` \", \" '\")\n","    return step6.strip()\n","\n","output = \"\"\n","\n","# BAD THING TO DO BELOW!!\n","model_used = model\n","model_used.to(device)\n","\n","with torch.no_grad():\n","    # RUN MODEL\n","    translation = untokenize(model.beam_search(\n","        nmt_document_preprocessor(input_string),\n","        beam_size=64,\n","        max_decoding_time_step=len(input_string)+10\n","    )[0].value)\n","\n","# Generate nice display\n","output += '<p style=\"font-family:verdana; font-size:110%;\">'\n","output += \" Input sequence: \"+input_string+\"</p>\"\n","output += '<p style=\"font-family:verdana; font-size:110%;\">'\n","output += f\" Translation to Shakespeare: {translation}</p><hr>\"\n","output = \"<h3>Results:</h3>\" + output\n","\n","display(HTML(output))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<h3>Results:</h3><p style=\"font-family:verdana; font-size:110%;\"> Input sequence: So there seems to be some confusion regarding the initialization component of the writeup.</p><p style=\"font-family:verdana; font-size:110%;\"> Translation to Shakespeare: haply the brinded cat hath mew'd.</p><hr>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"wXv5-s305lZb","executionInfo":{"status":"ok","timestamp":1637723832520,"user_tz":300,"elapsed":5066,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["%%capture\n","!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ngINOBtG5mXK","executionInfo":{"status":"ok","timestamp":1637723836371,"user_tz":300,"elapsed":668,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}}},"source":["%%capture\n","# the red text is a placeholder! Change it to your directory structure!\n","!cp 'drive/My Drive/CS 4740/Project 3.2/CS_4740_FA21_p3_part2_nar73_krm74.ipynb' ./ "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXizf5E25xlx","executionInfo":{"status":"ok","timestamp":1637723845074,"user_tz":300,"elapsed":6460,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"5f1f9872-dfc2-4590-a998-f893e259ca4e"},"source":["!jupyter nbconvert --to PDF \"CS_4740_FA21_p3_part2_nar73_krm74.ipynb\""],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook CS_4740_FA21_p3_part2_nar73_krm74.ipynb to PDF\n","[NbConvertApp] Writing 181511 bytes to ./notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n","[NbConvertApp] Running bibtex 1 time: [u'bibtex', u'./notebook']\n","[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 159779 bytes to CS_4740_FA21_p3_part2_nar73_krm74.pdf\n"]}]}]}