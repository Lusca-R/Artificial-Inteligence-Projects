{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS_4740_FA21_p2_nar73_krm74.ipynb","provenance":[{"file_id":"18QtjH-VPmE0kK9sHX9pqy4J6xciFwFom","timestamp":1633026625415},{"file_id":"1HWDVwVz1tr2i96Q8sYZ0JRXV-anaJLHZ","timestamp":1632198445173},{"file_id":"1ngYyf9OC5IYItD8WTDkLU1HDsOa2mQna","timestamp":1602035552807}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZNdFlbLJoUXM"},"source":["# Project 2: Named Entity Recognition (NER) with Sequence Labeling Models\n","## CS4740/5740 Fall 2021\n","\n","### Project Submission Due: Oct 15th, 2021 (11.59PM)\n","Please submit **pdf file** of this notebook on **Gradescope**, and **ipynb** on **CMS**. For instructions on generating pdf and ipynb files, please refer to project 1 instructions.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"871f2XhgtoYX"},"source":["**Names: Lusca Robinson, Kyrus Mama**\n","\n","**Netids: nar73, krm74**\n","\n","Don't forget to share your newly copied notebook with your partner!\n","\n","\n","**Reminder: both of you can't work in this notebook at the same time from different computers/browser windows because of sync issues. We even suggest to close the tab with this notebook when you are not working on it so your partner doesn't get sync issues.**\n"]},{"cell_type":"markdown","metadata":{"id":"iguUiw0mor52"},"source":["# **Introduction** ðŸ”Ž\n","\n","---\n","\n","In this project, you will implement a model that identifies named entities in text and tags them with the appropriate label. Particularly, the task of this project is **Named Entity Recognition**. A primer on this task is provided further on. The given dataset is a modified version of the CoNLL-2003 ([Sang et al](https://arxiv.org/pdf/cs/0306050v1.pdf)) dataset. Please use the datasets that we have released to you instead of versions found online as we have made simplifications to the dataset for your benefit. Your task is to develop NLP models to identify these named entities automatically. We will treat this as a **sequence-tagging task**: for each token in the input text, assign one of the following 5 labels: **ORG** (Organization), **PER** (Person), **LOC** (Location), **MISC** (Miscellaneous), and **O** (Not Named Entity). More information about the dataset is provided later\n","\n","For this project, you will implement two sequence labeling approaches:\n","- Model 1 : a Hidden Markov Model (HMM)\n","- Model 2 : a Maximum Entropy Markov Model (MEMM), which is an adaptation of an HMM in which a Logistic Regression classifier (also known as a MaxEnt classifier) is used to obtain the lexical generation probabilities (i.e., the observation/emission probability matrix, so \"observations\" == \"emissions\" == \"lexical generations\"). Feature engineering is strongly suggested for this model!\n","\n","Implementation of the Viterbi algorithm (for finding the most likely tag sequence to assign to an input text) is required for both models above, so make sure that you understand it ASAP.\n","\n","You will implement and train two sequence tagging models, generate your predictions for the provided test set, and submit them to **Kaggle**. Please enter all code in this colab notebook and answer all the questions in the supporting document.\n","\n","To refresh your memory on HMMs, MEMMs, and Viterbi you can refer to **Jurafsky & Martin Ch. 8.3â€“8.5** and the lecture slides which can be found on EdStem."]},{"cell_type":"markdown","metadata":{"id":"mib4pTXj3hir"},"source":["## **Logistics**\n","\n","---\n","\n","- You **must** work in **groups of 2 students**. Students in the same group will get the same grade. Thus, you should make sure that everyone in your group contributes to the project. \n","- **Remember to form groups on BOTH CMS and Gradescope** or not all group members will receive grades. You can use make a post on EdStem to find a partner for this project.\n","- Please complete the written questions of this notebook in a clear and informative way. We have created a template document for you to answer the written questions. This document can be found [here](https://docs.google.com/document/d/1vnxYFS-rxxLOYfKG6YN35YktJZnzqQBhE1Mj1Xu7IF0/edit?usp=sharing). Please make a copy of this document for yourself and add your names and netids in the header and answer the written questions on it. You will need to submit this document to gradescope as well (do not forget to do this please!).\n","- At the end: please make sure to submit the following 3 items:\n","  1. PDF version of Colab notebook on Gradescope (instructions for converting to PDF are at the end).\n","  2. PDF version of Google Doc with written answers to the numbered questions on this colab on Gradescope.\n","  3. .ipynb version of your colab notebook on CMS.\n","\n","- Note: When submitting the PDF documents to Gradescope (colab notebook & writeup doc) please join/concatenate the PDFs and then submit them as one. You may do this any way you please. You can use [this](https://pdfjoiner.com/) website if you wish to."]},{"cell_type":"markdown","metadata":{"id":"q2z7TIHV3kCH"},"source":["## **Advice**\n","\n","---\n","\n","1. Please read through the entire notebook before you start coding. That might inform your code structure.\n","2. Grading breakdown is found at the end; please consult it.\n","3. Google colab does **not** provide good synchronization; we do not recommend multiple people to work on the same notebook at the same time.\n","4. The project is somewhat open ended. (\"But that's a good thing.  Really. It's more fun that way\", says Claire.) We will ask you to implement some model, but precise data structures and so on can be chosen by you. However, to integrate with Kaggle, you will need to submit Kaggle predictions using the given evaluation code (more instructions later).\n","5. You will be asked to fill in your code at various points of the document. You will also be asked to answer questions that analyze your results and motivate your implementation. Please answer these on an additional writeup document. A template has been provided to you."]},{"cell_type":"markdown","metadata":{"id":"kBsjyfPu5V4t"},"source":["## **Named Entity Recognition: A Primer**\n","\n","---\n","\n","Let us now take a look at the task at hand: Named Entity Recognition (NER). This section provides a brief introduction to the task and why it is important.\n","\n","**What is NER?**\n","NER refers to the information extraction technique of identifying and categorizing key information about entities within textual data. Let's look at an example: \n","\n","<br/>\n","\n","![picture](https://drive.google.com/uc?id=1mxwn1_2Ef16_MJeyl9jJwwR6IohUOeHO)\n","\n","<br/>\n","\n","In the above example, we can see that the text has numerous named entities that can be categorized as LOC (location), ORG (organization), PER (person), etc. Today, the task of NER has been overwhelmed by deep learning approaches. However, for this assignment, we will try to do NER using something simpler: HMMs and MEMMs. NER is important for a number of reasons and has a wide variety of use cases such as but not limited to:\n","  - Detect entities in search engines and voice assistants for more relavent search results.\n","  - Automatically parsing resumes.\n","  - ...and many more!\n","\n","\n","To read more on NER, we refer to any of the following sources:\n","1. Medium post [1](https://umagunturi789.medium.com/everything-you-need-to-know-about-named-entity-recognition-2a136f38c08f) and [2](https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d).\n","2. Try out [this](https://demo.allennlp.org/named-entity-recognition/named-entity-recognition) AlllenNLP demo!"]},{"cell_type":"markdown","metadata":{"id":"DSFfegs8LKY8"},"source":["## **Entity Level Mean F1**\n","\n","---\n","\n","Let's take a look at the metrics that you will focus on in this assignment. The standard measures to report for NER are recall, precision, and F1 score\n","(also called F-measure) evaluated at the **named entity level** (not at the token level). The code for this has been provided later under the validation section under Part 2. Please use this code when evaluating your models. \n","\n","\n","If P and T are the sets of predicted and true *named entity spans*, respectively, (e.g, the five named entity spans in the above example are \"Zifa\", \"Renate Goetschl\", \"Austria\", \"World Cup\", and \"Germany\") then\n","\n","####<center>Precision = $\\frac{|\\text{P}\\;\\cap\\;\\text{C}|}{|\\text{C}|}$ and Recall = $\\frac{|\\text{P}\\;\\cap\\;\\text{C}|}{|\\text{P}|}$.</center><br/>\n","\n","\n","####<center>F1 = $\\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$. </center><br/>\n","\n","For each type of named entity, e.g. *LOC*ation, *MISC*ellaneous, *ORG*anization and *PER*son, we calculate the F1 score as shown above, and take the mean of all these F1 scores to get the **Entity Level Mean F1** score for the test set. If $N$ is the total number of labels (i.e., named entity types), then\n","\n","####<center>Entity Level Mean F1 = $\\frac{\\sum_{i = 1}^{N} \\text{F1}_{{label}_i}}{N}$. </center>\n","\n","More details under the validation section in Part 2.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7iP63fHj5saG"},"source":["# **Part 1: Dataset** ðŸ“ˆ"]},{"cell_type":"markdown","metadata":{"id":"pljkH2ow5U9x"},"source":["Load the dataset as follows:\n","  1. Obtain the data from Kaggle at https://www.kaggle.com/c/cs4740-fa21-p2/data.\n","  2. Unzip the data. Put it into your google drive, and mount it on colab as per below:"]},{"cell_type":"code","metadata":{"id":"c3dQChuccqfN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634440698311,"user_tz":240,"elapsed":21311,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"a09d8444-03cc-41f8-f482-092ddda47b09"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"uFXI7NRHn1Cc"},"source":["import json\n","import math\n","\n","# TODO: please change the line below with your drive organization\n","path = os.path.join(os.getcwd(), \"drive\", \"MyDrive\", \"CS 4740\", \"Project 2\", \"Dataset\")\n","\n","with open(os.path.join(path,'train.json'), 'r') as f:\n","     train = json.loads(f.read())\n","\n","with open(os.path.join(path,'test.json'), 'r') as f:\n","     test = json.loads(f.read())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zfjKFeE_7T7C"},"source":["Here's a few things to note about the dataset above:\n","1. We have just loaded 2 json files: train and test. Please note that these files are different from the original release of the CoNNL-2003 since we have already processed and tokenized them for you. Hence, the documents are represented as a list of strings. Note that it is **not** split into separate training and development/validation sets. You will need to do this yourself as needed using the train set.\n","2. The train file contains the following 4 fields (each is a nested list): \n","  - **'text'** - actual input tokens\n","  - **'NER'** - the token-level entity tag (ORG/PER/LOC/MISC/O) where **O is used to denote tokens that are not part of any named entity**\n","  - **'POS'** - the part of speech tag (will be handy for feature engineering of the MEMM model)\n","  - **'index'** - index of the token in the dataset\n","3. The test data only has 'text', 'POS' and 'index' fields. You will need to submit your prediction of the 'NER' tag to Kaggle. More instructions on this later!"]},{"cell_type":"markdown","metadata":{"id":"cradDk-37G8L"},"source":["Let's take a look at a sample sentence from the dataset!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PvvRSlhb6sAR","executionInfo":{"status":"ok","timestamp":1634440699534,"user_tz":240,"elapsed":195,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"fcc4f263-b5a1-4071-8eb4-ad152f7e78b3"},"source":["print(train['text'][1])\n","print(train['index'][1])\n","print(train['POS'][1])\n","print(train['NER'][1])\n","\n","lst = []\n","NER_counts = {}\n","for i in range(len(train['NER'])):\n","  for j in range(len(train['NER'][i])):\n","    if train['NER'][i][j] not in NER_counts:\n","      NER_counts[train['NER'][i][j]] = 1\n","    else:\n","      NER_counts[train['NER'][i][j]] += 1\n","print(NER_counts)\n","\n","for i in range(len(train['text'])):\n","  lst.append(len(train['text'][i]))\n","  # print(len(train['text'][i]))\n","print(\"mean\", sum(lst)/len(lst))\n","all_NER_list =[] \n","for i in range(len(train['NER'])):\n","  all_NER_list.extend(train['NER'][i])\n","Oentries=0\n","for x in all_NER_list:\n","  if x == \"O\":\n","    Oentries += 1\n","print(Oentries/len(all_NER_list))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Polish', 'schoolgirl', 'blackmailer', 'wanted', 'textbooks', '.', 'GDANSK', ',', 'Poland', '1996-08-22', 'A', 'Polish', 'schoolgirl', 'blackmailed', 'two', 'women', 'with', 'anonymous', 'letters', 'threatening', 'death', 'and', 'later', 'explained', 'that', 'she', 'needed', 'money', 'for', 'textbooks', ',', 'police', 'said', 'on', 'Thursday', '.', '\"', 'The', '13-year-old', 'girl', 'tried', 'to', 'extract', '60', 'and', '70', 'zlotys', '(', '$', '22', 'and', '$', '26', ')', 'from', 'two', 'residents', 'of', 'Sierakowice', 'by', 'threatening', 'to', 'take', 'their', 'lives', ',', '\"', 'a', 'police', 'spokesman', 'said', 'in', 'the', 'nearby', 'northern', 'city', 'of', 'Gdansk', 'on', 'Thursday', '.', 'He', 'said', 'the', 'women', 'reported', 'the', 'blackmail', 'letters', 'and', 'police', 'caught', 'the', 'girl', 'on', 'Wednesday', 'as', 'she', 'tried', 'to', 'pick', 'up', 'the', 'cash', 'at', 'the', 'Sierakowice', 'railway', 'station', '.', '\"', 'Interviewed', 'in', 'the', 'presence', 'of', 'a', 'psychologist', ',', 'she', 'said', 'she', 'wanted', 'to', 'use', 'the', 'money', 'for', 'school', 'books', 'and', 'clothes', ',', '\"', 'spokesman', 'Kazimierz', 'Socha', 'told', 'Reuters', '.', 'He', 'said', 'the', 'case', 'of', 'the', 'girl', ',', 'from', 'a', 'poor', 'family', 'that', 'had', 'never', 'been', 'in', 'trouble', 'with', 'the', 'law', ',', 'would', 'go', 'before', 'a', 'special', 'court', 'dealing', 'with', 'underage', 'offenders', '.']\n","[254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426]\n","['JJ', 'NN', 'NN', 'VBD', 'NNS', '.', 'NNP', ',', 'NNP', 'CD', 'DT', 'JJ', 'NN', 'VBN', 'CD', 'NNS', 'IN', 'JJ', 'NNS', 'VBG', 'NN', 'CC', 'RB', 'VBN', 'IN', 'PRP', 'VBD', 'NN', 'IN', 'NNS', ',', 'NN', 'VBD', 'IN', 'NNP', '.', '\"', 'DT', 'JJ', 'NN', 'VBD', 'TO', 'VB', 'CD', 'CC', 'CD', 'NNS', '(', '$', 'CD', 'CC', '$', 'CD', ')', 'IN', 'CD', 'NNS', 'IN', 'NNP', 'IN', 'VBG', 'TO', 'VB', 'PRP$', 'NNS', ',', '\"', 'DT', 'NN', 'NN', 'VBD', 'IN', 'DT', 'JJ', 'JJ', 'NN', 'IN', 'NNP', 'IN', 'NNP', '.', 'PRP', 'VBD', 'DT', 'NNS', 'VBD', 'DT', 'NN', 'NNS', 'CC', 'NN', 'VBD', 'DT', 'NN', 'IN', 'NNP', 'IN', 'PRP', 'VBD', 'TO', 'VB', 'RP', 'DT', 'NN', 'IN', 'DT', 'NNP', 'NN', 'NN', '.', '\"', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBD', 'PRP', 'VBD', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'NNS', 'CC', 'NNS', ',', '\"', 'NN', 'NNP', 'NNP', 'VBD', 'NNP', '.', 'PRP', 'VBD', 'DT', 'NN', 'IN', 'DT', 'NN', ',', 'IN', 'DT', 'JJ', 'NN', 'WDT', 'VBD', 'RB', 'VBN', 'IN', 'NN', 'IN', 'DT', 'NN', ',', 'MD', 'VB', 'IN', 'DT', 'JJ', 'NN', 'VBG', 'IN', 'JJ', 'NNS', '.']\n","['MISC', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'O', 'O', 'MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PER', 'PER', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n","{'LOC': 6556, 'O': 135186, 'ORG': 8065, 'MISC': 3659, 'PER': 8875}\n","mean 214.73677248677248\n","0.8327286390991802\n"]}]},{"cell_type":"markdown","metadata":{"id":"HwcyAkEeAIJj"},"source":["As you can see, the above the sentence, \"Romania state budget soars in June.\", has already been tokenized into an array of word tokens. The index array corresponds to the index of the token in the entire dataset (not the sentence). The POS tags and the NER tags correspond to the given indices. For example, the token: **Romania** has:\n","  - index: 0\n","  - POS: 'NNP'\n","  - NER: **'ORG'**"]},{"cell_type":"markdown","metadata":{"id":"sO7go9ivDusU"},"source":["### **Q1: Initial Data Observations**\n","What are your initial observations after you explore the dataset?  Provide some quantitative data exploration. Assess dataset size, document lengths and the token-level NER class distribution, and the entity-level NER class distribution (skipping the 'O' label for the latter). Give some examples of sentences with their named entities bracketed, e.g. [[LOC Romania] state budget soars in June .] and [[ORG Zifa] said [PER Renate Goetschl] of [LOC Austria]...]. \n","\n","Present your findings in the supporting template document!"]},{"cell_type":"markdown","metadata":{"id":"4NYvfchqqBf6"},"source":["# **Part 2: Hidden Markov Model** ðŸ§¨\n","\n","---\n","\n","1. Code for counting and smoothing of labels and words and unkown word handing as necessary to support the Viterbi algorithm. (This is pretty much what you already know how to do from project 1.)\n","2. Build a Hidden Markov Model in accordance with the starter code that has been provided. If you wish to change this starter code you can. However, please ensure that your code is clear, concise, and, most important of all, modular. So break your implementation down into functions or write it within a class. We suggest you compute all probabilities in a log form when building the HMM.\n","3. An implementation of the **Viterbi algorithm** that can be used to infer token-level labels (identifying the appropriate named entity) for an input document. This process is commonly referred to as **decoding**. Bigram-based Viterbi is $ \\mathcal{O}(sm^2)$ where s is the length of the sentence and m is the number of tags. Your implementation should have similar efficiency. The code for this can be used later on for the MEMM too.\n","\n","Code of Academic Integrity:  We encourage collaboration regarding ideas, etc. However, please **do not copy code from online or share code with other students**. We will be running programs to detect plagiarism.\n"]},{"cell_type":"markdown","metadata":{"id":"-jUVJwSaE1tI"},"source":["## **Unknown Word Handling**\n","---"]},{"cell_type":"code","metadata":{"id":"44Bja4eQEMJR"},"source":["# Implement unknown word handling here! You may do this any way that you please\n","\n","UNKNOWN_TOKEN = \"<UNK>\"\n","\n","def unknown_handling(tokens):\n","  return_tokens = []\n","  known_words = {}\n","  for i in range(len(tokens)):\n","    sentence = []\n","    for j in range(len(tokens[i])):\n","      token = tokens[i][j]\n","      if token not in known_words:\n","        sentence.append(UNKNOWN_TOKEN)\n","        known_words[token] = 0\n","      else:\n","        sentence.append(token)\n","    return_tokens.append(sentence)\n","    \n","  return return_tokens\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLuTFUV5FA3m"},"source":["## **HMM Implementation**\n","\n","---\n","\n","The code below is just a suggestion for how you may go about building your HMM. Feel free to change it any way you see fit. In fact, you will probably have to tweak it a little to implement smoothing. In the skeleton code below, we have broken down the HMM into its three components: the transition matrix, the emission (i.e., lexical generation, observation) matrix, and the starting state probabilities. We suggest you implement them separately and then use them to build the HMM.\n","\n","Note: it may help to map your classes (named entity types) to discrete values rather than string labels as you will have to do this for your MEMM anyways. However, the HMM can be done without this."]},{"cell_type":"code","metadata":{"id":"NBJgSrRtFZuG"},"source":["# returns the transition probabilities\n","def build_transition_matrix(labels, k=1):\n","  transition = {}\n","  for i in range(len(labels)):\n","    for j in range(len(labels[i])-1):\n","      prev = labels[i][j]\n","      post = labels[i][j+1]\n","      if prev not in transition:\n","        transition[prev] = {}\n","\n","      if post not in transition[prev]:\n","        transition[prev][post] = 1\n","      else:\n","        transition[prev][post] += 1\n","\n","  classes = list(transition.keys())\n","\n","  for prev in transition.keys():\n","    total = 0\n","    for post in transition[prev].keys():\n","      total += transition[prev][post]\n","    for post in transition[prev].keys():\n","      # transition[prev][post] /= total\n","      transition[prev][post] =  (transition[prev][post] + k) / (total + k * len(classes))\n","\n","    for pot_post in classes:\n","      if pot_post not in transition[prev]:\n","        transition[prev][pot_post] = k / (total + k * len(classes))\n","\n","  return transition\n","\n","# print(build_transition_matrix(train['NER']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gRt-pjh4FvZ2"},"source":["# returns the emission probabilities\n","def build_emission_matrix(tokens, labels, k=0.0001):\n","  emission = {}\n","  for i in range(len(labels)):\n","    for j in range(len(labels[i])):\n","      label = labels[i][j]\n","      token = tokens[i][j]\n","\n","      if label not in emission:\n","        emission[label] = {}\n","\n","      if token not in emission[label]:\n","        emission[label][token] = 1\n","      else:\n","        emission[label][token] += 1\n","  \n","  classes = list(emission.keys())\n","  for label in classes:\n","    for i in range(len(labels)):\n","      for j in range(len(labels[i])): \n","        token = tokens[i][j]\n","        if token not in emission[label]:\n","          emission[label][token] = 0\n","\n","  # vocab of all words in training\n","  V = len(list(emission[classes[0]].keys()))\n","\n","  for label in emission.keys():\n","    total = 0\n","    for token in emission[label].keys():\n","      total += emission[label][token]\n","    for token in emission[label].keys():\n","      # emission[label][token] /= total\n","      emission[label][token] = (emission[label][token] + k) / (total + k * V)\n","  return emission \n","  \n","# print(build_emission_matrix(train['text'], train['NER']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTN9B6k0HK77"},"source":["# returns the starting state probabilities\n","def get_start_state_probs(labels):\n","  initial = {}\n","  for i in range(len(labels)):\n","    label = labels[i][0]\n","\n","    if label not in initial:\n","      initial[label] = 1\n","    else:\n","      initial[label] += 1\n","\n","  total = 0\n","  for key in initial:\n","    total += initial[key]\n","  for key in initial:\n","    initial[key] /= total\n","\n","  return initial\n","\n","# print(get_start_state_probs(train['NER']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LU3Ff-CKInqo"},"source":["# takes in the tokens & labels and returns a representation of the HMM\n","# call the three functions above in this function to build your HMM\n","def build_hmm(tokens, labels):\n","  HMM = {}\n","  HMM['initial'] = get_start_state_probs(labels)\n","  HMM['transition'] = build_transition_matrix(labels)\n","  HMM['emission'] = build_emission_matrix(unknown_handling(tokens), labels)\n","  return HMM\n","\n","# print(build_hmm(train['text'], train['NER']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pbw3RTHPI31j"},"source":["## **Viterbi Implementation**\n","\n","---\n","\n","At the end of your implementation, we expect a function or class that maps a sequence of tokens (observation) to a sequence of labels via the Viterbi algorithm."]},{"cell_type":"code","metadata":{"id":"C_q3U42lI3LQ"},"source":["def get_emission_smoothing(emission_matrix, label, token):\n","  if token in emission_matrix[label]:\n","    return math.log(emission_matrix[label][token])\n","\n","  # For words that exist somewhere in the emission matrix, if they dont occur \n","  # for the current label we return a very negative number, considering \n","  # those outcomes very unlikely\n","  # for possible_label in emission_matrix.keys():\n","  #   if token in emission_matrix[possible_label]:\n","  #     return -10000000 \n","\n","  # if a word doesnt appear in the emission matrix at all, we skip that word's\n","  # emission probabilities for all labels.\n","  # print(label, token, emission_matrix[label][UNKNOWN_TOKEN])\n","\n","  return math.log(emission_matrix[label][UNKNOWN_TOKEN])\n","\n","\n","def get_transition_smoothing(transition_matrix, pre, post):\n","  if pre in transition_matrix:\n","    if post in transition_matrix[pre]:\n","      return math.log(transition_matrix[pre][post])\n","  assert False\n","\n","\n","# takes in the hmm build above and an observation: list of tokens\n","# and returns the appropriate named entity mappings for the tokens\n","def viterbi(hmm, observation):\n","  labels = list(hmm['initial'].keys())\n","  initial = hmm['initial']\n","  lst = []\n","  BPTR = []\n","\n","  for i in range(len(observation)):\n","    token = observation[i]\n","    if i == 0:\n","      probs = {}\n","      for label in labels:\n","        probs[label] = math.log(hmm['initial'][label]) + get_emission_smoothing(hmm['emission'], label, token)\n","      lst.append(probs)\n","    else:\n","      BPTR.append({})\n","      probs = {}\n","      for label in labels:\n","        max_lst = []\n","        for prev_label in labels:\n","          max_lst.append(\n","              lst[i-1][prev_label] + get_transition_smoothing(hmm['transition'], prev_label, label)\n","          )\n","\n","        max_prob = max(max_lst)\n","        BPTR[i-1][label] = labels[max_lst.index(max_prob)]\n","        probs[label] = max_prob + get_emission_smoothing(hmm['emission'], label, token)\n","      lst.append(probs)\n","\n","  final_probs = [lst[-1][key] for key in labels]\n","  max_prob = max(final_probs)\n","  backpointer = labels[final_probs.index(max_prob)]\n","  final_labels = [backpointer]\n","\n","  BPTR.reverse()\n","  for dic in BPTR:\n","    backpointer = dic[backpointer]\n","    final_labels.append(backpointer)\n","  \n","  final_labels.reverse()\n","    \n","  # print(lst)\n","  # print(final_labels)\n","  return final_labels\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ie6NXgJAKOEM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634440700201,"user_tz":240,"elapsed":452,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"7b9a75bb-c92b-406b-ad77-75f6d8874287"},"source":["# here's a samplle observations that you can use to test your code\n","obs_1 = ['Cornell',\n"," 'University',\n"," 'is',\n"," 'located',\n"," 'in',\n"," 'Germany',\n"," 'and',\n"," 'was',\n"," 'founded',\n"," 'by',\n"," 'Polish',\n"," 'schoolgirl']\n","\n","obs_1 = ['Germany',\n"," 'University',\n"," 'is',\n"," 'located',\n"," 'in',\n"," 'Ithaca',\n"," 'and',\n"," 'was',\n"," 'founded',\n"," 'by',\n"," 'Ezra',\n"," 'Cornell']\n","\n","# obs_1 = \"Bavarian lawmaker Markus SÃ¶der says conservatives in Germany will likely face a new era in opposition. His assessment contrasts strongly with that of fellow conservative leader Armin Laschet.\".split()\n","\n","viterbi(observation=obs_1, hmm=build_hmm(train['text'], train['NER']))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['LOC', 'LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"2J8NdM_V_A4u"},"source":["## **Validation Step**\n","\n","---\n","\n","In this part of the project, we expect you to split the training data into train and validation datasets. You may use whatever split you see fit and use any external libraries to perform this split. You may want to look into the following function for splitting data: [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n","\n","Once you have split the data, train your HMM model on the training data and evaluate it on the validation data. Report **Entity Level Mean F1**, which was explained earlier. Please use the code we have provided below to compute this metric.\n","\n","Please also take a look into your misclassified cases, as we will be performing error analysis in the *Evaluation* section. We expect smoothing, unknown word handling and correct emission (i.e., lexical generation) probabilities."]},{"cell_type":"markdown","metadata":{"id":"oTAhu_TG1V0R"},"source":["Consider the example below. After getting a sequence of NER labels for the sequence of tokens from your Viterbi algorithm implementation, you need to convert the sequence of tokens, associated token indices and NER labels into a format which can be used to calculate **Entity Level Mean F1**. We do this by finding the starting and ending indices of the spans representing each entity (as given in the corpus) and adding it to a list that is associated with the label with which the spans are labelled. To score your validation data on Google Colab or your local device, you can get a dictionary format as shown in the picture below from the function **format_output_labels** of both the predicted and true label sequences, and use the two dictionaries as input to the **mean_f1** function.\n","\n","NOTE: We do **not** include the spans of the tokens labelled as \"O\" in the formatted dictionary output."]},{"cell_type":"markdown","metadata":{"id":"FYoQTBMAxojF"},"source":["![picture](https://docs.google.com/uc?export=download&id=1M57DEHgfusVPU_hlvmiOpkS3yn9GGEgj)"]},{"cell_type":"code","metadata":{"id":"HdOOQdN7D2rv"},"source":["def format_output_labels(token_labels, token_indices):\n","    \"\"\"\n","    Returns a dictionary that has the labels (LOC, ORG, MISC or PER) as the keys, \n","    with the associated value being the list of entities predicted to be of that key label. \n","    Each entity is specified by its starting and ending position indicated in [token_indices].\n","\n","    Eg. if [token_labels] = [\"ORG\", \"ORG\", \"O\", \"O\", \"ORG\"]\n","           [token_indices] = [15, 16, 17, 18, 19]\n","        then dictionary returned is \n","        {'LOC': [], 'MISC': [], 'ORG': [(15, 16), (19, 19)], 'PER': []}\n","\n","    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n","    :type token_labels: List[String]\n","    :parameter token_indices: A list of token indices (taken from the dataset) \n","                              corresponding to the labels in [token_labels].\n","    :type token_indices: List[int]\n","    \"\"\"\n","    label_dict = {\"LOC\":[], \"MISC\":[], \"ORG\":[], \"PER\":[]}\n","    prev_label = token_labels[0]\n","    start = token_indices[0]\n","    for idx, label in enumerate(token_labels):\n","      if prev_label != label:\n","        end = token_indices[idx-1]\n","        if prev_label != \"O\":\n","            label_dict[prev_label].append((start, end))\n","        start = token_indices[idx]\n","      prev_label = label\n","      if idx == len(token_labels) - 1:\n","        if prev_label != \"O\":\n","            label_dict[prev_label].append((start, token_indices[idx]))\n","    return label_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfjVJLNhL_fc"},"source":["# Code for mean F1\n","\n","import numpy as np\n","\n","def mean_f1(y_pred_dict, y_true_dict):\n","    \"\"\" \n","    Calculates the entity-level mean F1 score given the actual/true and \n","    predicted span labels.\n","    :parameter y_pred_dict: A dictionary containing predicted labels as keys and the \n","                            list of associated span labels as the corresponding\n","                            values.\n","    :type y_pred_dict: Dict<key [String] : value List[Tuple]>\n","    :parameter y_true_dict: A dictionary containing true labels as keys and the \n","                            list of associated span labels as the corresponding\n","                            values.\n","    :type y_true_dict: Dict<key [String] : value List[Tuple]>\n","\n","    Implementation modified from original by author @shonenkov at\n","    https://www.kaggle.com/shonenkov/competition-metrics.\n","    \"\"\"\n","    F1_lst = []\n","    for key in y_true_dict:\n","        TP, FN, FP = 0, 0, 0\n","        num_correct, num_true = 0, 0\n","        preds = y_pred_dict[key]\n","        trues = y_true_dict[key]\n","        for true in trues:\n","            num_true += 1\n","            if true in preds:\n","                num_correct += 1\n","            else:\n","                continue\n","        num_pred = len(preds)\n","        if num_true != 0:\n","            if num_pred != 0 and num_correct != 0:\n","                R = num_correct / num_true\n","                P = num_correct / num_pred\n","                F1 = 2*P*R / (P + R)\n","            else:\n","                F1 = 0      # either no predictions or no correct predictions\n","        else:\n","            continue\n","        F1_lst.append(F1)\n","    return np.mean(F1_lst)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rwLFPhetVok","executionInfo":{"status":"ok","timestamp":1634440700204,"user_tz":240,"elapsed":8,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"0b36ace9-82cd-420b-8712-eae463e08a85"},"source":["# Usage using above example\n","\n","pred_token_labels = [\"ORG\", \"O\", \"PER\", \"PER\", \"O\", \"LOC\", \"O\", \"O\", \"O\", \"O\", \"MISC\", \"O\", \"O\", \"O\", \"O\", \"LOC\"]\n","true_token_labels = [\"ORG\", \"O\", \"PER\", \"PER\", \"O\", \"LOC\", \"O\", \"O\", \"O\", \"O\", \"MISC\", \"MISC\", \"O\", \"O\", \"O\", \"LOC\"]\n","token_indices = [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n","\n","y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n","print(\"y_pred_dict is : \" + str(y_pred_dict))\n","y_true_dict = format_output_labels(true_token_labels, token_indices)\n","print(\"y_true_dict is : \" + str(y_true_dict))\n","\n","print(\"Entity Level Mean F1 score is : \" + str(mean_f1(y_pred_dict, y_true_dict)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["y_pred_dict is : {'LOC': [(18, 18), (28, 28)], 'MISC': [(23, 23)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n","y_true_dict is : {'LOC': [(18, 18), (28, 28)], 'MISC': [(23, 24)], 'ORG': [(13, 13)], 'PER': [(15, 16)]}\n","Entity Level Mean F1 score is : 0.75\n"]}]},{"cell_type":"code","metadata":{"id":"KmxUdk-iIHab"},"source":["# Evaluate/validate your model here\n","from sklearn.model_selection import train_test_split\n","\n","text_train, text_val, NER_train, NER_val, index_train, index_val = train_test_split(train['text'], train['NER'], train['index'], test_size=0.2)\n","# print(len(text_train), len(text_val))\n","\n","# print(text_train)\n","# print(text_val)\n","# print(NER_train)\n","# print(NER_val)\n","# print(index_train)\n","# print(index_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Lmx9z5WGAfq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634440702338,"user_tz":240,"elapsed":1282,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"6c67fab7-0a3a-4559-a973-df4905464c44"},"source":["hmm=build_hmm(text_train, NER_train)\n","predicted_labels = []\n","for i in range(len(text_val)):\n","  val_example = text_val[i]\n","  labels = viterbi(hmm, val_example)\n","  predicted_labels.append(labels)\n","\n","f1_lst = []\n","for i in range(len(NER_val)):\n","  pred_token_labels = predicted_labels[i]\n","  token_indices = index_val[i]\n","  true_token_labels = NER_val[i]\n","\n","  y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n","  # print(\"y_pred_dict is : \" + str(y_pred_dict))\n","  y_true_dict = format_output_labels(true_token_labels, token_indices)\n","  # print(\"y_true_dict is : \" + str(y_true_dict))\n","  f1 = mean_f1(y_pred_dict, y_true_dict)\n","  # print(\"Entity Level Mean F1 score is : \" + str(f1))\n","  f1_lst.append(f1)\n","\n","print(\"Mean f1 : \" + str(sum(f1_lst)/len(f1_lst)))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean f1 : 0.5917124620211327\n"]}]},{"cell_type":"markdown","metadata":{"id":"YRLi5TDyuSx_"},"source":["### **Q2.1: Explain your HMM Implementations**\n","\n","Explain how you implemented the HMM including the Viterbi algorithm (e.g. **which algorithms/data structures** you used). Make clear which parts were implemented from scratch vs. obtained via an existing package. Explain and motivate any design choices providing the intuition behind them (e.g. which methods you used for your HMM implementation, and why?). (Please answer on the written questions template document)\n"]},{"cell_type":"markdown","metadata":{"id":"DHbzRuil-yjG"},"source":["### **Q2.2: Results Analysis**\n","\n","Explain here how you evaluated the models. Summarize the performance of your system and any variations that you experimented with on the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis. (Please answer on the written questions template document)\n"]},{"cell_type":"markdown","metadata":{"id":"rTIPGnLFNc43"},"source":["### **Q2.3: Error Analysis**\n","When did the system work well? When did it fail?  Any ideas as to why? How might you improve the system? (Please answer on the written questions template document)\n"]},{"cell_type":"markdown","metadata":{"id":"mf6ziT36NteS"},"source":["### **Q2.4: What is the effect of unknown word handling and smoothing?**\n","(Please answer on the written questions template document)"]},{"cell_type":"markdown","metadata":{"id":"31e3sMHZrLWP"},"source":["# **Part 3: Maximum Entropy Markov Model** ðŸ’«\n","\n","---\n","\n","In this section, you will implement a Maximum Entropy Markov Model (**MEMM**) to perform the same NER task. Your model should consist of a MaxEnt classifier with Viterbi decoding. \n","\n","1. We have already performed tokenizations for documents. You can either use a MaxEnt classifier from an existing package or write the MaxEnt code yourself. **Important note:  MaxEnt classifiers are statistically equivalent to multi-class logistic regression, so you can use packages for multi-class LR instead of MaxEnt.**\n","\n","2. Use the classifier to learn a probability $P(t_i|features)$. You may replace either the lexical generation probability â€“ $P(w_i|t_i)$ â€“ or the transition probability â€“ $P(t_i|t_{iâˆ’1})$ â€“ in the HMM with it, or you may replace the entire *lexical generation probability * transition probability*  calculation â€“ $P (w_i|t_i) âˆ— P (t_i|t_{iâˆ’1)} â€“ $ in the HMM with it. \n","\n","3. To train such classifier, you need to pick some feature set. The content of the feature set is up to your choice. You should be trying different feature sets, and evaluate your choices on the validation set. Pick the feature set that performs overall the best according to the F1 measure. If you draw inspiration for your features from external sources, please link them in the code.\n","\n","4. Use your own implementation of the **Viterbi algorithm**, which you can modify from the one you developed for the HMM model. You will need the probabilities that you obtain from the MaxEnt classifier. \n","\n","5. Remember to use same training and validation split when evaluating the MEMM to have a **fair comparison** with your **HMM model**.\n","\n","\n","Please also take a look into your misclassified cases, as we will be performing error analysis in *Evaluation* section. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"PJIosHVJZ-1o"},"source":["\n","\n","---\n","Here's a summary of the workflow for Part 3:\n","\n","![alt text](https://drive.google.com/uc?export=view&id=14VfjW3yDyXLojWM_u0LeJYdDOSLkElBn)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8QGSijPUW_Bi"},"source":["Note that we have not provided any skeleton code for how you should do feature engineering since this is meant to be an open ended task and we want you to experiment with the dataset. However, please remember to make sure that you code is concise, clean, and readable! Ultimately, we expect a function or class  mapping a sequence of tokens to a sequence of labels. At the end of this section you should have done the following:\n","1. Extract Features\n","2. Build & Train MaxEnt\n","3. Call Viterbi when evaluating"]},{"cell_type":"markdown","metadata":{"id":"Pd2PwG4wYkhQ"},"source":["### **Feature Engineering**\n","---"]},{"cell_type":"code","metadata":{"id":"Pwo2Q2kZIziq"},"source":["from sklearn.linear_model import LogisticRegression\n","import pandas as pd\n","\n","# pd.get_dummies(train['POS'][0])\n","# COLUMNS_RESET = True\n","CLASSES = ['O', 'LOC', 'ORG', 'PER', 'MISC']\n","COLUMNS = ['\"', '$', \"''\", '(', ')', ',', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW',\n","           'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS',\n","           'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP',\n","           'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT',\n","           'WP', 'WP$', 'WRB']\n","\n","\n","def flatten(tokens, POSs, NERs, indices):\n","  POS_lst = []\n","  token_lst = []\n","  NER_lst = []\n","  indices_lst = []\n","  for i in range(len(tokens)):\n","    POS_lst.extend(POSs[i])\n","    token_lst.extend(tokens[i])\n","    NER_lst.extend([CLASSES.index(n) for n in NERs[i]])\n","    indices_lst.extend(indices[i])\n","  return token_lst, POS_lst, NER_lst, indices_lst\n","\n","# POS_lst = []\n","# token_lst = []\n","# NER_lst = []\n","# for i in range(len(train['POS'])):\n","#   POS_lst.extend(train['POS'][i])\n","#   token_lst.extend(train['text'][i])\n","#   NER_lst.extend([CLASSES.index(n) for n in train['NER'][i]])\n","\n","\n","# token_lst, POS_lst, NER_lst, indices_lst = flatten(train['text'], train['POS'], train['NER'], train['index'])\n","\n","# print(NER_lst)\n","# print(token_lst)\n"," \n","def feature_extraction(POS_lst):\n","  global COLUMNS_RESET\n","  features = pd.get_dummies(POS_lst)\n","\n","  # print(list(features.columns))\n","  # if COLUMNS_RESET:\n","  #   COLUMNS = list(features.columns)\n","  #   COLUMNS_RESET = False\n","\n","  for i in COLUMNS:\n","    if i not in features.columns:\n","      features[i] = 0\n","\n","  features = features[COLUMNS]\n","  features_copy = features.copy()\n","\n","  # Add previous word POS\n","  temp_features = features.drop(features_copy.shape[0]-1)\n","\n","  temp_features.loc[-1, :] = [0] * len(COLUMNS)\n","  temp_features = temp_features.sort_index()\n","\n","  for i in COLUMNS:\n","    col_name = \"Prev_\" + i\n","    features[col_name] = list(temp_features[i])\n","\n","  # Add next word POS\n","  temp_features = features_copy.drop(0)\n","\n","  temp_features.loc[temp_features.shape[0]+1, :] = [0] * len(COLUMNS)\n","  temp_features = temp_features.sort_index()\n","\n","  for i in COLUMNS:\n","    col_name = \"Post_\" + i\n","    # print( len(list(temp_features[i])))\n","    # print(len(features[i]))\n","    features[col_name] = list(temp_features[i])\n","  # print(features)\n","  return features\n","\n","def build_lrm(features, expected):\n","  lrm = LogisticRegression(solver='newton-cg')\n","  lrm.fit(features, expected)\n","  return lrm\n","  \n","# features = feature_extraction(POS_lst)\n","# lrm = build_lrm(features, NER_lst)\n","\n","# predicted = lrm.predict(features)\n","# print(NER_lst)\n","# print(list(predicted))\n","# print(lrm.score(features, NER_lst))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jCkP4A3IYwit"},"source":["### **MEMM Implementation**\n","---"]},{"cell_type":"code","metadata":{"id":"iAR5IjDxY4GJ"},"source":["# Your implementation here\n","\n","# takes in the hmm build above and an observation: list of tokens\n","# and returns the appropriate named entity mappings for the tokens\n","def MEMM(hmm, lrm, observation, observation_POS):\n","\n","  test_features = feature_extraction(observation_POS)\n","\n","  # print(test_features)\n","  prediction = lrm.predict_proba(test_features)\n","  \n","  labels = list(hmm['initial'].keys())\n","  initial = hmm['initial']\n","  lst = []\n","  BPTR = []\n","\n","  for i in range(len(observation)):\n","    token = observation[i]\n","    if i == 0:\n","      probs = {}\n","      for label in labels:\n","        probs[label] = math.log(hmm['initial'][label]) + math.log(prediction[0][CLASSES.index(label)]) + get_emission_smoothing(hmm['emission'], label, token)\n","      lst.append(probs)\n","    else:\n","      BPTR.append({})\n","      probs = {}\n","      for label in labels:\n","        max_lst = []\n","        for prev_label in labels:\n","          max_lst.append(\n","              lst[i-1][prev_label]#] + get_transition_smoothing(hmm['transition'], prev_label, label)\n","          )\n","\n","        max_prob = max(max_lst)\n","        BPTR[i-1][label] = labels[max_lst.index(max_prob)]\n","        # print(prediction[i][CLASSES.index(label)])\n","        probs[label] = max_prob + math.log(prediction[i][CLASSES.index(label)]) + get_emission_smoothing(hmm['emission'], label, token)\n","      lst.append(probs)\n","\n","  final_probs = [lst[-1][key] for key in labels]\n","  max_prob = max(final_probs)\n","  backpointer = labels[final_probs.index(max_prob)]\n","  final_labels = [backpointer]\n","\n","  BPTR.reverse()\n","  for dic in BPTR:\n","    backpointer = dic[backpointer]\n","    final_labels.append(backpointer)\n","  \n","  final_labels.reverse()\n","    \n","  # print(lst)\n","  # print(final_labels)\n","  return final_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvoeAVMlX4gp"},"source":["### **Validation**\n","---\n","In this section we want you to run your MaxEnt model on the validation dataset you extracted earlier. We want you to play around with different combinations of features in order to find which features work the best for your implementation. You will be asked to write about this process in detail in written question 3.3 so please spend time experimenting with features! Once again, please use the code we provided for computing Entity Level Avg F1 earlier when validating your model."]},{"cell_type":"code","metadata":{"id":"sNBgHDrJUswR"},"source":["from sklearn.model_selection import train_test_split\n","\n","text_train, text_val, NER_train, NER_val, index_train, index_val, POS_train, POS_val =\\\n"," train_test_split(train['text'], train['NER'], train['index'], train['POS'], test_size=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TmUnuuxvHKXM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634440793083,"user_tz":240,"elapsed":90501,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"28be4eb5-2abe-479e-c1cd-e63291afd3d5"},"source":["# Run your model on validation set\n","# You will need to \n","# 1. Call your function above to get a prediction result on Validation Set\n","# 2. Report Metrics\n","# (See if you need to modify your feature set)\n","\n","token_lst, POS_lst, NER_lst, indices_lst = flatten(text_train, POS_train, NER_train, index_train)\n","features = feature_extraction(POS_lst)\n","# features = feature_extraction(token_lst)\n","\n","\n","# print(features.columns)\n","lrm = build_lrm(features, NER_lst)\n","\n","hmm=build_hmm(text_train, NER_train)\n","\n","predicted_labels = []\n","for i in range(len(text_val)):\n","  val_example = text_val[i]\n","  POS_example = POS_val[i]\n","  # print(POS_example)\n","  labels = MEMM(hmm, lrm, val_example, POS_example)\n","  predicted_labels.append(labels)\n","\n","f1_lst = []\n","for i in range(len(NER_val)):\n","  pred_token_labels = predicted_labels[i]\n","  token_indices = index_val[i]\n","  true_token_labels = NER_val[i]\n","\n","  y_pred_dict = format_output_labels(pred_token_labels, token_indices)\n","  # print(\"y_pred_dict is : \" + str(y_pred_dict))\n","  y_true_dict = format_output_labels(true_token_labels, token_indices)\n","  # print(\"y_true_dict is : \" + str(y_true_dict))\n","  f1 = mean_f1(y_pred_dict, y_true_dict)\n","  # print(\"Entity Level Mean F1 score is : \" + str(f1))\n","  f1_lst.append(f1)\n","\n","print(\"Mean f1 : \" + str(sum(f1_lst)/len(f1_lst)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean f1 : 0.7016762126137126\n"]}]},{"cell_type":"code","metadata":{"id":"7AfIETB2iepg"},"source":["token_lst, POS_lst, NER_lst, indices_lst = flatten(text_train, POS_train, NER_train, index_train)\n","features = feature_extraction(POS_lst)\n","# features = feature_extraction(token_lst)\n","\n","\n","# print(features.columns)\n","lrm = build_lrm(features, NER_lst)\n","\n","hmm = build_hmm(text_train, NER_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"aVootJ6DhB1t","executionInfo":{"status":"ok","timestamp":1634440926154,"user_tz":240,"elapsed":174,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"6780d693-6af9-490f-a6c4-e9102587bb21"},"source":["# Error Analysis\n","# 20, 22, \n","ind = 1\n","obs_2 = text_val[ind][6:15]\n","obs_2_POS = POS_val[ind][6:15]\n","obs_2_NER = NER_val[ind][6:15]\n","\n","# obs_1 = \"Bavarian lawmaker Markus SÃ¶der says conservatives in Germany will likely face a new era in opposition. His assessment contrasts strongly with that of fellow conservative leader Armin Laschet.\".split()\n","# print(obs_2)\n","# print(\"MEMM\", MEMM(hmm, lrm, obs_2, obs_2_POS))\n","# print(\"Vite\", viterbi(hmm, obs_2))\n","# print(\"True\", obs_2_NER)\n","\n","comparison = [MEMM(hmm, lrm, obs_2, obs_2_POS), viterbi(hmm, obs_2), obs_2_NER]\n","df = pd.DataFrame(comparison)\n","df.index = [\"MEMM\", \"HMM\", \"Real\"]\n","df.columns = obs_2\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>silence</th>\n","      <th>.</th>\n","      <th>Kieran</th>\n","      <th>Murray</th>\n","      <th>LAS</th>\n","      <th>CRUCES</th>\n","      <th>,</th>\n","      <th>N.M.</th>\n","      <th>1996-08-22</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>MEMM</th>\n","      <td>O</td>\n","      <td>O</td>\n","      <td>PER</td>\n","      <td>PER</td>\n","      <td>ORG</td>\n","      <td>PER</td>\n","      <td>O</td>\n","      <td>PER</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>HMM</th>\n","      <td>O</td>\n","      <td>O</td>\n","      <td>PER</td>\n","      <td>PER</td>\n","      <td>PER</td>\n","      <td>PER</td>\n","      <td>O</td>\n","      <td>O</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>Real</th>\n","      <td>O</td>\n","      <td>O</td>\n","      <td>PER</td>\n","      <td>PER</td>\n","      <td>LOC</td>\n","      <td>LOC</td>\n","      <td>O</td>\n","      <td>O</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     silence  . Kieran Murray  LAS CRUCES  , N.M. 1996-08-22\n","MEMM       O  O    PER    PER  ORG    PER  O  PER          O\n","HMM        O  O    PER    PER  PER    PER  O    O          O\n","Real       O  O    PER    PER  LOC    LOC  O    O          O"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"-wpenpKP4bBv","executionInfo":{"status":"ok","timestamp":1634440929450,"user_tz":240,"elapsed":393,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"2f6084e5-b3f7-4738-e5e7-089779bf6f0b"},"source":["#Comparing our feature set to the training data\n","counts_of_POS ={}\n","\n","for j in range(len(train[\"POS\"])):\n","  for i in range(len(train['POS'][j])):\n","      \n","    POS_at_i = train['POS'][j][i]\n","    NER_at_i = train['NER'][j][i]\n","\n","    if POS_at_i not in counts_of_POS:\n","      counts_of_POS[POS_at_i] = {}\n","    if NER_at_i not in counts_of_POS[POS_at_i]:\n","      counts_of_POS[POS_at_i][NER_at_i] = 1\n","    else: \n","      counts_of_POS[POS_at_i][NER_at_i] += 1\n","\n","maxOfEach = {}\n","percentOfTotalTags=[]\n","for POS in counts_of_POS:\n","  maxNER = 0\n","  nameNER =\"\"\n","\n","  totalTags = 0\n","  for NER in counts_of_POS[POS]:\n","    totalTags += counts_of_POS[POS][NER]\n","    if counts_of_POS[POS][NER] > maxNER:\n","      maxNER = counts_of_POS[POS][NER]\n","      nameNER = NER\n","  maxOfEach[POS] ={}\n","  maxOfEach[POS][nameNER] = maxNER\n","  percentOfTotalTags.append(maxNER/totalTags)\n","\n","keysofMax = list(maxOfEach.keys())\n","NERlst = []\n","totalofMAX = []\n","\n","for POS in maxOfEach.keys():\n","  for NER in maxOfEach[POS]:\n","    NERlst.append(NER)\n","    totalofMAX.append(maxOfEach[POS][NER])\n","for i in range(len(keysofMax)):\n","  keysofMax[i] = keysofMax[i] + \" : \" + NERlst[i]\n","\n","POSandNERdf = pd.DataFrame([totalofMAX, percentOfTotalTags])\n","\n","#POSandNERdf = pd.DataFrame(totalofMAX)\n","POSandNERdf.index = [\"Max NER for given POS token\", \"Percentage of Total NER Tags\"]\n","POSandNERdf.columns = keysofMax\n","POSandNERdf\n","#set like list\n","# for elt in counts_of_POS.keys():\n","#   counts_of_POS[elt]= set(counts_of_POS[elt])\n","# print(counts_of_POS)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>NNP : PER</th>\n","      <th>NN : O</th>\n","      <th>NNS : O</th>\n","      <th>IN : O</th>\n","      <th>. : O</th>\n","      <th>RB : O</th>\n","      <th>CD : O</th>\n","      <th>POS : O</th>\n","      <th>VBD : O</th>\n","      <th>TO : O</th>\n","      <th>DT : O</th>\n","      <th>JJ : O</th>\n","      <th>VBN : O</th>\n","      <th>, : O</th>\n","      <th>CC : O</th>\n","      <th>VB : O</th>\n","      <th>VBG : O</th>\n","      <th>VBZ : O</th>\n","      <th>PRP : O</th>\n","      <th>JJR : O</th>\n","      <th>WDT : O</th>\n","      <th>VBP : O</th>\n","      <th>RP : O</th>\n","      <th>: : O</th>\n","      <th>\" : O</th>\n","      <th>( : O</th>\n","      <th>$ : O</th>\n","      <th>) : O</th>\n","      <th>PRP$ : O</th>\n","      <th>MD : O</th>\n","      <th>NNPS : ORG</th>\n","      <th>JJS : O</th>\n","      <th>LS : O</th>\n","      <th>WP : O</th>\n","      <th>RBR : O</th>\n","      <th>FW : O</th>\n","      <th>EX : O</th>\n","      <th>WRB : O</th>\n","      <th>SYM : O</th>\n","      <th>WP$ : O</th>\n","      <th>PDT : O</th>\n","      <th>UH : O</th>\n","      <th>'' : O</th>\n","      <th>RBS : O</th>\n","      <th>NN|SYM : ORG</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Max NER for given POS token</th>\n","      <td>8370.000000</td>\n","      <td>18380.000000</td>\n","      <td>7707.000000</td>\n","      <td>15078.000000</td>\n","      <td>5924.000000</td>\n","      <td>3106.000000</td>\n","      <td>15038.000000</td>\n","      <td>1177.000000</td>\n","      <td>6640.000000</td>\n","      <td>2771.000000</td>\n","      <td>10681.000000</td>\n","      <td>7869.000000</td>\n","      <td>3292.000000</td>\n","      <td>5808.000000</td>\n","      <td>2894.000000</td>\n","      <td>3295.000000</td>\n","      <td>1966.000000</td>\n","      <td>1915.00000</td>\n","      <td>2508.000000</td>\n","      <td>277.000000</td>\n","      <td>416.000000</td>\n","      <td>1158.000000</td>\n","      <td>416.000000</td>\n","      <td>1926.000000</td>\n","      <td>1701.0</td>\n","      <td>2348.000000</td>\n","      <td>298.000000</td>\n","      <td>2351.000000</td>\n","      <td>1194.0</td>\n","      <td>960.000000</td>\n","      <td>227.000000</td>\n","      <td>178.000000</td>\n","      <td>11.0</td>\n","      <td>413.0</td>\n","      <td>132.0</td>\n","      <td>95.000000</td>\n","      <td>107.0</td>\n","      <td>311.0</td>\n","      <td>343.00</td>\n","      <td>17.0</td>\n","      <td>26.0</td>\n","      <td>10.00000</td>\n","      <td>30.0000</td>\n","      <td>29.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Percentage of Total NER Tags</th>\n","      <td>0.305664</td>\n","      <td>0.964677</td>\n","      <td>0.963134</td>\n","      <td>0.988851</td>\n","      <td>0.998483</td>\n","      <td>0.973973</td>\n","      <td>0.993919</td>\n","      <td>0.970322</td>\n","      <td>0.997147</td>\n","      <td>0.988936</td>\n","      <td>0.993304</td>\n","      <td>0.834907</td>\n","      <td>0.987995</td>\n","      <td>0.998281</td>\n","      <td>0.979689</td>\n","      <td>0.962606</td>\n","      <td>0.957623</td>\n","      <td>0.98407</td>\n","      <td>0.996424</td>\n","      <td>0.926421</td>\n","      <td>0.976526</td>\n","      <td>0.993139</td>\n","      <td>0.976526</td>\n","      <td>0.997927</td>\n","      <td>1.0</td>\n","      <td>0.994915</td>\n","      <td>0.856322</td>\n","      <td>0.994922</td>\n","      <td>1.0</td>\n","      <td>0.997921</td>\n","      <td>0.424299</td>\n","      <td>0.936842</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.612903</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.98</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.47619</td>\n","      <td>0.9375</td>\n","      <td>0.966667</td>\n","      <td>0.666667</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                NNP : PER  ...  NN|SYM : ORG\n","Max NER for given POS token   8370.000000  ...      2.000000\n","Percentage of Total NER Tags     0.305664  ...      0.666667\n","\n","[2 rows x 45 columns]"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"fGkhL1imxpmH"},"source":["### **Q3.1: Implementation Details**\n","Explain how you implemented the MEMM and whether/how you modified Viterbi (e.g. which algorithms/data structures you used, what features are included). Make clear which parts were implemented from scratch vs. obtained via an existing package. (Please answer on the written questions template document)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m_eDwiILvHGL"},"source":["### **Q3.2: Results Analysis**\n","Explain here how you evaluated the MEMM model. Summarize the performance of your system and any variations that you experimented with the validation datasets. Put the results into clearly labeled tables or diagrams and include your observations and analysis. (Please answer on the written questions template document)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ammZn20RZn8h"},"source":["### **Q3.3: Feature Engineering**\n","What features are considered most important by your MaxEnt Classifier? Why do you think these features make sense? Describe your experiments with feature sets. An analysis on feature selection for the MEMM is required â€“ e.g. what features **help most**, why? An **error analysis** is required â€“ e.g. what sorts of errors occurred, why? (Please answer on the written questions template document)"]},{"cell_type":"markdown","metadata":{"id":"h4XaDMlcaDBA"},"source":["### **Q3.4: Room for Improvement**\n","When did the system work well, when did it fail and any ideas as to why? How might you improve the system?\n"]},{"cell_type":"markdown","metadata":{"id":"lQwPwqU3vMiS"},"source":["# **Part 4: Comparing HMMs and MEMMs**\n","\n","---\n","\n","In this section you will be asked to analyze and compare the models you have developed!\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ndVxVFzFagZ5"},"source":["### **Q4.1: Result Comparison**\n","Compare here your results (validation scores) for your HMM and the MEMM. Which of them performs better? Why? (Please answer on the written questions template document)"]},{"cell_type":"markdown","metadata":{"id":"cjGcdm5aafl3"},"source":["### **Q4.2: Error Analysis 1**\n","Do some error analysis. What are error patterns you observed that the HMM makes but the MEMM does not? Try to justify why/why not? **Please give examples from the dataset.** (Please answer on the written questions template document)"]},{"cell_type":"markdown","metadata":{"id":"rZYfZAgga9UB"},"source":["### **Q4.3: Error Analysis 2**\n","What are error patterns you observed that MEMM makes but the HMM does not? Try to justify what you observe? **Please give examples from the dataset.** (Please answer on the written questions template document)"]},{"cell_type":"markdown","metadata":{"id":"TpugxBD7RBy1"},"source":["# **Part 5: Kaggle Submission**\n","---\n","\n","Using the best-performing system from among all of your HMM and MEMM models, generate predictions for the test set, and submit them to Kaggle at https://www.kaggle.com/c/cs4740-fa21-p2. Note, you **need** to use our tokenizer as the labels on Kaggle corresponds to these. Below, we provide a function that submits given predicted tokens and associated token indices in the correct format. As a scoring metric on Kaggle, we use **Entity Level Mean F1**.\n","\n","Your submission to Kaggle should be a CSV file consisting of five lines and two columns. The first line is a fixed header, and each of the remaining four lines corresponds to one of the four types of named entities. The first column is the label identifier *Id* (one of PER, LOC, ORG or MISC), and the second column *Predicted* is a list of entities (separated by single space) that you predict to be of that type. Each entity is specified by its starting and ending index (concatenated by a hypen) as given in the test corpus. \n","\n","You can use the function **create_submission** that takes the list of predicted labels and the list of associated token indices as inputs and creates the the output CSV file at a specified path.\n","\n","NOTE: Ensure that there are **no** rows with *Id* = \"O\" in your Kaggle Submission."]},{"cell_type":"markdown","metadata":{"id":"1o4ssI2P_ZSi"},"source":["![picture](https://docs.google.com/uc?export=download&id=1pQkAyOdWQz62jB-YBaj8mHuwI6iWJ1GZ)"]},{"cell_type":"code","metadata":{"id":"893l9j77ETFM"},"source":["import csv\n","\n","def create_submission(output_filepath, token_labels, token_inds):\n","    \"\"\"\n","    :parameter output_filepath: The full path (including file name) of the output file, \n","                                with extension .csv\n","    :type output_filepath: [String]\n","    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).\n","    :type token_labels: List[String]\n","    :parameter token_indices: A list of token indices (taken from the dataset) \n","                              corresponding to the labels in [token_labels].\n","    :type token_indices: List[int]\n","    \"\"\"\n","    label_dict = format_output_labels(token_labels, token_inds)\n","    with open(output_filepath, mode='w') as csv_file:\n","        fieldnames = ['Id', 'Expected']\n","        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","        writer.writeheader()\n","        for key in label_dict:\n","            p_string = \" \".join([str(start)+\"-\"+str(end) for start,end in label_dict[key]])\n","            writer.writerow({'Id': key, 'Expected': p_string})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JmekAQpEcW2y"},"source":["hmm=build_hmm(train['text'], train['NER'])\n","token_labels = []\n","token_inds = []\n","\n","for obs_id in range(len(test['text'])):\n","  obs = test['text'][obs_id]\n","  token_labels.extend(viterbi(observation=obs, hmm=hmm))\n","  token_inds.extend(test['index'][obs_id])\n","# print(token_inds)\n","# print(token_labels)\n","\n","file_path = os.path.join(path,'submit.csv')\n","create_submission(file_path, token_labels, token_inds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSDbgvivuD2Y"},"source":["token_lst, POS_lst, NER_lst, indices_lst = flatten(train['text'], train['POS'], train['NER'], train['index'])\n","features = feature_extraction(POS_lst)\n","# features = feature_extraction(token_lst)\n","\n","\n","# print(features.columns)\n","lrm = build_lrm(features, NER_lst)\n","\n","hmm = build_hmm(text_train, NER_train)\n","\n","token_labels = []\n","token_inds = []\n","\n","for obs_id in range(len(test['text'])):\n","  obs = test['text'][obs_id]\n","  pos = test['POS'][obs_id]\n","  token_labels.extend(MEMM(hmm, lrm, obs, pos))\n","  token_inds.extend(test['index'][obs_id])\n","# print(token_inds)\n","# print(token_labels)\n","\n","file_path = os.path.join(path,'submit_MEMM.csv')\n","create_submission(file_path, token_labels, token_inds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BEZP_FGivVRI"},"source":["---\n","### **Q5.1: Competition Score**\n","\n","Include your **team name** and the **screenshot** of your best score from Kaggle. (Please answer on the written questions template document)"]},{"cell_type":"code","metadata":{"id":"CzkDSgZ6MYID"},"source":["#Convert into PDF\n","%%capture\n","!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RDu723QFMcKJ"},"source":["%%capture\n","# the red text is a placeholder! Change it to your directory structure!\n","!cp 'drive/My Drive/CS 4740/Project 2/CS_4740_FA21_p2_nar73_krm74.ipynb' ./ "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVgM5k_7MdXS","executionInfo":{"status":"ok","timestamp":1634440997404,"user_tz":240,"elapsed":4006,"user":{"displayName":"Kyrus Mama","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07268006968861800772"}},"outputId":"cd558f15-2596-4e14-e338-ba9e6ec6515a"},"source":["# the red text is a placeholder! Change it to the name of this notebook!\n","!jupyter nbconvert --to PDF \"CS_4740_FA21_p2_nar73_krm74.ipynb\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook CS_4740_FA21_p2_nar73_krm74.ipynb to PDF\n","[NbConvertApp] Writing 130901 bytes to ./notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: [u'xelatex', u'./notebook.tex', '-quiet']\n","[NbConvertApp] CRITICAL | xelatex failed: [u'xelatex', u'./notebook.tex', '-quiet']\n","This is XeTeX, Version 3.14159265-2.6-0.99998 (TeX Live 2017/Debian) (preloaded format=xelatex)\n"," restricted \\write18 enabled.\n","entering extended mode\n","(./notebook.tex\n","LaTeX2e <2017-04-15>\n","Babel <3.18> and hyphenation patterns for 3 language(s) loaded.\n","(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls\n","Document Class: article 2014/09/29 v1.4h Standard LaTeX document class\n","(/usr/share/texlive/texmf-dist/tex/latex/base/size11.clo))\n","(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcolorbox.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-lists.t\n","ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.def\n","(/usr/share/texlive/texmf-dist/tex/latex/ms/everyshi.sty))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex))\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphicx.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphics.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/trig.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics-def/xetex.def)))\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.code.t\n","ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.def\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfmx.def\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-pdf.de\n","f))))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol.code.\n","tex)) (/usr/share/texlive/texmf-dist/tex/latex/xcolor/xcolor.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/color.cfg))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basic.code\n",".tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trigonomet\n","ric.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.random.cod\n","e.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comparison\n",".code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.round.code\n",".tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.integerari\n","thmetics.code.tex)))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathconstruct.\n","code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusage.code\n",".tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphicstate.c\n","ode.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransformation\n","s.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.code.tex\n",")\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.code.t\n","ex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathprocessing\n",".code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.code.tex\n",")\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.code.tex\n","\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal.code.\n","tex))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransparency.c\n","ode.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns.code.\n","tex)))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.code.tex\n",") (/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code.tex\n",")\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-0-65\n",".sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-1-18\n",".sty)) (/usr/share/texlive/texmf-dist/tex/latex/tools/verbatim.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/environ/environ.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/trimspaces/trimspaces.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/etoolbox/etoolbox.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcbbreakable.code.tex\n","Library (tcolorbox): 'tcbbreakable.code.tex' version '4.12'\n",")) (/usr/share/texlive/texmf-dist/tex/latex/float/float.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/base/t1enc.def)\n","(/usr/share/texmf/tex/latex/lm/t1lmr.fd))\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/mathpazo.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/caption/caption.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/caption/caption3.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjustbox.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/xkeyval/xkeyval.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkeyval.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkvutils.tex)))\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjcalc.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/trimclip.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/collectbox/collectbox.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/tc-xetex.def))\n","(/usr/share/texlive/texmf-dist/tex/latex/ifoddpage/ifoddpage.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/varwidth/varwidth.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/tools/enumerate.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/geometry/geometry.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifpdf.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifvtex.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/ifxetex/ifxetex.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty\n","For additional information on amsmath, use the `?' option.\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/base/textcomp.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/base/ts1enc.def))\n","(/usr/share/texlive/texmf-dist/tex/latex/upquote/upquote.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/eurosym/eurosym.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/ucs/ucs.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/ucs/data/uni-global.def))\n","(/usr/share/texlive/texmf-dist/tex/latex/base/inputenc.sty\n","\n","Package inputenc Warning: inputenc package ignored with utf8 based engines.\n","\n",") (/usr/share/texlive/texmf-dist/tex/latex/fancyvrb/fancyvrb.sty\n","Style option: `fancyvrb' v2.7a, with DG/SPQR fixes, and firstline=lastline fix \n","<2008/02/07> (tvz))\n","(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/grffile.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/kvoptions.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ltxcmds.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/kvsetkeys.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/infwarerr.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/etexcmds.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifluatex.sty))))\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/pdftexcmds.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-hyperref.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/hobsub-generic.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/auxhook.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/pd1enc.def)\n","(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/hyperref.cfg)\n","(/usr/share/texlive/texmf-dist/tex/latex/url/url.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hxetex.def\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/puenc.def)\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/stringenc.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/rerunfilecheck.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/tools/longtable.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/booktabs/booktabs.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/enumitem/enumitem.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/ulem/ulem.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/mathrsfs.sty)\n","No file notebook.aux.\n","(/usr/share/texlive/texmf-dist/tex/latex/base/ts1cmr.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/t1ppl.fd)\n","ABD: EveryShipout initializing macros\n","(/usr/share/texlive/texmf-dist/tex/latex/caption/ltcaption.sty)\n","*geometry* driver: auto-detecting\n","*geometry* detected driver: xetex\n","*geometry* verbose mode - [ preamble ] result:\n","* driver: xetex\n","* paper: <default>\n","* layout: <same size as paper>\n","* layoutoffset:(h,v)=(0.0pt,0.0pt)\n","* modes: \n","* h-part:(L,W,R)=(72.26999pt, 469.75502pt, 72.26999pt)\n","* v-part:(T,H,B)=(72.26999pt, 650.43001pt, 72.26999pt)\n","* \\paperwidth=614.295pt\n","* \\paperheight=794.96999pt\n","* \\textwidth=469.75502pt\n","* \\textheight=650.43001pt\n","* \\oddsidemargin=0.0pt\n","* \\evensidemargin=0.0pt\n","* \\topmargin=-37.0pt\n","* \\headheight=12.0pt\n","* \\headsep=25.0pt\n","* \\topskip=11.0pt\n","* \\footskip=30.0pt\n","* \\marginparwidth=59.0pt\n","* \\marginparsep=10.0pt\n","* \\columnsep=10.0pt\n","* \\skip\\footins=10.0pt plus 4.0pt minus 2.0pt\n","* \\hoffset=0.0pt\n","* \\voffset=0.0pt\n","* \\mag=1000\n","* \\@twocolumnfalse\n","* \\@twosidefalse\n","* \\@mparswitchfalse\n","* \\@reversemarginfalse\n","* (1in=72.27pt=25.4mm, 1cm=28.453pt)\n","\n","(/usr/share/texlive/texmf-dist/tex/latex/ucs/ucsencs.def)\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/nameref.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/gettitlestring.sty))\n","\n","Package hyperref Warning: Rerun to get /PageLabels entry.\n","\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/ot1ppl.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/omlzplm.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/omszplm.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/omxzplm.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/psnfss/ot1zplm.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/ursfs.fd)\n","\n","LaTeX Warning: No \\author given.\n","\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/se-ascii-print.def)\n","[1] (/usr/share/texlive/texmf-dist/tex/latex/psnfss/ts1ppl.fd) [2]\n","\n","! LaTeX Error: File `https://drive.google.com/uc?id=1mxwn1_2Ef16_MJeyl9jJwwR6Io\n","hUOeHO' not found.\n","\n","See the LaTeX manual or LaTeX Companion for explanation.\n","Type  H <return>  for immediate help.\n"," ...                                              \n","                                                  \n","l.527 .../uc?id=1mxwn1_2Ef16_MJeyl9jJwwR6IohUOeHO}\n","                                                  \n","? \n","! Emergency stop.\n"," ...                                              \n","                                                  \n","l.527 .../uc?id=1mxwn1_2Ef16_MJeyl9jJwwR6IohUOeHO}\n","                                                  \n","Output written on notebook.pdf (2 pages).\n","Transcript written on notebook.log.\n","\n","[NbConvertApp] PDF successfully created\n","[NbConvertApp] Writing 21560 bytes to CS_4740_FA21_p2_nar73_krm74.pdf\n"]}]}]}